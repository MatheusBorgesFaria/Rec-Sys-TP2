{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disentangled Multimodal Representation Learning for Recommendation (DMRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideias:\n",
    "- FEITO: Usar a coluna para no texto\n",
    "- FEITO: Limpar os dados de conteudo\n",
    "- FEITO (lucas): ordenar os itens novos que ficaram por ultimo por popularidade\n",
    "- FEITO: Adicionar o nome da coluna, porque vai indicar o assunto, em conjunto do texto\n",
    "\n",
    "- tunar os hiperparametros\n",
    "- usa a imagem alem do texto\n",
    "- usar algum algoritmo mais simples apenas para usuarios novos, knn para content based\n",
    "- Entender o parametro exclude_unknowns=True do RatioSplit e se tem alguma forma do DMRL gerar previsoes para itens e usuarios novos \n",
    "\n",
    "- paralelizar a previsao\n",
    "Perguntas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cornac\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cornac.metrics import NDCG\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.data import TextModality, ImageModality\n",
    "from cornac.models.dmrl.recom_dmrl import DMRL \n",
    "from tqdm import tqdm\n",
    "from utils import load_data, preprocessing_content_data\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings, content, targets = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[\"TimestampDate\"] = ratings['Timestamp'].dt.date\n",
    "ratings.loc[ratings.Rating == 0, \"Rating\"] = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpar os dados\n",
    "\n",
    "- FEITO - Year: deletar o - no texto '2013–'\n",
    "- FEITO - Rated: alterar as diversas formas de escrever NA para NA. Esse e um caso especial\n",
    "- FEITO - alterar as diversas formas de escrever NA para None em todas as coluans\n",
    "- FEITO - Language: tem varias linhas que possuem bizarices como  'None, English' e 'None, French' , 'English, None'...\n",
    "- FEITO - Ratings: criar uma coluna para cada chave do dicionario, entender quais sao todas as chaves que existem\n",
    "\n",
    "-----------------------------------------------\n",
    "Variaveis qwue nao precisariam ser tratadas com um bert:\n",
    "- Metascore\n",
    "- imdbRating\n",
    "- Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar = content.drop(columns=[\"Poster\", \"Website\", \"Response\", \"Episode\", \"seriesID\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar['Year'] = content_auxiliar['Year'].str.replace('–', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = content.totalSeasons.unique()[0]\n",
    "dict_transform_to_na = {\n",
    "    \"Rated\":['N/A', 'Not Rated', 'Unrated', 'UNRATED', 'NOT RATED'],\n",
    "    \"all\": [nan, 'N/A', 'None', np.nan],\n",
    "}\n",
    "\n",
    "for na_value in dict_transform_to_na[\"all\"]:\n",
    "    content_auxiliar = content_auxiliar.replace(na_value, None)\n",
    "\n",
    "for na_value in dict_transform_to_na[\"Rated\"]:\n",
    "    content_auxiliar['Rated'] = content_auxiliar['Rated'].replace(na_value, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar['Language'] = content_auxiliar['Language'].str.replace('None, ', '')\n",
    "content_auxiliar['Language'] = content_auxiliar['Language'].str.replace(', None', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entendendo os valores possiveis para a coluna Ratings\n",
    "# A coluna content_auxiliar.Ratings quarda uma lista que posde ter entre 0 e 3 dicionarios. Cada dicionario possui a chave 'Source', 'Value'.\n",
    "num_ratings_per_item = []\n",
    "unique_keys = []\n",
    "rating_sources = []\n",
    "rating_values = []\n",
    "\n",
    "for rating_list in content_auxiliar.Ratings:\n",
    "    num_ratings_per_item.append(len(rating_list))\n",
    "    for rating_dict in rating_list:\n",
    "        for key in rating_dict:\n",
    "            unique_keys.append(key)\n",
    "        rating_sources.append(rating_dict['Source'])\n",
    "        rating_values.append(rating_dict['Value'])\n",
    "\n",
    "set(rating_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "InternetMovieDatabase_list = []\n",
    "Metacritic_list = []\n",
    "RottenTomatoes_list = []\n",
    "for rating_list in content_auxiliar.Ratings:\n",
    "    InternetMovieDatabase_list.append(None)\n",
    "    Metacritic_list.append(None)\n",
    "    RottenTomatoes_list.append(None)\n",
    "    for rating_dict in rating_list:\n",
    "        if rating_dict['Source'] == 'Internet Movie Database':\n",
    "            InternetMovieDatabase_list[-1] = rating_dict['Value']\n",
    "        elif rating_dict['Source'] == 'Metacritic':\n",
    "            Metacritic_list[-1] = rating_dict['Value']\n",
    "        elif rating_dict['Source'] == 'Rotten Tomatoes':\n",
    "            RottenTomatoes_list[-1] = rating_dict['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar['Internet Movie Database'] = InternetMovieDatabase_list\n",
    "content_auxiliar['Metacritic'] = Metacritic_list\n",
    "content_auxiliar['Rotten Tomatoes'] = RottenTomatoes_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar.drop(columns=['Ratings'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apendar a coluna no valor do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_columns = content_auxiliar.columns.to_list()\n",
    "content_columns.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in content_columns:\n",
    "    content_auxiliar[column] = content_auxiliar[column].apply(lambda x: f\"{column}: {x}; \" if x is not None else f\"{column}: unknown value; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_processed = content_auxiliar[['ItemId']].copy()\n",
    "content_processed[\"text\"] = content_auxiliar[content_columns].astype(str).fillna('').agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegando os dados de imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = 'poster_images'\n",
    "\n",
    "# # Lista todos os arquivos na pasta\n",
    "# file_list = os.listdir(folder_path)\n",
    "# item_id_saved = [file.split('=')[1].split('.')[0] for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_to_get_image = content.loc[content.Poster != 'N/A', ['ItemId', 'Poster']].copy()\n",
    "# content_to_get_image = content_to_get_image.loc[~content_to_get_image.ItemId.isin(item_id_saved), :]\n",
    "# content_to_get_image = content_to_get_image.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_image(index):\n",
    "#     image_url = content_to_get_image.loc[index, \"Poster\"]\n",
    "#     item_id = content_to_get_image.loc[index, \"ItemId\"]\n",
    "\n",
    "#     # Fazer a requisição HTTP para baixar a imagem\n",
    "#     # try:\n",
    "#     response = requests.get(image_url)\n",
    "#     # except:\n",
    "#         # return None\n",
    "\n",
    "#     # Verificar se a requisição foi bem-sucedida\n",
    "#     if response.status_code == 200:\n",
    "#         # Abrir a imagem\n",
    "#         img = Image.open(BytesIO(response.content))\n",
    "#         # Salvar a imagem\n",
    "#         img.save(f\"{folder_path}/item_id={item_id}.jpg\")\n",
    "#         return None\n",
    "#     else:\n",
    "#         return (item_id, image_url)\n",
    "\n",
    "# # Lista de índices para processar\n",
    "# indices = content_to_get_image.index.tolist()\n",
    "\n",
    "# # Usar todos os CPUs disponíveis\n",
    "# result_list = Parallel(n_jobs=-1, verbose=100)(delayed(download_image)(index) for index in indices)\n",
    "\n",
    "# # Filtrar os resultados que não conseguiram baixar\n",
    "# nao_conseguiu_baixar = [result for result in result_list if result is None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# folder_path = 'poster_images'\n",
    "# image_files = os.listdir(folder_path)\n",
    "# images_item_ids = [file.split('=')[1].split('.')[0] for file in image_files]\n",
    "\n",
    "# # Initialize a list to store the image matrices\n",
    "# def process_image(image_file):\n",
    "#     image_path = os.path.join(folder_path, image_file)\n",
    "#     image = Image.open(image_path)\n",
    "#     image_matrix = np.array(image)\n",
    "#     return image_matrix.reshape(-1)\n",
    "\n",
    "# # Use Parallel to process images in parallel\n",
    "# image_matrices = Parallel(n_jobs=-1, verbose=100)(delayed(process_image)(image_file) for image_file in image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  E preciso que todas as imagens tenham o mesmo tamanho, para isso vamos adicionar zeros no final das imagens menores\n",
    "\n",
    "# maior_imagem = -1\n",
    "# for image in image_matrices:\n",
    "#     if image.shape[0] > maior_imagem:\n",
    "#         maior_imagem = image.shape[0]\n",
    "\n",
    "# for index, image in enumerate(image_matrices):\n",
    "#     new_image = np.insert(image, len(image), np.zeros(maior_imagem - len(image)))\n",
    "#     del image\n",
    "#     image_matrices[index] = new_image\n",
    "\n",
    "# def add_zeros_to_image(image):\n",
    "#     new_image = np.insert(image, len(image), np.zeros(maior_imagem - len(image)))\n",
    "#     del image\n",
    "#     return new_image\n",
    "\n",
    "# image_matrices = Parallel(n_jobs=-1, verbose=100)(delayed(add_zeros_to_image)(image) for image in image_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  E preciso que todas as imagens tenham o mesmo tamanho, para isso vamos deletar as maoires imagens\n",
    "# menor_imagem = np.Inf\n",
    "# for image in image_matrices:\n",
    "#     if image.shape[0] < menor_imagem:\n",
    "#         menor_imagem = image.shape[0]\n",
    "\n",
    "# def diminui_a_imagem(image):\n",
    "#     return image[:menor_imagem]\n",
    "\n",
    "# image_matrices2 = Parallel(n_jobs=-1, verbose=100)(delayed(diminui_a_imagem)(image) for image in image_matrices)\n",
    "\n",
    "# Convert the list of image matrices to a numpy array\n",
    "# image_matrices2 = np.array(image_matrices2)\n",
    "# image_modality = ImageModality(features=image_matrices2, ids=images_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_text_modality = TextModality(\n",
    "    corpus=content_processed.text.to_list(),\n",
    "    ids=content_processed.ItemId.to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import dummy_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Continuous(\"learning_rate\", low=0.00001, high=0.1),\n",
    "Continuous(\"decay_c\", low=0.001, high=1),\n",
    "Continuous(\"decay_1\", low=0.0001, high=0.1),\n",
    "Discrete(\"embedding_dim\", [40, 128, 200, 300, 500]),\n",
    "Discrete(\"bert_text_dim\", [100, 383, 600, 800, 1000]),\n",
    "Discrete(\"num_factors\", [2, 4, 8]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DMR_pipeline(parameters):\n",
    "    learning_rate, decay_c, decay_r = parameters\n",
    "    # learning_rate, decay_c, decay_r, epochs, embedding_dim, bert_text_dim, num_factors = parameters\n",
    "\n",
    "    ratio_split = RatioSplit(\n",
    "        data=ratings[['UserId', 'ItemId', 'Rating']].values.tolist(),\n",
    "        val_size=0.2,\n",
    "        test_size=0.2,\n",
    "        exclude_unknowns=True,\n",
    "        verbose=False,\n",
    "        seed=12012001,\n",
    "        rating_threshold=0.5,\n",
    "        item_text=item_text_modality,\n",
    "    )\n",
    "    dmrl_recommender = DMRL(\n",
    "        learning_rate=learning_rate,\n",
    "        decay_c=decay_c,\n",
    "        decay_r=decay_r,\n",
    "        # epochs=epochs,\n",
    "        # embedding_dim=embedding_dim,\n",
    "        # bert_text_dim=bert_text_dim,\n",
    "        # num_factors=num_factors,\n",
    "        verbose=False,\n",
    "    )\n",
    "    experiment = cornac.Experiment(\n",
    "        eval_method=ratio_split,\n",
    "        models=[dmrl_recommender], metrics=[NDCG(20)]\n",
    "    )\n",
    "    experiment_returned = experiment.run()\n",
    "    experiment = cornac.Experiment(\n",
    "        eval_method=ratio_split,\n",
    "        models=[dmrl_recommender], metrics=[NDCG(20)]\n",
    "    )\n",
    "    experiment_returned = experiment.run()\n",
    "    return -1 * experiment.result[0].metric_avg_results['NDCG@20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = [\n",
    "    (0.00001, 0.1),  # learning_rate\n",
    "    (0.001, 1),  # decay_c\n",
    "    (0.0001, 0.1),  # decay_r\n",
    "    # [20, 30, 40, 50], # epochs \n",
    "    # [40, 128, 200, 300, 500],  # embedding_dim\n",
    "    # [100, 383, 600, 800, 1000],  # bert_text_dim\n",
    "    # [2, 4, 8],  # num_factors\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dummy_minimize(DMR_pipeline, space, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put everything together into an experiment and run it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_score(user_id):\n",
    "#     target_line = pd.DataFrame()\n",
    "#     user_index = ratio_split.train_set.uid_map.get(user_id)\n",
    "\n",
    "#     columns = [\"UserId\", \"ItemId\", \"Rating\"]\n",
    "#     items_to_predict = targets.loc[targets.UserId == user_id, \"ItemId\"].to_list()\n",
    "#     target_line[\"ItemId\"] = items_to_predict\n",
    "#     target_line[\"UserId\"] = user_id\n",
    "#     target_line[\"Rating\"] = -1\n",
    "#     if user_index is None:\n",
    "#         return target_line[columns]\n",
    "\n",
    "#     # Get the train dataframe index of the items to predict\n",
    "#     items_to_predict_index = np.array([ratio_split.train_set.iid_map.get(item_id) for item_id in items_to_predict])\n",
    "#     items_to_predict_tensor = torch.tensor([idx for idx in items_to_predict_index if idx is not None])\n",
    "\n",
    "#     # Get the position of items that are not in the train set\n",
    "#     none_indices = [i for i, x in enumerate(items_to_predict_index) if x is None]\n",
    "\n",
    "#     # Get the prediction for the items\n",
    "#     line_rating = dmrl_recommender.score(user_index=user_index, item_indices=items_to_predict_tensor)\n",
    "\n",
    "#     # Insert -1 in the position of items that are not in the train set\n",
    "#     for index_to_insert in none_indices:\n",
    "#         line_rating = np.insert(line_rating, index_to_insert, -1)\n",
    "\n",
    "#     # Insert the prediction in the target_prediction dataframe\n",
    "#     target_line[\"Rating\"] = line_rating\n",
    "#     return target_line[columns]\n",
    "\n",
    "# user_id_list = targets.UserId.unique()\n",
    "# target_prediction_list = Parallel(n_jobs=-1, verbose=100)(delayed(compute_score)(user_id) for user_id in user_id_list)\n",
    "# target_prediction = pd.concat(target_prediction_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = targets.copy()\n",
    "target_prediction[\"Rating\"] = -1\n",
    "\n",
    "user_id_list = targets.UserId.unique()\n",
    "for user_id in user_id_list:\n",
    "    # Get the train dataframe index of the user to predict\n",
    "    user_index = ratio_split.train_set.uid_map.get(user_id)\n",
    "\n",
    "    if user_index is None:\n",
    "        print(f\"User {user_id} is not in the train set\")\n",
    "        continue\n",
    "\n",
    "    # Flter by items to predict \n",
    "    items_to_predict = targets.loc[targets.UserId == user_id, \"ItemId\"].to_list()\n",
    "\n",
    "    # Get the train dataframe index of the items to predict\n",
    "    items_to_predict_index = np.array([ratio_split.train_set.iid_map.get(item_id) for item_id in items_to_predict])\n",
    "\n",
    "    items_to_predict_tensor = torch.tensor([idx for idx in items_to_predict_index if idx is not None])\n",
    "\n",
    "    # Get the position of items that are not in the train set\n",
    "    none_indices = [i for i, x in enumerate(items_to_predict_index) if x is None]\n",
    "\n",
    "    # Get the prediction for the items\n",
    "    line_rating = dmrl_recommender.score(user_index=user_index, item_indices=items_to_predict_tensor)\n",
    "\n",
    "    # Insert -1 in the position of items that are not in the train set\n",
    "    for index_to_insert in none_indices:\n",
    "        line_rating = np.insert(line_rating, index_to_insert, -1)\n",
    "\n",
    "    # Insert the prediction in the target_prediction dataframe\n",
    "    target_prediction.loc[targets.UserId == user_id, \"Rating\"] = line_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = target_prediction.sort_values([\"UserId\", \"Rating\"], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction.to_csv(\"submissao_5_DMRL_versao_4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = target_prediction.drop(columns=\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction.to_csv(\"submissao_5_DMRL_versao_4_sem_rating.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sistema_recomendacao_tp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
