{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disentangled Multimodal Representation Learning for Recommendation (DMRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideias:\n",
    "- FEITO: Usar a coluna para no texto\n",
    "- FEITO: Limpar os dados de conteudo\n",
    "- FEITO (lucas): ordenar os itens novos que ficaram por ultimo por popularidade\n",
    "- FEITO: Adicionar o nome da coluna, porque vai indicar o assunto, em conjunto do texto\n",
    "\n",
    "- tunar os hiperparametros\n",
    "- usa a imagem alem do texto\n",
    "- usar algum algoritmo mais simples apenas para usuarios novos, knn para content based\n",
    "- Entender o parametro exclude_unknowns=True do RatioSplit e se tem alguma forma do DMRL gerar previsoes para itens e usuarios novos \n",
    "\n",
    "- paralelizar a previsao\n",
    "Perguntas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus/miniconda/envs/sistema_recomendacao_tp2_versao3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import cornac\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cornac.metrics import NDCG\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.data import TextModality, ImageModality\n",
    "from cornac.models.dmrl.recom_dmrl import DMRL \n",
    "from tqdm import tqdm\n",
    "from utils import load_data, preprocessing_content_data\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings, content, targets = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[\"TimestampDate\"] = ratings['Timestamp'].dt.date\n",
    "ratings.loc[ratings.Rating == 0, \"Rating\"] = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpar os dados de conteudo\n",
    "\n",
    "- FEITO - Year: deletar o - no texto '2013–'\n",
    "- FEITO - Rated: alterar as diversas formas de escrever NA para NA. Esse e um caso especial\n",
    "- FEITO - alterar as diversas formas de escrever NA para None em todas as coluans\n",
    "- FEITO - Language: tem varias linhas que possuem bizarices como  'None, English' e 'None, French' , 'English, None'...\n",
    "- FEITO - Ratings: criar uma coluna para cada chave do dicionario, entender quais sao todas as chaves que existem\n",
    "\n",
    "-----------------------------------------------\n",
    "Variaveis que nao precisariam ser tratadas com um bert:\n",
    "- Metascore\n",
    "- imdbRating\n",
    "- Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar = content.drop(columns=[\"Poster\", \"Website\", \"Response\", \"Episode\", \"seriesID\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar['Year'] = content_auxiliar['Year'].str.replace('–', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = content.totalSeasons.unique()[0]\n",
    "dict_transform_to_na = {\n",
    "    \"Rated\":['N/A', 'Not Rated', 'Unrated', 'UNRATED', 'NOT RATED'],\n",
    "    \"all\": [nan, 'N/A', 'None', np.nan],\n",
    "}\n",
    "\n",
    "for na_value in dict_transform_to_na[\"all\"]:\n",
    "    content_auxiliar = content_auxiliar.replace(na_value, None)\n",
    "\n",
    "for na_value in dict_transform_to_na[\"Rated\"]:\n",
    "    content_auxiliar['Rated'] = content_auxiliar['Rated'].replace(na_value, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar['Language'] = content_auxiliar['Language'].str.replace('None, ', '')\n",
    "content_auxiliar['Language'] = content_auxiliar['Language'].str.replace(', None', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Internet Movie Database', 'Metacritic', 'Rotten Tomatoes'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entendendo os valores possiveis para a coluna Ratings\n",
    "# A coluna content_auxiliar.Ratings quarda uma lista que posde ter entre 0 e 3 dicionarios. Cada dicionario possui a chave 'Source', 'Value'.\n",
    "num_ratings_per_item = []\n",
    "unique_keys = []\n",
    "rating_sources = []\n",
    "rating_values = []\n",
    "\n",
    "for rating_list in content_auxiliar.Ratings:\n",
    "    num_ratings_per_item.append(len(rating_list))\n",
    "    for rating_dict in rating_list:\n",
    "        for key in rating_dict:\n",
    "            unique_keys.append(key)\n",
    "        rating_sources.append(rating_dict['Source'])\n",
    "        rating_values.append(rating_dict['Value'])\n",
    "\n",
    "set(rating_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "InternetMovieDatabase_list = []\n",
    "Metacritic_list = []\n",
    "RottenTomatoes_list = []\n",
    "for rating_list in content_auxiliar.Ratings:\n",
    "    InternetMovieDatabase_list.append(None)\n",
    "    Metacritic_list.append(None)\n",
    "    RottenTomatoes_list.append(None)\n",
    "    for rating_dict in rating_list:\n",
    "        if rating_dict['Source'] == 'Internet Movie Database':\n",
    "            InternetMovieDatabase_list[-1] = rating_dict['Value']\n",
    "        elif rating_dict['Source'] == 'Metacritic':\n",
    "            Metacritic_list[-1] = rating_dict['Value']\n",
    "        elif rating_dict['Source'] == 'Rotten Tomatoes':\n",
    "            RottenTomatoes_list[-1] = rating_dict['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar['Internet Movie Database'] = InternetMovieDatabase_list\n",
    "content_auxiliar['Metacritic'] = Metacritic_list\n",
    "content_auxiliar['Rotten Tomatoes'] = RottenTomatoes_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar.drop(columns=['Ratings'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apendar a coluna no valor do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ItemId'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_columns = content_auxiliar.columns.to_list()\n",
    "content_columns.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in content_columns:\n",
    "    content_auxiliar[column] = content_auxiliar[column].apply(lambda x: f\"{column}: {x}; \" if x is not None else f\"{column}: unknown value; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_processed = content_auxiliar[['ItemId']].copy()\n",
    "content_processed[\"text\"] = content_auxiliar[content_columns].astype(str).fillna('').agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegando os dados de imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = 'poster_images'\n",
    "\n",
    "# # Lista todos os arquivos na pasta\n",
    "# file_list = os.listdir(folder_path)\n",
    "# item_id_saved = [file.split('=')[1].split('.')[0] for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_to_get_image = content.loc[content.Poster != 'N/A', ['ItemId', 'Poster']].copy()\n",
    "# content_to_get_image = content_to_get_image.loc[~content_to_get_image.ItemId.isin(item_id_saved), :]\n",
    "# content_to_get_image = content_to_get_image.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_image(index):\n",
    "#     image_url = content_to_get_image.loc[index, \"Poster\"]\n",
    "#     item_id = content_to_get_image.loc[index, \"ItemId\"]\n",
    "\n",
    "#     # Fazer a requisição HTTP para baixar a imagem\n",
    "#     # try:\n",
    "#     response = requests.get(image_url)\n",
    "#     # except:\n",
    "#         # return None\n",
    "\n",
    "#     # Verificar se a requisição foi bem-sucedida\n",
    "#     if response.status_code == 200:\n",
    "#         # Abrir a imagem\n",
    "#         img = Image.open(BytesIO(response.content))\n",
    "#         # Salvar a imagem\n",
    "#         img.save(f\"{folder_path}/item_id={item_id}.jpg\")\n",
    "#         return None\n",
    "#     else:\n",
    "#         return (item_id, image_url)\n",
    "\n",
    "# # Lista de índices para processar\n",
    "# indices = content_to_get_image.index.tolist()\n",
    "\n",
    "# # Usar todos os CPUs disponíveis\n",
    "# result_list = Parallel(n_jobs=-1, verbose=100)(delayed(download_image)(index) for index in indices)\n",
    "\n",
    "# # Filtrar os resultados que não conseguiram baixar\n",
    "# nao_conseguiu_baixar = [result for result in result_list if result is None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processamento de imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# folder_path = 'poster_images'\n",
    "# image_files = os.listdir(folder_path)\n",
    "# images_item_ids = [file.split('=')[1].split('.')[0] for file in image_files]\n",
    "\n",
    "# # Initialize a list to store the image matrices\n",
    "# def process_image(image_file):\n",
    "#     image_path = os.path.join(folder_path, image_file)\n",
    "#     image = Image.open(image_path)\n",
    "#     image_matrix = np.array(image)\n",
    "#     return image_matrix.reshape(-1)\n",
    "\n",
    "# # Use Parallel to process images in parallel\n",
    "# image_matrices = Parallel(n_jobs=-1, verbose=100)(delayed(process_image)(image_file) for image_file in image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  E preciso que todas as imagens tenham o mesmo tamanho, para isso vamos adicionar zeros no final das imagens menores\n",
    "\n",
    "# maior_imagem = -1\n",
    "# for image in image_matrices:\n",
    "#     if image.shape[0] > maior_imagem:\n",
    "#         maior_imagem = image.shape[0]\n",
    "\n",
    "# for index, image in enumerate(image_matrices):\n",
    "#     new_image = np.insert(image, len(image), np.zeros(maior_imagem - len(image)))\n",
    "#     del image\n",
    "#     image_matrices[index] = new_image\n",
    "\n",
    "# def add_zeros_to_image(image):\n",
    "#     new_image = np.insert(image, len(image), np.zeros(maior_imagem - len(image)))\n",
    "#     del image\n",
    "#     return new_image\n",
    "\n",
    "# image_matrices = Parallel(n_jobs=-1, verbose=100)(delayed(add_zeros_to_image)(image) for image in image_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  E preciso que todas as imagens tenham o mesmo tamanho, para isso vamos deletar as maoires imagens\n",
    "# menor_imagem = np.Inf\n",
    "# for image in image_matrices:\n",
    "#     if image.shape[0] < menor_imagem:\n",
    "#         menor_imagem = image.shape[0]\n",
    "\n",
    "# def diminui_a_imagem(image):\n",
    "#     return image[:menor_imagem]\n",
    "\n",
    "# image_matrices2 = Parallel(n_jobs=-1, verbose=100)(delayed(diminui_a_imagem)(image) for image in image_matrices)\n",
    "\n",
    "# Convert the list of image matrices to a numpy array\n",
    "# image_matrices2 = np.array(image_matrices2)\n",
    "# image_modality = ImageModality(features=image_matrices2, ids=images_item_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formato do dado de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_text_modality = TextModality(\n",
    "    corpus=content_processed.text.to_list(),\n",
    "    ids=content_processed.ItemId.to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimizacao de hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import dummy_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        dill.dump(obj, file)\n",
    "\n",
    "def load_object(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return dill.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating_threshold = 0.5\n",
      "exclude_unknowns = True\n",
      "---\n",
      "Training data:\n",
      "Number of users = 46750\n",
      "Number of items = 27045\n",
      "Number of ratings = 527776\n",
      "Max rating = 10.0\n",
      "Min rating = 0.0\n",
      "Global mean = 7.3\n",
      "---\n",
      "Test data:\n",
      "Number of users = 46750\n",
      "Number of items = 27045\n",
      "Number of ratings = 124031\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Total users = 46750\n",
      "Total items = 27045\n"
     ]
    }
   ],
   "source": [
    "ratio_split = RatioSplit(\n",
    "    data=ratings[['UserId', 'ItemId', 'Rating']].values.tolist(),\n",
    "    test_size=0.2,\n",
    "    exclude_unknowns=True,\n",
    "    verbose=True,\n",
    "    seed=12012001,\n",
    "    rating_threshold=0.5,\n",
    "    item_text=item_text_modality\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DMR_pipeline(parameters):\n",
    "    learning_rate, decay_c, decay_r, num_factors = parameters\n",
    "    # learning_rate, decay_c, decay_r, epochs, embedding_dim, bert_text_dim, num_factors = parameters\n",
    "    print(\"Parametros da minimizacao\" + \"=\"*150)\n",
    "    model_parameters = (\n",
    "        # f\"learning_rate={learning_rate},decay_c={decay_c},decay_r={decay_r},embedding_dim={embedding_dim},bert_text_dim={bert_text_dim},num_factor={num_factors}\"\n",
    "        f\"learning_rate={learning_rate},decay_c={decay_c},decay_r={decay_r},num_factor={num_factors}\"\n",
    "\n",
    "        )\n",
    "    print(model_parameters)\n",
    "\n",
    "    dmrl_recommender = DMRL(\n",
    "        batch_size=1024,\n",
    "        epochs=20,\n",
    "        learning_rate=learning_rate,\n",
    "        decay_c=decay_c,\n",
    "        decay_r=decay_r,\n",
    "        # epochs=epochs,\n",
    "        # embedding_dim=embedding_dim,\n",
    "        # bert_text_dim=bert_text_dim,\n",
    "        num_factors=num_factors,\n",
    "        verbose=True,\n",
    "    )\n",
    "    experiment = cornac.Experiment(\n",
    "        eval_method=ratio_split,\n",
    "        models=[dmrl_recommender], metrics=[NDCG(20)]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        print(\"Iniciando o experimento...\")\n",
    "        experiment_returned = experiment.run()\n",
    "        metric = experiment.result[0].metric_avg_results['NDCG@20']\n",
    "        print(\"Metrica NDCG@20: \", metric)\n",
    "        model_path = \"dmrl_models/\" + f\"NDCG@20={metric},\" + model_parameters + \".model\"\n",
    "        save_object(dmrl_recommender, model_path)\n",
    "        # dmrl_recommender.save(\"dmrl_models/\" + f\"NDCG@20={metric},\" + model_parameters + \".model\")\n",
    "        print(\"Fim do experimento\", \"=\"*50)\n",
    "        return -1 * metric\n",
    "    except RuntimeError as e:\n",
    "        print(\"Erro na execução do experimento, retornando Infinito\")\n",
    "        print(e)\n",
    "        print(\"Fim do experimento\", \"=\"*100)\n",
    "        return np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = [\n",
    "    (0.00001, 0.1),  # learning_rate\n",
    "    (0.001, 1),  # decay_c # comentar isso nao resolveu o problema do cuda\n",
    "    (0.0001, 0.1),  # decay_r # comentar isso nao resolveu o problema do cuda\n",
    "    # [20, 30, 40, 50], # epochs # comentar isso nao resolveu o problema do cuda\n",
    "    # [40, 128, 200, 300, 500],  # embedding_dim # tem relacao com o erro de dimensao de multiplicacao de matriz\n",
    "    # [100, 383, 600, 800, 1000],  # bert_text_dim # tem relacao com o erro de dimensao de multiplicacao de matriz\n",
    "    [2, 4, 8],  # num_factors # comentar isso nao resolveu o problema do cuda\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = dummy_minimize(DMR_pipeline, space, verbose=True, n_calls=30, random_state=12012001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le o melhor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_object('dmrl_models/NDCG@20=0.08492874336076696,learning_rate=0.09938198721604344,decay_c=0.8755381083510204,decay_r=0.0004231976548177414,num_factor=2.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preve o modelo para os dados de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_train = ratings.UserId.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/51671 [00:00<54:09, 15.90it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1f0e3dad99 is not in the train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/51671 [00:01<46:31, 18.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User a5bfc9e079 is not in the train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 29/51671 [00:01<55:16, 15.57it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must have equal len keys and value when setting with an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 31\u001b[0m\n\u001b[1;32m     24\u001b[0m line_rating \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore(user_index\u001b[38;5;241m=\u001b[39muser_index, item_indices\u001b[38;5;241m=\u001b[39mitems_to_predict_tensor)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Insert -1 in the position of items that are not in the train set\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# for index_to_insert in none_indices:\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#     line_rating = np.insert(line_rating, index_to_insert, -1)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Insert the prediction in the ratings_prediction dataframe\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mratings_prediction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mratings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUserId\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPredictRating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m line_rating\n",
      "File \u001b[0;32m~/miniconda/envs/sistema_recomendacao_tp2_versao3/lib/python3.9/site-packages/pandas/core/indexing.py:818\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    817\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 818\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/sistema_recomendacao_tp2_versao3/lib/python3.9/site-packages/pandas/core/indexing.py:1795\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[1;32m   1794\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[0;32m-> 1795\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[0;32m~/miniconda/envs/sistema_recomendacao_tp2_versao3/lib/python3.9/site-packages/pandas/core/indexing.py:1850\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_integer(info_axis):\n\u001b[1;32m   1846\u001b[0m         \u001b[38;5;66;03m# This is a case like df.iloc[:3, [1]] = [0]\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m         \u001b[38;5;66;03m#  where we treat as df.iloc[:3, 1] = 0\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer((pi, info_axis[\u001b[38;5;241m0\u001b[39m]), value[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 1850\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1851\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust have equal len keys and value \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1852\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen setting with an iterable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1853\u001b[0m     )\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m lplane_indexer \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;66;03m# We get here in one case via .loc with a all-False mask\u001b[39;00m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Must have equal len keys and value when setting with an iterable"
     ]
    }
   ],
   "source": [
    "ratings_prediction = ratings.copy()\n",
    "ratings_prediction[\"PredictRating\"] = -1\n",
    "\n",
    "for user_id in tqdm(user_id_train):\n",
    "    # Get the train dataframe index of the user to predict\n",
    "    user_index = ratio_split.train_set.uid_map.get(user_id)\n",
    "\n",
    "    if user_index is None:\n",
    "        # print(f\"User {user_id} is not in the train set\")\n",
    "        continue\n",
    "\n",
    "    # Flter by items to predict \n",
    "    items_to_predict = ratings.loc[ratings.UserId == user_id, \"ItemId\"].to_list()\n",
    "\n",
    "    # Get the train dataframe index of the items to predict\n",
    "    items_to_predict_index = np.array([ratio_split.train_set.iid_map.get(item_id) for item_id in items_to_predict])\n",
    "\n",
    "    items_to_predict_tensor = torch.tensor([idx for idx in items_to_predict_index if idx is not None])\n",
    "\n",
    "    # Get the position of items that are not in the train set\n",
    "    none_indices = [i for i, x in enumerate(items_to_predict_index) if x is None]\n",
    "\n",
    "    # Get the prediction for the items\n",
    "    line_rating = model.score(user_index=user_index, item_indices=items_to_predict_tensor)\n",
    "\n",
    "    # Insert -1 in the position of items that are not in the train set\n",
    "    for index_to_insert in none_indices:\n",
    "        line_rating = np.insert(line_rating, index_to_insert, -1)\n",
    "\n",
    "    # Insert the prediction in the ratings_prediction dataframe\n",
    "    ratings_prediction.loc[ratings.UserId == user_id, \"PredictRating\"] = line_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preve para os dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 02b780c583 is not in the train set\n",
      "User 03fbe3c1e8 is not in the train set\n",
      "User 0503bf6097 is not in the train set\n",
      "User 0958560bd4 is not in the train set\n",
      "User 0b17c3df40 is not in the train set\n",
      "User 10c3aef33f is not in the train set\n",
      "User 119653373a is not in the train set\n",
      "User 139ccdbf5a is not in the train set\n",
      "User 1671fa4df3 is not in the train set\n",
      "User 16757f4d90 is not in the train set\n",
      "User 187454feb4 is not in the train set\n",
      "User 1af811e7b9 is not in the train set\n",
      "User 1bbf59e4a4 is not in the train set\n",
      "User 1ef563bb8b is not in the train set\n",
      "User 1f6a565547 is not in the train set\n",
      "User 211a7a84d3 is not in the train set\n",
      "User 243be2818a is not in the train set\n",
      "User 24a13b72fc is not in the train set\n",
      "User 27649dc46e is not in the train set\n",
      "User 2a3500f75b is not in the train set\n",
      "User 2ae6906d5d is not in the train set\n",
      "User 2cfdb14e05 is not in the train set\n",
      "User 2ed4d0c23c is not in the train set\n",
      "User 2ef13886ae is not in the train set\n",
      "User 302c6ebc7d is not in the train set\n",
      "User 3174db8344 is not in the train set\n",
      "User 32806098d9 is not in the train set\n",
      "User 33f009eb84 is not in the train set\n",
      "User 3402dfd220 is not in the train set\n",
      "User 35ecd9a245 is not in the train set\n",
      "User 3bf47241fd is not in the train set\n",
      "User 3cdc0a294d is not in the train set\n",
      "User 3f332c6d51 is not in the train set\n",
      "User 40008b9a53 is not in the train set\n",
      "User 40bf82623c is not in the train set\n",
      "User 4152564eeb is not in the train set\n",
      "User 448cd8ad0a is not in the train set\n",
      "User 454ea70f2b is not in the train set\n",
      "User 4579da10f3 is not in the train set\n",
      "User 49e73b2050 is not in the train set\n",
      "User 4a125a89bf is not in the train set\n",
      "User 4ad5bd1af9 is not in the train set\n",
      "User 4aec1b3435 is not in the train set\n",
      "User 4c0f9546c5 is not in the train set\n",
      "User 4c964a55e0 is not in the train set\n",
      "User 4e701bcd4c is not in the train set\n",
      "User 50a40208ac is not in the train set\n",
      "User 51aa0a41c5 is not in the train set\n",
      "User 51e0fff3f3 is not in the train set\n",
      "User 51f4efbfb3 is not in the train set\n",
      "User 52a8ed6a81 is not in the train set\n",
      "User 53d6d0c034 is not in the train set\n",
      "User 548b57cc0f is not in the train set\n",
      "User 564ed2dbdd is not in the train set\n",
      "User 621eb0b827 is not in the train set\n",
      "User 64bb3c7589 is not in the train set\n",
      "User 65d03e7a8e is not in the train set\n",
      "User 676c6ba546 is not in the train set\n",
      "User 689b243e8f is not in the train set\n",
      "User 69f65a2e91 is not in the train set\n",
      "User 6b620aedfa is not in the train set\n",
      "User 6c2665d7c3 is not in the train set\n",
      "User 6ec3ff0c92 is not in the train set\n",
      "User 70b0e5c6b6 is not in the train set\n",
      "User 71887f62f0 is not in the train set\n",
      "User 790d67b8e3 is not in the train set\n",
      "User 7c9ff93b39 is not in the train set\n",
      "User 7cef9fb420 is not in the train set\n",
      "User 7d5606f152 is not in the train set\n",
      "User 82ccad1ecc is not in the train set\n",
      "User 84550b83da is not in the train set\n",
      "User 8487e01fba is not in the train set\n",
      "User 848ceaa463 is not in the train set\n",
      "User 84a955d5ff is not in the train set\n",
      "User 84c230a5b1 is not in the train set\n",
      "User 86f2df8b1f is not in the train set\n",
      "User 87f3c42a65 is not in the train set\n",
      "User 8b2b3227d4 is not in the train set\n",
      "User 8c5a6a7482 is not in the train set\n",
      "User 8d2355364e is not in the train set\n",
      "User 8deddf03f0 is not in the train set\n",
      "User 8f1fbc45b8 is not in the train set\n",
      "User 8fc809ece3 is not in the train set\n",
      "User 8ff4a061e9 is not in the train set\n",
      "User 93c442d40a is not in the train set\n",
      "User 9424d06c85 is not in the train set\n",
      "User 943422623d is not in the train set\n",
      "User 964e752094 is not in the train set\n",
      "User 9752ba9f1e is not in the train set\n",
      "User 997d421097 is not in the train set\n",
      "User 9b03bd15cd is not in the train set\n",
      "User 9c415bdd4d is not in the train set\n",
      "User 9c810c5750 is not in the train set\n",
      "User a13e00b085 is not in the train set\n",
      "User a559244ec4 is not in the train set\n",
      "User acd6425f2d is not in the train set\n",
      "User acf73df8e4 is not in the train set\n",
      "User ad17a667ff is not in the train set\n",
      "User ad52498830 is not in the train set\n",
      "User adeaee4e38 is not in the train set\n",
      "User ae845eb437 is not in the train set\n",
      "User af4d3ce5d7 is not in the train set\n",
      "User b0e7534770 is not in the train set\n",
      "User b6ef5d5380 is not in the train set\n",
      "User b9898a075a is not in the train set\n",
      "User baa84d7484 is not in the train set\n",
      "User bb07992b31 is not in the train set\n",
      "User bbc77a1cfa is not in the train set\n",
      "User bdbe7e66f5 is not in the train set\n",
      "User be800ff41f is not in the train set\n",
      "User c14cc363e3 is not in the train set\n",
      "User c29b3351dd is not in the train set\n",
      "User c4f7cfb1e9 is not in the train set\n",
      "User c846afeb97 is not in the train set\n",
      "User c9b7db2d84 is not in the train set\n",
      "User cb955adc83 is not in the train set\n",
      "User cd13636b75 is not in the train set\n",
      "User cdd9c13725 is not in the train set\n",
      "User cf247d6daf is not in the train set\n",
      "User d1e7b08bdb is not in the train set\n",
      "User d29ee932b7 is not in the train set\n",
      "User d38901788c is not in the train set\n",
      "User d3956e2280 is not in the train set\n",
      "User d481fbe55e is not in the train set\n",
      "User d5cb15b7ec is not in the train set\n",
      "User d5e542d0cb is not in the train set\n",
      "User d606046667 is not in the train set\n",
      "User d9e6b46563 is not in the train set\n",
      "User daf33743af is not in the train set\n",
      "User db081d0be2 is not in the train set\n",
      "User db9d463dc9 is not in the train set\n",
      "User dc09c97fd7 is not in the train set\n",
      "User dcfd532698 is not in the train set\n",
      "User dd669b686f is not in the train set\n",
      "User def80cfcb4 is not in the train set\n",
      "User e321abd135 is not in the train set\n",
      "User e33bff0ce0 is not in the train set\n",
      "User e449b9317d is not in the train set\n",
      "User e480783401 is not in the train set\n",
      "User e9405e0733 is not in the train set\n",
      "User ea4b47f29f is not in the train set\n",
      "User ee16fa83c0 is not in the train set\n",
      "User ee6e910d8a is not in the train set\n",
      "User f0897e959b is not in the train set\n",
      "User f28f881458 is not in the train set\n",
      "User f4573fc71c is not in the train set\n",
      "User f7bf6074f6 is not in the train set\n",
      "User f8ea2e8463 is not in the train set\n",
      "User f9db6a7cd6 is not in the train set\n",
      "User fa07516b1d is not in the train set\n",
      "User fa2093fecd is not in the train set\n",
      "User fb2e203234 is not in the train set\n",
      "User fbe486dc4c is not in the train set\n",
      "User fc1aaf2d2e is not in the train set\n",
      "User fc1dc4549d is not in the train set\n",
      "User fdacbbcc2e is not in the train set\n",
      "User fe9de77866 is not in the train set\n",
      "User ff5552bd9e is not in the train set\n",
      "User ffbad8b9da is not in the train set\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_prediction = targets.copy()\n",
    "ratings_prediction[\"Rating\"] = -1\n",
    "\n",
    "user_id_list = targets.UserId.unique()\n",
    "for user_id in user_id_list:\n",
    "    # Get the train dataframe index of the user to predict\n",
    "    user_index = ratio_split.train_set.uid_map.get(user_id)\n",
    "\n",
    "    if user_index is None:\n",
    "        print(f\"User {user_id} is not in the train set\")\n",
    "        continue\n",
    "\n",
    "    # Flter by items to predict \n",
    "    items_to_predict = targets.loc[targets.UserId == user_id, \"ItemId\"].to_list()\n",
    "\n",
    "    # Get the train dataframe index of the items to predict\n",
    "    items_to_predict_index = np.array([ratio_split.train_set.iid_map.get(item_id) for item_id in items_to_predict])\n",
    "\n",
    "    items_to_predict_tensor = torch.tensor([idx for idx in items_to_predict_index if idx is not None])\n",
    "\n",
    "    # Get the position of items that are not in the train set\n",
    "    none_indices = [i for i, x in en\n",
    "                    \n",
    "                    umerate(items_to_predict_index) if x is None]\n",
    "\n",
    "    # Get the prediction for the items\n",
    "    line_rating = model.score(user_index=user_index, item_indices=items_to_predict_tensor)\n",
    "\n",
    "    # Insert -1 in the position of items that are not in the train set\n",
    "    for index_to_insert in none_indices:\n",
    "        line_rating = np.insert(line_rating, index_to_insert, -1)\n",
    "\n",
    "    # Insert the prediction in the ratings_prediction dataframe\n",
    "    target_prediction.loc[targets.UserId == user_id, \"Rating\"] = line_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = target_prediction.sort_values([\"UserId\", \"Rating\"], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction.to_csv(\"submissoes/DMRL_versao_6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = target_prediction.drop(columns=\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction.to_csv(\"submissoes/DMRL_versao_6_sem_rating.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sistema_recomendacao_tp2_versao3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
