{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvMF\n",
    "\n",
    "Demora muitooo para rodar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideias:\n",
    "- FEITO: Usar a coluna para no texto\n",
    "- FEITO: Limpar os dados de conteudo\n",
    "- FEITO (lucas): ordenar os itens novos que ficaram por ultimo por popularidade\n",
    "- FEITO: Adicionar o nome da coluna, porque vai indicar o assunto, em conjunto do texto\n",
    "\n",
    "- tunar os hiperparametros\n",
    "- usa a imagem alem do texto\n",
    "- usar algum algoritmo mais simples apenas para usuarios novos, knn para content based\n",
    "- Entender o parametro exclude_unknowns=True do RatioSplit e se tem alguma forma do DMRL gerar previsoes para itens e usuarios novos \n",
    "\n",
    "- paralelizar a previsao\n",
    "Perguntas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus/miniconda/envs/sistema_recomendacao_tp2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cornac\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cornac.metrics import NDCG\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.data import TextModality, ImageModality\n",
    "# from cornac.models.dmrl.recom_dmrl import DMRL \n",
    "from cornac.models.hrdr.recom_hrdr import HRDR\n",
    "from tqdm import tqdm\n",
    "from utils import load_data, preprocessing_content_data\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings, content, targets = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[\"TimestampDate\"] = ratings['Timestamp'].dt.date\n",
    "ratings.loc[ratings.Rating == 0, \"Rating\"] = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpar os dados\n",
    "\n",
    "- FEITO - Year: deletar o - no texto '2013–'\n",
    "- FEITO - Rated: alterar as diversas formas de escrever NA para NA. Esse e um caso especial\n",
    "- FEITO - alterar as diversas formas de escrever NA para None em todas as coluans\n",
    "- FEITO - Language: tem varias linhas que possuem bizarices como  'None, English' e 'None, French' , 'English, None'...\n",
    "- FEITO - Ratings: criar uma coluna para cada chave do dicionario, entender quais sao todas as chaves que existem\n",
    "\n",
    "-----------------------------------------------\n",
    "Variaveis qwue nao precisariam ser tratadas com um bert:\n",
    "- Metascore\n",
    "- imdbRating\n",
    "- Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar = content.drop(columns=[\"Poster\", \"Website\", \"Response\", \"Episode\", \"seriesID\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar['Year'] = content_auxiliar['Year'].str.replace('–', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = content.totalSeasons.unique()[0]\n",
    "dict_transform_to_na = {\n",
    "    \"Rated\":['N/A', 'Not Rated', 'Unrated', 'UNRATED', 'NOT RATED'],\n",
    "    \"all\": [nan, 'N/A', 'None', np.nan],\n",
    "}\n",
    "\n",
    "for na_value in dict_transform_to_na[\"all\"]:\n",
    "    content_auxiliar = content_auxiliar.replace(na_value, None)\n",
    "\n",
    "for na_value in dict_transform_to_na[\"Rated\"]:\n",
    "    content_auxiliar['Rated'] = content_auxiliar['Rated'].replace(na_value, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar['Language'] = content_auxiliar['Language'].str.replace('None, ', '')\n",
    "content_auxiliar['Language'] = content_auxiliar['Language'].str.replace(', None', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Internet Movie Database', 'Metacritic', 'Rotten Tomatoes'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entendendo os valores possiveis para a coluna Ratings\n",
    "# A coluna content_auxiliar.Ratings quarda uma lista que posde ter entre 0 e 3 dicionarios. Cada dicionario possui a chave 'Source', 'Value'.\n",
    "num_ratings_per_item = []\n",
    "unique_keys = []\n",
    "rating_sources = []\n",
    "rating_values = []\n",
    "\n",
    "for rating_list in content_auxiliar.Ratings:\n",
    "    num_ratings_per_item.append(len(rating_list))\n",
    "    for rating_dict in rating_list:\n",
    "        for key in rating_dict:\n",
    "            unique_keys.append(key)\n",
    "        rating_sources.append(rating_dict['Source'])\n",
    "        rating_values.append(rating_dict['Value'])\n",
    "\n",
    "set(rating_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "InternetMovieDatabase_list = []\n",
    "Metacritic_list = []\n",
    "RottenTomatoes_list = []\n",
    "for rating_list in content_auxiliar.Ratings:\n",
    "    InternetMovieDatabase_list.append(None)\n",
    "    Metacritic_list.append(None)\n",
    "    RottenTomatoes_list.append(None)\n",
    "    for rating_dict in rating_list:\n",
    "        if rating_dict['Source'] == 'Internet Movie Database':\n",
    "            InternetMovieDatabase_list[-1] = rating_dict['Value']\n",
    "        elif rating_dict['Source'] == 'Metacritic':\n",
    "            Metacritic_list[-1] = rating_dict['Value']\n",
    "        elif rating_dict['Source'] == 'Rotten Tomatoes':\n",
    "            RottenTomatoes_list[-1] = rating_dict['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar['Internet Movie Database'] = InternetMovieDatabase_list\n",
    "content_auxiliar['Metacritic'] = Metacritic_list\n",
    "content_auxiliar['Rotten Tomatoes'] = RottenTomatoes_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar.drop(columns=['Ratings'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apendar a coluna no valor do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ItemId'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_columns = content_auxiliar.columns.to_list()\n",
    "content_columns.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in content_columns:\n",
    "    content_auxiliar[column] = content_auxiliar[column].apply(lambda x: f\"{column}: {x}; \" if x is not None else f\"{column}: unknown value; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_processed = content_auxiliar[['ItemId']].copy()\n",
    "content_processed[\"text\"] = content_auxiliar[content_columns].astype(str).fillna('').agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegando os dados de imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = 'poster_images'\n",
    "\n",
    "# # Lista todos os arquivos na pasta\n",
    "# file_list = os.listdir(folder_path)\n",
    "# item_id_saved = [file.split('=')[1].split('.')[0] for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_to_get_image = content.loc[content.Poster != 'N/A', ['ItemId', 'Poster']].copy()\n",
    "# content_to_get_image = content_to_get_image.loc[~content_to_get_image.ItemId.isin(item_id_saved), :]\n",
    "# content_to_get_image = content_to_get_image.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_image(index):\n",
    "#     image_url = content_to_get_image.loc[index, \"Poster\"]\n",
    "#     item_id = content_to_get_image.loc[index, \"ItemId\"]\n",
    "\n",
    "#     # Fazer a requisição HTTP para baixar a imagem\n",
    "#     # try:\n",
    "#     response = requests.get(image_url)\n",
    "#     # except:\n",
    "#         # return None\n",
    "\n",
    "#     # Verificar se a requisição foi bem-sucedida\n",
    "#     if response.status_code == 200:\n",
    "#         # Abrir a imagem\n",
    "#         img = Image.open(BytesIO(response.content))\n",
    "#         # Salvar a imagem\n",
    "#         img.save(f\"{folder_path}/item_id={item_id}.jpg\")\n",
    "#         return None\n",
    "#     else:\n",
    "#         return (item_id, image_url)\n",
    "\n",
    "# # Lista de índices para processar\n",
    "# indices = content_to_get_image.index.tolist()\n",
    "\n",
    "# # Usar todos os CPUs disponíveis\n",
    "# result_list = Parallel(n_jobs=-1, verbose=100)(delayed(download_image)(index) for index in indices)\n",
    "\n",
    "# # Filtrar os resultados que não conseguiram baixar\n",
    "# nao_conseguiu_baixar = [result for result in result_list if result is None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# folder_path = 'poster_images'\n",
    "# image_files = os.listdir(folder_path)\n",
    "# images_item_ids = [file.split('=')[1].split('.')[0] for file in image_files]\n",
    "\n",
    "# # Initialize a list to store the image matrices\n",
    "# def process_image(image_file):\n",
    "#     image_path = os.path.join(folder_path, image_file)\n",
    "#     image = Image.open(image_path)\n",
    "#     image_matrix = np.array(image)\n",
    "#     return image_matrix.reshape(-1)\n",
    "\n",
    "# # Use Parallel to process images in parallel\n",
    "# image_matrices = Parallel(n_jobs=-1, verbose=100)(delayed(process_image)(image_file) for image_file in image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  E preciso que todas as imagens tenham o mesmo tamanho, para isso vamos adicionar zeros no final das imagens menores\n",
    "\n",
    "# maior_imagem = -1\n",
    "# for image in image_matrices:\n",
    "#     if image.shape[0] > maior_imagem:\n",
    "#         maior_imagem = image.shape[0]\n",
    "\n",
    "# for index, image in enumerate(image_matrices):\n",
    "#     new_image = np.insert(image, len(image), np.zeros(maior_imagem - len(image)))\n",
    "#     del image\n",
    "#     image_matrices[index] = new_image\n",
    "\n",
    "# def add_zeros_to_image(image):\n",
    "#     new_image = np.insert(image, len(image), np.zeros(maior_imagem - len(image)))\n",
    "#     del image\n",
    "#     return new_image\n",
    "\n",
    "# image_matrices = Parallel(n_jobs=-1, verbose=100)(delayed(add_zeros_to_image)(image) for image in image_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  E preciso que todas as imagens tenham o mesmo tamanho, para isso vamos deletar as maoires imagens\n",
    "# menor_imagem = np.Inf\n",
    "# for image in image_matrices:\n",
    "#     if image.shape[0] < menor_imagem:\n",
    "#         menor_imagem = image.shape[0]\n",
    "\n",
    "# def diminui_a_imagem(image):\n",
    "#     return image[:menor_imagem]\n",
    "\n",
    "# image_matrices2 = Parallel(n_jobs=-1, verbose=100)(delayed(diminui_a_imagem)(image) for image in image_matrices)\n",
    "\n",
    "# Convert the list of image matrices to a numpy array\n",
    "# image_matrices2 = np.array(image_matrices2)\n",
    "# image_modality = ImageModality(features=image_matrices2, ids=images_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_text_modality = TextModality(\n",
    "    corpus=content_processed.text.to_list(),\n",
    "    ids=content_processed.ItemId.to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_processed.text.str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import dummy_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cornac.models.cdr.recom_cdr import CDR\n",
    "# from cornac.models.cvae.recom_cvae import CVAE\n",
    "from cornac.models.conv_mf.recom_convmf import ConvMF # tentar de novo\n",
    "# from cornac.models.hft.recom_hft import HFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline():#parameters):\n",
    "    # learning_rate, decay_c, decay_r = parameters\n",
    "    # learning_rate, decay_c, decay_r, epochs, embedding_dim, bert_text_dim, num_factors = parameters\n",
    "\n",
    "    ratio_split = RatioSplit(\n",
    "        data=ratings[['UserId', 'ItemId', 'Rating']].values.tolist(),\n",
    "        val_size=0.2,\n",
    "        test_size=0.2,\n",
    "        exclude_unknowns=True,\n",
    "        verbose=True,\n",
    "        seed=12012001,\n",
    "        rating_threshold=0.5,\n",
    "        item_text=item_text_modality,\n",
    "    )\n",
    "    recommender = ConvMF(\n",
    "        n_epochs = 20,\n",
    "        # learning_rate=learning_rate,\n",
    "        # decay_c=decay_c,\n",
    "        # decay_r=decay_r,\n",
    "        # epochs=epochs,\n",
    "        # embedding_dim=embedding_dim,\n",
    "        # bert_text_dim=bert_text_dim,\n",
    "        # num_factors=num_factors,\n",
    "        verbose=True,\n",
    "    )\n",
    "    experiment = cornac.Experiment(\n",
    "        eval_method=ratio_split,\n",
    "        models=[recommender], metrics=[NDCG(20)]\n",
    "    )\n",
    "    experiment_returned = experiment.run()\n",
    "    experiment = cornac.Experiment(\n",
    "        eval_method=ratio_split,\n",
    "        models=[recommender], metrics=[NDCG(20)]\n",
    "    )\n",
    "    experiment_returned = experiment.run()\n",
    "    return -1 * experiment.result[0].metric_avg_results['NDCG@20'], recommender, experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating_threshold = 0.5\n",
      "exclude_unknowns = True\n",
      "---\n",
      "Training data:\n",
      "Number of users = 41203\n",
      "Number of items = 23803\n",
      "Number of ratings = 395832\n",
      "Max rating = 10.0\n",
      "Min rating = 0.0\n",
      "Global mean = 7.3\n",
      "---\n",
      "Test data:\n",
      "Number of users = 41203\n",
      "Number of items = 23803\n",
      "Number of ratings = 122629\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Validation data:\n",
      "Number of users = 41203\n",
      "Number of items = 23803\n",
      "Number of ratings = 122459\n",
      "---\n",
      "Total users = 41203\n",
      "Total items = 23803\n",
      "\n",
      "[ConvMF] Training started!\n",
      "Epoch: 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing CNN: 100%|██████████| 5/5 [03:39<00:00, 43.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 18902022481.53336 Elapsed: 258.9615s Converge: 1890202248153335621083921260769002823599427888322997315960832.000000 \n",
      "Epoch: 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing CNN: 100%|██████████| 5/5 [03:10<00:00, 38.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3237600961.02908 Elapsed: 2146.8272s Converge: 0.828717 \n",
      "Epoch: 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing CNN: 100%|██████████| 5/5 [03:09<00:00, 37.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1808915720.93850 Elapsed: 1922.3001s Converge: 0.441279 \n",
      "Epoch: 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing CNN: 100%|██████████| 5/5 [03:10<00:00, 38.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1616844336.08673 Elapsed: 2008.3668s Converge: 0.106180 \n",
      "Epoch: 5/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m metric, recommender, experiment \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 30\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m recommender \u001b[38;5;241m=\u001b[39m ConvMF(\n\u001b[1;32m     16\u001b[0m     n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# learning_rate=learning_rate,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m experiment \u001b[38;5;241m=\u001b[39m cornac\u001b[38;5;241m.\u001b[39mExperiment(\n\u001b[1;32m     27\u001b[0m     eval_method\u001b[38;5;241m=\u001b[39mratio_split,\n\u001b[1;32m     28\u001b[0m     models\u001b[38;5;241m=\u001b[39m[recommender], metrics\u001b[38;5;241m=\u001b[39m[NDCG(\u001b[38;5;241m20\u001b[39m)]\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m experiment_returned \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m experiment \u001b[38;5;241m=\u001b[39m cornac\u001b[38;5;241m.\u001b[39mExperiment(\n\u001b[1;32m     32\u001b[0m     eval_method\u001b[38;5;241m=\u001b[39mratio_split,\n\u001b[1;32m     33\u001b[0m     models\u001b[38;5;241m=\u001b[39m[recommender], metrics\u001b[38;5;241m=\u001b[39m[NDCG(\u001b[38;5;241m20\u001b[39m)]\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m experiment_returned \u001b[38;5;241m=\u001b[39m experiment\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/miniconda/envs/sistema_recomendacao_tp2/lib/python3.9/site-packages/cornac/experiment/experiment.py:142\u001b[0m, in \u001b[0;36mExperiment.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m         model\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[0;32m--> 142\u001b[0m     test_result, val_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_based\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_based\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mappend(test_result)\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/sistema_recomendacao_tp2/lib/python3.9/site-packages/cornac/eval_methods/base_method.py:734\u001b[0m, in \u001b[0;36mBaseMethod.evaluate\u001b[0;34m(self, model, metrics, user_based, show_validation)\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] Training started!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model\u001b[38;5;241m.\u001b[39mname))\n\u001b[1;32m    733\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 734\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m##############\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# EVALUATION #\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m##############\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/sistema_recomendacao_tp2/lib/python3.9/site-packages/cornac/models/conv_mf/recom_convmf.py:168\u001b[0m, in \u001b[0;36mConvMF.fit\u001b[0;34m(self, train_set, val_set)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(train_set)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_convmf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/envs/sistema_recomendacao_tp2/lib/python3.9/site-packages/cornac/models/conv_mf/recom_convmf.py:258\u001b[0m, in \u001b[0;36mConvMF._fit_convmf\u001b[0;34m(self, train_set)\u001b[0m\n\u001b[1;32m    256\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_u \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk) \u001b[38;5;241m+\u001b[39m V_i\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(V_i)\n\u001b[1;32m    257\u001b[0m     B \u001b[38;5;241m=\u001b[39m (V_i \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39mtile(R_i, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT))\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU[i] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     user_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_u \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU[i])\n\u001b[1;32m    262\u001b[0m item_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda/envs/sistema_recomendacao_tp2/lib/python3.9/site-packages/numpy/linalg/linalg.py:400\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    398\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdd->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    399\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 400\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(r\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metric, recommender, experiment = pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = [\n",
    "    # (0.00001, 0.1),  # learning_rate\n",
    "    # (0.001, 1),  # decay_c\n",
    "    # (0.0001, 0.1),  # decay_r\n",
    "    # [20, 30, 40, 50], # epochs \n",
    "    # [40, 128, 200, 300, 500],  # embedding_dim\n",
    "    # [100, 383, 600, 800, 1000],  # bert_text_dim\n",
    "    # [2, 4, 8],  # num_factors\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = dummy_minimize(DMR_pipeline, space, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put everything together into an experiment and run it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_score(user_id):\n",
    "#     target_line = pd.DataFrame()\n",
    "#     user_index = ratio_split.train_set.uid_map.get(user_id)\n",
    "\n",
    "#     columns = [\"UserId\", \"ItemId\", \"Rating\"]\n",
    "#     items_to_predict = targets.loc[targets.UserId == user_id, \"ItemId\"].to_list()\n",
    "#     target_line[\"ItemId\"] = items_to_predict\n",
    "#     target_line[\"UserId\"] = user_id\n",
    "#     target_line[\"Rating\"] = -1\n",
    "#     if user_index is None:\n",
    "#         return target_line[columns]\n",
    "\n",
    "#     # Get the train dataframe index of the items to predict\n",
    "#     items_to_predict_index = np.array([ratio_split.train_set.iid_map.get(item_id) for item_id in items_to_predict])\n",
    "#     items_to_predict_tensor = torch.tensor([idx for idx in items_to_predict_index if idx is not None])\n",
    "\n",
    "#     # Get the position of items that are not in the train set\n",
    "#     none_indices = [i for i, x in enumerate(items_to_predict_index) if x is None]\n",
    "\n",
    "#     # Get the prediction for the items\n",
    "#     line_rating = dmrl_recommender.score(user_index=user_index, item_indices=items_to_predict_tensor)\n",
    "\n",
    "#     # Insert -1 in the position of items that are not in the train set\n",
    "#     for index_to_insert in none_indices:\n",
    "#         line_rating = np.insert(line_rating, index_to_insert, -1)\n",
    "\n",
    "#     # Insert the prediction in the target_prediction dataframe\n",
    "#     target_line[\"Rating\"] = line_rating\n",
    "#     return target_line[columns]\n",
    "\n",
    "# user_id_list = targets.UserId.unique()\n",
    "# target_prediction_list = Parallel(n_jobs=-1, verbose=100)(delayed(compute_score)(user_id) for user_id in user_id_list)\n",
    "# target_prediction = pd.concat(target_prediction_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = targets.copy()\n",
    "target_prediction[\"Rating\"] = -1\n",
    "\n",
    "user_id_list = targets.UserId.unique()\n",
    "for user_id in user_id_list:\n",
    "    # Get the train dataframe index of the user to predict\n",
    "    user_index = ratio_split.train_set.uid_map.get(user_id)\n",
    "\n",
    "    if user_index is None:\n",
    "        print(f\"User {user_id} is not in the train set\")\n",
    "        continue\n",
    "\n",
    "    # Flter by items to predict \n",
    "    items_to_predict = targets.loc[targets.UserId == user_id, \"ItemId\"].to_list()\n",
    "\n",
    "    # Get the train dataframe index of the items to predict\n",
    "    items_to_predict_index = np.array([ratio_split.train_set.iid_map.get(item_id) for item_id in items_to_predict])\n",
    "\n",
    "    items_to_predict_tensor = torch.tensor([idx for idx in items_to_predict_index if idx is not None])\n",
    "\n",
    "    # Get the position of items that are not in the train set\n",
    "    none_indices = [i for i, x in enumerate(items_to_predict_index) if x is None]\n",
    "\n",
    "    # Get the prediction for the items\n",
    "    line_rating = dmrl_recommender.score(user_index=user_index, item_indices=items_to_predict_tensor)\n",
    "\n",
    "    # Insert -1 in the position of items that are not in the train set\n",
    "    for index_to_insert in none_indices:\n",
    "        line_rating = np.insert(line_rating, index_to_insert, -1)\n",
    "\n",
    "    # Insert the prediction in the target_prediction dataframe\n",
    "    target_prediction.loc[targets.UserId == user_id, \"Rating\"] = line_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = target_prediction.sort_values([\"UserId\", \"Rating\"], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction.to_csv(\"HFT_versao_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = target_prediction.drop(columns=\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction.to_csv(\"HFT_versao_1_sem_rating.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sistema_recomendacao_tp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
