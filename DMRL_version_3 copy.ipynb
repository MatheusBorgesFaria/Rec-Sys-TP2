{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disentangled Multimodal Representation Learning for Recommendation (DMRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideias:\n",
    "- FEITO: Usar a coluna para no texto\n",
    "- FEITO: Limpar os dados de conteudo\n",
    "- FEITO (lucas): ordenar os itens novos que ficaram por ultimo por popularidade\n",
    "- FEITO: Adicionar o nome da coluna, porque vai indicar o assunto, em conjunto do texto\n",
    "\n",
    "- tunar os hiperparametros\n",
    "- usa a imagem alem do texto\n",
    "- usar algum algoritmo mais simples apenas para usuarios novos, knn para content based\n",
    "- Entender o parametro exclude_unknowns=True do RatioSplit e se tem alguma forma do DMRL gerar previsoes para itens e usuarios novos \n",
    "\n",
    "- paralelizar a previsao\n",
    "Perguntas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus/miniconda/envs/sistema_recomendacao_tp2_versao3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import cornac\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cornac.metrics import NDCG\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.data import TextModality\n",
    "from cornac.models.dmrl.recom_dmrl import DMRL, ImageModality\n",
    "from tqdm import tqdm\n",
    "from utils import load_data, preprocessing_content_data\n",
    "# import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings, content, targets = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[\"TimestampDate\"] = ratings['Timestamp'].dt.date\n",
    "ratings.loc[ratings.Rating == 0, \"Rating\"] = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpar os dados\n",
    "\n",
    "- FEITO - Year: deletar o - no texto '2013–'\n",
    "- FEITO - Rated: alterar as diversas formas de escrever NA para NA. Esse e um caso especial\n",
    "- FEITO - alterar as diversas formas de escrever NA para None em todas as coluans\n",
    "- FEITO - Language: tem varias linhas que possuem bizarices como  'None, English' e 'None, French' , 'English, None'...\n",
    "- FEITO - Ratings: criar uma coluna para cada chave do dicionario, entender quais sao todas as chaves que existem\n",
    "\n",
    "-----------------------------------------------\n",
    "Variaveis qwue nao precisariam ser tratadas com um bert:\n",
    "- Metascore\n",
    "- imdbRating\n",
    "- Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar = content.drop(columns=[\"Poster\", \"Website\", \"Response\", \"Episode\", \"seriesID\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar['Year'] = content_auxiliar['Year'].str.replace('–', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = content.totalSeasons.unique()[0]\n",
    "dict_transform_to_na = {\n",
    "    \"Rated\":['N/A', 'Not Rated', 'Unrated', 'UNRATED', 'NOT RATED'],\n",
    "    \"all\": [nan, 'N/A', 'None', np.nan],\n",
    "}\n",
    "\n",
    "for na_value in dict_transform_to_na[\"all\"]:\n",
    "    content_auxiliar = content_auxiliar.replace(na_value, None)\n",
    "\n",
    "for na_value in dict_transform_to_na[\"Rated\"]:\n",
    "    content_auxiliar['Rated'] = content_auxiliar['Rated'].replace(na_value, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar['Language'] = content_auxiliar['Language'].str.replace('None, ', '')\n",
    "content_auxiliar['Language'] = content_auxiliar['Language'].str.replace(', None', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Internet Movie Database', 'Metacritic', 'Rotten Tomatoes'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entendendo os valores possiveis para a coluna Ratings\n",
    "# A coluna content_auxiliar.Ratings quarda uma lista que posde ter entre 0 e 3 dicionarios. Cada dicionario possui a chave 'Source', 'Value'.\n",
    "num_ratings_per_item = []\n",
    "unique_keys = []\n",
    "rating_sources = []\n",
    "rating_values = []\n",
    "\n",
    "for rating_list in content_auxiliar.Ratings:\n",
    "    num_ratings_per_item.append(len(rating_list))\n",
    "    for rating_dict in rating_list:\n",
    "        for key in rating_dict:\n",
    "            unique_keys.append(key)\n",
    "        rating_sources.append(rating_dict['Source'])\n",
    "        rating_values.append(rating_dict['Value'])\n",
    "\n",
    "set(rating_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "InternetMovieDatabase_list = []\n",
    "Metacritic_list = []\n",
    "RottenTomatoes_list = []\n",
    "for rating_list in content_auxiliar.Ratings:\n",
    "    InternetMovieDatabase_list.append(None)\n",
    "    Metacritic_list.append(None)\n",
    "    RottenTomatoes_list.append(None)\n",
    "    for rating_dict in rating_list:\n",
    "        if rating_dict['Source'] == 'Internet Movie Database':\n",
    "            InternetMovieDatabase_list[-1] = rating_dict['Value']\n",
    "        elif rating_dict['Source'] == 'Metacritic':\n",
    "            Metacritic_list[-1] = rating_dict['Value']\n",
    "        elif rating_dict['Source'] == 'Rotten Tomatoes':\n",
    "            RottenTomatoes_list[-1] = rating_dict['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar['Internet Movie Database'] = InternetMovieDatabase_list\n",
    "content_auxiliar['Metacritic'] = Metacritic_list\n",
    "content_auxiliar['Rotten Tomatoes'] = RottenTomatoes_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar.drop(columns=['Ratings'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apendar a coluna no valor do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ItemId'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_columns = content_auxiliar.columns.to_list()\n",
    "content_columns.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in content_columns:\n",
    "    content_auxiliar[column] = content_auxiliar[column].apply(lambda x: f\"{column}: {x}; \" if x is not None else f\"{column}: unknown value; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_processed = content_auxiliar[['ItemId']].copy()\n",
    "content_processed[\"text\"] = content_auxiliar[content_columns].astype(str).fillna('').agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegando os dados de imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'poster_images'\n",
    "\n",
    "# Lista todos os arquivos na pasta\n",
    "file_list = os.listdir(folder_path)\n",
    "item_id_saved = [file.split('=')[1].split('.')[0] for file in file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_to_get_image = content.loc[content.Poster != 'N/A', ['ItemId', 'Poster']].copy()\n",
    "content_to_get_image = content_to_get_image.loc[~content_to_get_image.ItemId.isin(item_id_saved), :]\n",
    "content_to_get_image = content_to_get_image.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_image(index):\n",
    "    image_url = content_to_get_image.loc[index, \"Poster\"]\n",
    "    item_id = content_to_get_image.loc[index, \"ItemId\"]\n",
    "\n",
    "    # Fazer a requisição HTTP para baixar a imagem\n",
    "    # try:\n",
    "    response = requests.get(image_url)\n",
    "    # except:\n",
    "        # return None\n",
    "\n",
    "    # Verificar se a requisição foi bem-sucedida\n",
    "    if response.status_code == 200:\n",
    "        # Abrir a imagem\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        # Salvar a imagem\n",
    "        img.save(f\"{folder_path}/item_id={item_id}.jpg\")\n",
    "        return None\n",
    "    else:\n",
    "        return (item_id, image_url)\n",
    "\n",
    "# Lista de índices para processar\n",
    "indices = content_to_get_image.index.tolist()\n",
    "\n",
    "# Usar todos os CPUs disponíveis\n",
    "result_list = Parallel(n_jobs=-1, verbose=100)(delayed(download_image)(index) for index in indices)\n",
    "\n",
    "# Filtrar os resultados que não conseguiram baixar\n",
    "nao_conseguiu_baixar = [result for result in result_list if result is None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cornac\n",
    "from cornac.data import TextModality, ImageModality\n",
    "from cornac.datasets import amazon_clothing\n",
    "from cornac.eval_methods import RatioSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features, image_item_ids = amazon_clothing.load_visual_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_modality = ImageModality(features=image_features, ids=image_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3393, 4096)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "folder_path = 'poster_images'\n",
    "image_files = os.listdir(folder_path)\n",
    "images_item_ids = [file.split('=')[1].split('.')[0] for file in image_files]\n",
    "\n",
    "# Initialize a list to store the image matrices\n",
    "def process_image(image_file):\n",
    "    image_path = os.path.join(folder_path, image_file)\n",
    "    image = Image.open(image_path)\n",
    "    image_matrix = np.array(image)\n",
    "    return image_matrix.reshape(-1)\n",
    "\n",
    "# Use Parallel to process images in parallel\n",
    "image_matrices = Parallel(n_jobs=-1, verbose=100)(delayed(process_image)(image_file) for image_file in image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  E preciso que todas as imagens tenham o mesmo tamanho, para isso vamos adicionar zeros no final das imagens menores\n",
    "\n",
    "# maior_imagem = -1\n",
    "# for image in image_matrices:\n",
    "#     if image.shape[0] > maior_imagem:\n",
    "#         maior_imagem = image.shape[0]\n",
    "\n",
    "# for index, image in enumerate(image_matrices):\n",
    "#     new_image = np.insert(image, len(image), np.zeros(maior_imagem - len(image)))\n",
    "#     del image\n",
    "#     image_matrices[index] = new_image\n",
    "\n",
    "# def add_zeros_to_image(image):\n",
    "#     new_image = np.insert(image, len(image), np.zeros(maior_imagem - len(image)))\n",
    "#     del image\n",
    "#     return new_image\n",
    "\n",
    "# image_matrices = Parallel(n_jobs=-1, verbose=100)(delayed(add_zeros_to_image)(image) for image in image_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  E preciso que todas as imagens tenham o mesmo tamanho, para isso vamos deletar as maoires imagens\n",
    "menor_imagem = np.Inf\n",
    "for image in image_matrices:\n",
    "    if image.shape[0] < menor_imagem:\n",
    "        menor_imagem = image.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diminui_a_imagem(image):\n",
    "    return image[:menor_imagem]\n",
    "\n",
    "image_matrices2 = Parallel(n_jobs=-1, verbose=100)(delayed(diminui_a_imagem)(image) for image in image_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of image matrices to a numpy array\n",
    "image_matrices2 = np.array(image_matrices2)\n",
    "\n",
    "# print(f\"Loaded {len(image_matrices)} images into a numpy array with shape {image_matrices.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_modality = ImageModality(features=image_matrices2, ids=images_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_text_modality = TextModality(\n",
    "    corpus=content_processed.text.to_list(),\n",
    "    ids=content_processed.ItemId.to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating_threshold = 0.5\n",
      "exclude_unknowns = True\n",
      "---\n",
      "Training data:\n",
      "Number of users = 46750\n",
      "Number of items = 27045\n",
      "Number of ratings = 527776\n",
      "Max rating = 10.0\n",
      "Min rating = 0.0\n",
      "Global mean = 7.3\n",
      "---\n",
      "Test data:\n",
      "Number of users = 46750\n",
      "Number of items = 27045\n",
      "Number of ratings = 124031\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Total users = 46750\n",
      "Total items = 27045\n"
     ]
    }
   ],
   "source": [
    "ratio_split = RatioSplit(\n",
    "    data=ratings[['UserId', 'ItemId', 'Rating']].values.tolist(),\n",
    "    test_size=0.2,\n",
    "    exclude_unknowns=True,\n",
    "    verbose=True,\n",
    "    seed=12012001,\n",
    "    rating_threshold=0.5,\n",
    "    item_text=item_text_modality,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate DMRL recommender\n",
    "# dmrl_recommender = DMRL(\n",
    "#     batch_size=4096,\n",
    "#     epochs=20,\n",
    "#     log_metrics=False,\n",
    "#     learning_rate=0.01,\n",
    "#     num_factors=2,\n",
    "#     decay_r=0.5,\n",
    "#     decay_c=0.01,\n",
    "#     num_neg=3,\n",
    "#     embedding_dim=100,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmrl_recommender = DMRL(\n",
    "    batch_size=1024,\n",
    "    epochs=20,\n",
    "    log_metrics=False,\n",
    "    learning_rate=0.1,\n",
    "    num_factors=2,\n",
    "    decay_r=0.0001,\n",
    "    decay_c=0.001,\n",
    "    num_neg=4,\n",
    "    embedding_dim=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DMRL] Training started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus/miniconda/envs/sistema_recomendacao_tp2_versao3/lib/python3.9/site-packages/cornac/models/recommender.py:322: UserWarning: Model is already fitted. Re-fitting will overwrite the previous model.\n",
      "  warnings.warn(\n",
      "/home/matheus/miniconda/envs/sistema_recomendacao_tp2_versao3/lib/python3.9/site-packages/cornac/models/dmrl/transformer_text.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_ids = torch.load(id_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-encoding the entire corpus. This might take a while.\n",
      "Using device cuda for training\n",
      "  batch 5 loss: 713.9724731445312\n",
      "  batch 10 loss: 739.7259643554687\n",
      "  batch 15 loss: 782.1755249023438\n",
      "  batch 20 loss: 835.6427978515625\n",
      "  batch 25 loss: 780.5334838867187\n",
      "  batch 30 loss: 722.6097534179687\n",
      "  batch 35 loss: 705.4067993164062\n",
      "  batch 40 loss: 693.2071166992188\n",
      "  batch 45 loss: 690.8145629882813\n",
      "  batch 50 loss: 670.7322875976563\n",
      "  batch 55 loss: 666.0890258789062\n",
      "  batch 60 loss: 667.0150512695312\n",
      "  batch 65 loss: 645.9036376953125\n",
      "  batch 70 loss: 640.0469970703125\n",
      "  batch 75 loss: 624.4564453125\n",
      "  batch 80 loss: 622.3016845703125\n",
      "  batch 85 loss: 619.26552734375\n",
      "  batch 90 loss: 621.0539672851562\n",
      "  batch 95 loss: 608.0799438476563\n",
      "  batch 100 loss: 614.8724731445312\n",
      "  batch 105 loss: 598.6807006835937\n",
      "  batch 110 loss: 600.4018676757812\n",
      "  batch 115 loss: 587.0396606445313\n",
      "  batch 120 loss: 602.9758666992187\n",
      "  batch 125 loss: 590.5377075195313\n",
      "  batch 130 loss: 580.2975830078125\n",
      "  batch 135 loss: 584.0286499023438\n",
      "  batch 140 loss: 582.1665161132812\n",
      "  batch 145 loss: 584.2012939453125\n",
      "  batch 150 loss: 584.030859375\n",
      "  batch 155 loss: 573.380322265625\n",
      "  batch 160 loss: 569.8539916992188\n",
      "  batch 165 loss: 572.5861572265625\n",
      "  batch 170 loss: 576.390869140625\n",
      "  batch 175 loss: 576.104052734375\n",
      "  batch 180 loss: 577.7125122070313\n",
      "  batch 185 loss: 585.0945556640625\n",
      "  batch 190 loss: 558.159716796875\n",
      "  batch 195 loss: 568.9560180664063\n",
      "  batch 200 loss: 577.7991943359375\n",
      "  batch 205 loss: 582.5945922851563\n",
      "  batch 210 loss: 561.648486328125\n",
      "  batch 215 loss: 572.8515747070312\n",
      "  batch 220 loss: 570.722412109375\n",
      "  batch 225 loss: 572.5957885742188\n",
      "  batch 230 loss: 578.0977172851562\n",
      "  batch 235 loss: 562.7385131835938\n",
      "  batch 240 loss: 576.6788696289062\n",
      "  batch 245 loss: 561.771240234375\n",
      "  batch 250 loss: 555.2357666015625\n",
      "  batch 255 loss: 538.804443359375\n",
      "  batch 260 loss: 562.3134765625\n",
      "  batch 265 loss: 542.429931640625\n",
      "  batch 270 loss: 553.4166137695313\n",
      "  batch 275 loss: 550.0597534179688\n",
      "  batch 280 loss: 531.091455078125\n",
      "  batch 285 loss: 535.888525390625\n",
      "  batch 290 loss: 535.9558837890625\n",
      "  batch 295 loss: 523.1192321777344\n",
      "  batch 300 loss: 521.1081848144531\n",
      "  batch 305 loss: 519.165087890625\n",
      "  batch 310 loss: 505.9724853515625\n",
      "  batch 315 loss: 509.6893798828125\n",
      "  batch 320 loss: 502.3503173828125\n",
      "  batch 325 loss: 505.58367309570315\n",
      "  batch 330 loss: 506.8069702148438\n",
      "  batch 335 loss: 496.00361328125\n",
      "  batch 340 loss: 503.1474914550781\n",
      "  batch 345 loss: 496.8314697265625\n",
      "  batch 350 loss: 490.46876831054686\n",
      "  batch 355 loss: 486.85810546875\n",
      "  batch 360 loss: 492.00792236328124\n",
      "  batch 365 loss: 468.0181396484375\n",
      "  batch 370 loss: 470.3078125\n",
      "  batch 375 loss: 475.9930847167969\n",
      "  batch 380 loss: 469.4609680175781\n",
      "  batch 385 loss: 469.40947875976565\n",
      "  batch 390 loss: 466.0767333984375\n",
      "  batch 395 loss: 465.378662109375\n",
      "  batch 400 loss: 473.9405578613281\n",
      "  batch 405 loss: 461.2044372558594\n",
      "  batch 410 loss: 466.70362548828126\n",
      "  batch 415 loss: 460.05572509765625\n",
      "  batch 420 loss: 446.46103515625\n",
      "  batch 425 loss: 475.2879333496094\n",
      "  batch 430 loss: 454.9240661621094\n",
      "  batch 435 loss: 460.4567565917969\n",
      "  batch 440 loss: 466.7293212890625\n",
      "  batch 445 loss: 476.4905944824219\n",
      "  batch 450 loss: 449.2473571777344\n",
      "  batch 455 loss: 465.3098876953125\n",
      "  batch 460 loss: 449.11272583007815\n",
      "  batch 465 loss: 455.6142822265625\n",
      "  batch 470 loss: 470.223681640625\n",
      "  batch 475 loss: 449.4839172363281\n",
      "  batch 480 loss: 455.02122192382814\n",
      "  batch 485 loss: 440.7861572265625\n",
      "  batch 490 loss: 452.2846984863281\n",
      "  batch 495 loss: 461.6917663574219\n",
      "  batch 500 loss: 450.83429565429685\n",
      "  batch 505 loss: 460.1436340332031\n",
      "  batch 510 loss: 475.62611694335936\n",
      "  batch 515 loss: 455.1819152832031\n",
      "Epoch: 0 is done\n",
      "  batch 5 loss: 365.58628540039064\n",
      "  batch 10 loss: 364.6868103027344\n",
      "  batch 15 loss: 376.14073486328124\n",
      "  batch 20 loss: 369.3124938964844\n",
      "  batch 25 loss: 370.63988037109374\n",
      "  batch 30 loss: 362.43024291992185\n",
      "  batch 35 loss: 377.9564270019531\n",
      "  batch 40 loss: 352.0572509765625\n",
      "  batch 45 loss: 373.90244140625\n",
      "  batch 50 loss: 370.8853515625\n",
      "  batch 55 loss: 366.6293884277344\n",
      "  batch 60 loss: 362.31212158203124\n",
      "  batch 65 loss: 343.07355346679685\n",
      "  batch 70 loss: 368.70423583984376\n",
      "  batch 75 loss: 358.3145812988281\n",
      "  batch 80 loss: 363.2002014160156\n",
      "  batch 85 loss: 361.97832641601565\n",
      "  batch 90 loss: 361.4101257324219\n",
      "  batch 95 loss: 350.29155883789065\n",
      "  batch 100 loss: 362.8261352539063\n",
      "  batch 105 loss: 365.6140441894531\n",
      "  batch 110 loss: 337.395166015625\n",
      "  batch 115 loss: 372.78387451171875\n",
      "  batch 120 loss: 375.3225341796875\n",
      "  batch 125 loss: 369.9948791503906\n",
      "  batch 130 loss: 375.4829895019531\n",
      "  batch 135 loss: 362.3658935546875\n",
      "  batch 140 loss: 366.25396728515625\n",
      "  batch 145 loss: 353.45994873046874\n",
      "  batch 150 loss: 366.7422790527344\n",
      "  batch 155 loss: 352.86259765625\n",
      "  batch 160 loss: 366.2793212890625\n",
      "  batch 165 loss: 361.9148376464844\n",
      "  batch 170 loss: 376.7352783203125\n",
      "  batch 175 loss: 384.0242858886719\n",
      "  batch 180 loss: 367.5006408691406\n",
      "  batch 185 loss: 352.81248779296874\n",
      "  batch 190 loss: 378.60341796875\n",
      "  batch 195 loss: 374.1885620117188\n",
      "  batch 200 loss: 374.71776123046874\n",
      "  batch 205 loss: 369.4623596191406\n",
      "  batch 210 loss: 356.6930358886719\n",
      "  batch 215 loss: 360.2095153808594\n",
      "  batch 220 loss: 371.7324584960937\n",
      "  batch 225 loss: 365.70142822265626\n",
      "  batch 230 loss: 372.6598754882813\n",
      "  batch 235 loss: 366.7800537109375\n",
      "  batch 240 loss: 356.9649169921875\n",
      "  batch 245 loss: 361.70587768554685\n",
      "  batch 250 loss: 356.709912109375\n",
      "  batch 255 loss: 365.1659240722656\n",
      "  batch 260 loss: 357.34038696289065\n",
      "  batch 265 loss: 353.8763671875\n",
      "  batch 270 loss: 358.4932556152344\n",
      "  batch 275 loss: 356.7271484375\n",
      "  batch 280 loss: 364.8203430175781\n",
      "  batch 285 loss: 361.4520751953125\n",
      "  batch 290 loss: 356.49134521484376\n",
      "  batch 295 loss: 362.4989807128906\n",
      "  batch 300 loss: 352.8627624511719\n",
      "  batch 305 loss: 358.806103515625\n",
      "  batch 310 loss: 373.7291015625\n",
      "  batch 315 loss: 363.06650390625\n",
      "  batch 320 loss: 363.90411987304685\n",
      "  batch 325 loss: 361.32764892578126\n",
      "  batch 330 loss: 371.1569091796875\n",
      "  batch 335 loss: 373.1634460449219\n",
      "  batch 340 loss: 364.90350341796875\n",
      "  batch 345 loss: 356.16790771484375\n",
      "  batch 350 loss: 372.3533203125\n",
      "  batch 355 loss: 352.19459228515626\n",
      "  batch 360 loss: 361.76954345703126\n",
      "  batch 365 loss: 374.0046875\n",
      "  batch 370 loss: 345.19725341796874\n",
      "  batch 375 loss: 352.9790771484375\n",
      "  batch 380 loss: 340.136474609375\n",
      "  batch 385 loss: 361.0200500488281\n",
      "  batch 390 loss: 349.0674072265625\n",
      "  batch 395 loss: 364.789697265625\n",
      "  batch 400 loss: 371.64580078125\n",
      "  batch 405 loss: 358.11412353515624\n",
      "  batch 410 loss: 367.83128662109374\n",
      "  batch 415 loss: 346.31773071289064\n",
      "  batch 420 loss: 349.73358154296875\n",
      "  batch 425 loss: 361.47216796875\n",
      "  batch 430 loss: 344.7326354980469\n",
      "  batch 435 loss: 357.7103332519531\n",
      "  batch 440 loss: 356.28160400390624\n",
      "  batch 445 loss: 347.7661499023437\n",
      "  batch 450 loss: 346.3637451171875\n",
      "  batch 455 loss: 362.3446105957031\n",
      "  batch 460 loss: 344.86025390625\n",
      "  batch 465 loss: 359.04921875\n",
      "  batch 470 loss: 354.155810546875\n",
      "  batch 475 loss: 354.4528747558594\n",
      "  batch 480 loss: 341.8391357421875\n",
      "  batch 485 loss: 353.02989501953124\n",
      "  batch 490 loss: 345.78431396484376\n",
      "  batch 495 loss: 341.7403076171875\n",
      "  batch 500 loss: 355.02870483398436\n",
      "  batch 505 loss: 358.192822265625\n",
      "  batch 510 loss: 358.6838317871094\n",
      "  batch 515 loss: 354.41904296875\n",
      "Epoch: 1 is done\n",
      "  batch 5 loss: 249.2320556640625\n",
      "  batch 10 loss: 244.9484832763672\n",
      "  batch 15 loss: 237.01700744628906\n",
      "  batch 20 loss: 240.94131469726562\n",
      "  batch 25 loss: 235.6277618408203\n",
      "  batch 30 loss: 242.705224609375\n",
      "  batch 35 loss: 247.83759155273438\n",
      "  batch 40 loss: 250.45548095703126\n",
      "  batch 45 loss: 243.72726440429688\n",
      "  batch 50 loss: 235.9489013671875\n",
      "  batch 55 loss: 251.4799072265625\n",
      "  batch 60 loss: 250.56695556640625\n",
      "  batch 65 loss: 258.8623382568359\n",
      "  batch 70 loss: 252.0559509277344\n",
      "  batch 75 loss: 257.0134307861328\n",
      "  batch 80 loss: 244.58102416992188\n",
      "  batch 85 loss: 243.6451629638672\n",
      "  batch 90 loss: 262.25628356933595\n",
      "  batch 95 loss: 260.5850006103516\n",
      "  batch 100 loss: 243.12012634277343\n",
      "  batch 105 loss: 245.88560791015624\n",
      "  batch 110 loss: 250.66888427734375\n",
      "  batch 115 loss: 251.50608520507814\n",
      "  batch 120 loss: 254.80286254882813\n",
      "  batch 125 loss: 248.53387756347655\n",
      "  batch 130 loss: 264.19561767578125\n",
      "  batch 135 loss: 250.07793579101562\n",
      "  batch 140 loss: 243.5980010986328\n",
      "  batch 145 loss: 273.04217529296875\n",
      "  batch 150 loss: 250.4981475830078\n",
      "  batch 155 loss: 246.45457458496094\n",
      "  batch 160 loss: 248.52408142089843\n",
      "  batch 165 loss: 264.71055908203124\n",
      "  batch 170 loss: 265.7830810546875\n",
      "  batch 175 loss: 255.99903564453126\n",
      "  batch 180 loss: 246.47804565429686\n",
      "  batch 185 loss: 261.79254150390625\n",
      "  batch 190 loss: 248.41085815429688\n",
      "  batch 195 loss: 260.7660339355469\n",
      "  batch 200 loss: 269.47593688964844\n",
      "  batch 205 loss: 244.35792236328126\n",
      "  batch 210 loss: 259.80621337890625\n",
      "  batch 215 loss: 267.5028045654297\n",
      "  batch 220 loss: 267.05808410644534\n",
      "  batch 225 loss: 269.3431640625\n",
      "  batch 230 loss: 283.1877471923828\n",
      "  batch 235 loss: 287.7806793212891\n",
      "  batch 240 loss: 270.22844848632815\n",
      "  batch 245 loss: 262.55800170898436\n",
      "  batch 250 loss: 282.36915283203126\n",
      "  batch 255 loss: 267.04344482421874\n",
      "  batch 260 loss: 270.3239501953125\n",
      "  batch 265 loss: 277.64519653320315\n",
      "  batch 270 loss: 264.12738647460935\n",
      "  batch 275 loss: 274.58424987792966\n",
      "  batch 280 loss: 259.80312194824216\n",
      "  batch 285 loss: 272.0307189941406\n",
      "  batch 290 loss: 309.87781982421876\n",
      "  batch 295 loss: 302.9274536132813\n",
      "  batch 300 loss: 314.3640441894531\n",
      "  batch 305 loss: 280.87952880859376\n",
      "  batch 310 loss: 285.5480529785156\n",
      "  batch 315 loss: 302.7005676269531\n",
      "  batch 320 loss: 278.4018249511719\n",
      "  batch 325 loss: 288.811279296875\n",
      "  batch 330 loss: 295.5273193359375\n",
      "  batch 335 loss: 274.294873046875\n",
      "  batch 340 loss: 288.80811767578126\n",
      "  batch 345 loss: 294.0683654785156\n",
      "  batch 350 loss: 270.0591186523437\n",
      "  batch 355 loss: 286.1211242675781\n",
      "  batch 360 loss: 269.99908447265625\n",
      "  batch 365 loss: 286.21657104492186\n",
      "  batch 370 loss: 301.8398071289063\n",
      "  batch 375 loss: 289.45908203125\n",
      "  batch 380 loss: 284.8112060546875\n",
      "  batch 385 loss: 287.3561584472656\n",
      "  batch 390 loss: 297.28675537109376\n",
      "  batch 395 loss: 273.21002197265625\n",
      "  batch 400 loss: 286.792236328125\n",
      "  batch 405 loss: 272.3961639404297\n",
      "  batch 410 loss: 278.16961669921875\n",
      "  batch 415 loss: 293.0267700195312\n",
      "  batch 420 loss: 261.419970703125\n",
      "  batch 425 loss: 282.3669372558594\n",
      "  batch 430 loss: 286.79221496582034\n",
      "  batch 435 loss: 284.5438720703125\n",
      "  batch 440 loss: 277.5466369628906\n",
      "  batch 445 loss: 280.3289794921875\n",
      "  batch 450 loss: 278.4249725341797\n",
      "  batch 455 loss: 294.47682495117186\n",
      "  batch 460 loss: 284.22264099121094\n",
      "  batch 465 loss: 281.25933227539065\n",
      "  batch 470 loss: 274.34580383300784\n",
      "  batch 475 loss: 287.1105712890625\n",
      "  batch 480 loss: 270.96156616210936\n",
      "  batch 485 loss: 283.1694030761719\n",
      "  batch 490 loss: 268.7565124511719\n",
      "  batch 495 loss: 267.64801635742185\n",
      "  batch 500 loss: 279.6031982421875\n",
      "  batch 505 loss: 273.10167236328124\n",
      "  batch 510 loss: 305.55620727539065\n",
      "  batch 515 loss: 286.5503723144531\n",
      "Epoch: 2 is done\n",
      "  batch 5 loss: 197.2493896484375\n",
      "  batch 10 loss: 193.82652893066407\n",
      "  batch 15 loss: 202.6597442626953\n",
      "  batch 20 loss: 208.9362579345703\n",
      "  batch 25 loss: 184.50853881835937\n",
      "  batch 30 loss: 317.48817443847656\n",
      "  batch 35 loss: 245.7400909423828\n",
      "  batch 40 loss: 231.49291381835937\n",
      "  batch 45 loss: 218.03856201171874\n",
      "  batch 50 loss: 220.20865783691406\n",
      "  batch 55 loss: 220.10455322265625\n",
      "  batch 60 loss: 211.6386474609375\n",
      "  batch 65 loss: 200.61492004394532\n",
      "  batch 70 loss: 206.15609741210938\n",
      "  batch 75 loss: 209.3541015625\n",
      "  batch 80 loss: 205.6448486328125\n",
      "  batch 85 loss: 208.75289306640624\n",
      "  batch 90 loss: 209.6759826660156\n",
      "  batch 95 loss: 214.90340881347657\n",
      "  batch 100 loss: 220.56831970214844\n",
      "  batch 105 loss: 212.28200073242186\n",
      "  batch 110 loss: 213.64226684570312\n",
      "  batch 115 loss: 202.15292663574218\n",
      "  batch 120 loss: 202.4895446777344\n",
      "  batch 125 loss: 204.51566162109376\n",
      "  batch 130 loss: 197.76539916992186\n",
      "  batch 135 loss: 208.5886260986328\n",
      "  batch 140 loss: 206.4296447753906\n",
      "  batch 145 loss: 213.94407958984374\n",
      "  batch 150 loss: 208.90978088378907\n",
      "  batch 155 loss: 217.37034301757814\n",
      "  batch 160 loss: 220.63520202636718\n",
      "  batch 165 loss: 203.94375915527343\n",
      "  batch 170 loss: 205.73902893066406\n",
      "  batch 175 loss: 273.2365661621094\n",
      "  batch 180 loss: 246.22455444335938\n",
      "  batch 185 loss: 245.03592224121093\n",
      "  batch 190 loss: 239.98648986816406\n",
      "  batch 195 loss: 228.9672119140625\n",
      "  batch 200 loss: 225.89652404785156\n",
      "  batch 205 loss: 224.36145629882813\n",
      "  batch 210 loss: 266.0992370605469\n",
      "  batch 215 loss: 265.63930053710936\n",
      "  batch 220 loss: 268.7347717285156\n",
      "  batch 225 loss: 249.02573852539064\n",
      "  batch 230 loss: 241.10735473632812\n",
      "  batch 235 loss: 237.8713165283203\n",
      "  batch 240 loss: 248.72551879882812\n",
      "  batch 245 loss: 235.43055725097656\n",
      "  batch 250 loss: 253.23864135742187\n",
      "  batch 255 loss: 239.08456726074218\n",
      "  batch 260 loss: 230.97714538574218\n",
      "  batch 265 loss: 245.75830688476563\n",
      "  batch 270 loss: 231.49752502441407\n",
      "  batch 275 loss: 226.69166564941406\n",
      "  batch 280 loss: 295.76837463378905\n",
      "  batch 285 loss: 230.64732666015624\n",
      "  batch 290 loss: 250.30662536621094\n",
      "  batch 295 loss: 231.36435241699218\n",
      "  batch 300 loss: 229.04776611328126\n",
      "  batch 305 loss: 243.82492370605468\n",
      "  batch 310 loss: 230.70702209472657\n",
      "  batch 315 loss: 251.2508056640625\n",
      "  batch 320 loss: 227.7577880859375\n",
      "  batch 325 loss: 231.4554412841797\n",
      "  batch 330 loss: 235.48047790527343\n",
      "  batch 335 loss: 240.8427001953125\n",
      "  batch 340 loss: 229.01006774902345\n",
      "  batch 345 loss: 215.5256317138672\n",
      "  batch 350 loss: 226.88086853027343\n",
      "  batch 355 loss: 230.51273193359376\n",
      "  batch 360 loss: 221.59463806152343\n",
      "  batch 365 loss: 225.51920166015626\n",
      "  batch 370 loss: 234.04092712402343\n",
      "  batch 375 loss: 221.9950164794922\n",
      "  batch 380 loss: 222.13771362304686\n",
      "  batch 385 loss: 231.88898315429688\n",
      "  batch 390 loss: 225.89454650878906\n",
      "  batch 395 loss: 223.9371124267578\n",
      "  batch 400 loss: 232.75892028808593\n",
      "  batch 405 loss: 228.85767211914063\n",
      "  batch 410 loss: 232.81807861328124\n",
      "  batch 415 loss: 215.75445861816405\n",
      "  batch 420 loss: 235.95377502441406\n",
      "  batch 425 loss: 238.96605529785157\n",
      "  batch 430 loss: 223.64857788085936\n",
      "  batch 435 loss: 233.65005187988282\n",
      "  batch 440 loss: 228.64694519042968\n",
      "  batch 445 loss: 225.93894958496094\n",
      "  batch 450 loss: 227.60179748535157\n",
      "  batch 455 loss: 230.8307891845703\n",
      "  batch 460 loss: 232.83128662109374\n",
      "  batch 465 loss: 232.9689453125\n",
      "  batch 470 loss: 222.46670837402343\n",
      "  batch 475 loss: 231.95050048828125\n",
      "  batch 480 loss: 235.9029052734375\n",
      "  batch 485 loss: 224.291259765625\n",
      "  batch 490 loss: 227.57107849121093\n",
      "  batch 495 loss: 213.62242126464844\n",
      "  batch 500 loss: 246.78675842285156\n",
      "  batch 505 loss: 252.259228515625\n",
      "  batch 510 loss: 235.16168518066405\n",
      "  batch 515 loss: 295.7634521484375\n",
      "Epoch: 3 is done\n",
      "  batch 5 loss: 176.61810607910155\n",
      "  batch 10 loss: 177.9509521484375\n",
      "  batch 15 loss: 201.4390075683594\n",
      "  batch 20 loss: 164.08656005859376\n",
      "  batch 25 loss: 171.29680480957032\n",
      "  batch 30 loss: 187.81564331054688\n",
      "  batch 35 loss: 159.32301330566406\n",
      "  batch 40 loss: 174.2083770751953\n",
      "  batch 45 loss: 176.99410705566407\n",
      "  batch 50 loss: 159.906494140625\n",
      "  batch 55 loss: 167.7321044921875\n",
      "  batch 60 loss: 171.11119995117187\n",
      "  batch 65 loss: 164.3568542480469\n",
      "  batch 70 loss: 168.25220947265626\n",
      "  batch 75 loss: 184.97325134277344\n",
      "  batch 80 loss: 176.07453002929688\n",
      "  batch 85 loss: 174.91502685546874\n",
      "  batch 90 loss: 168.90494689941406\n",
      "  batch 95 loss: 165.94988708496095\n",
      "  batch 100 loss: 167.76582946777344\n",
      "  batch 105 loss: 163.89314880371094\n",
      "  batch 110 loss: 170.88384094238282\n",
      "  batch 115 loss: 170.67940979003907\n",
      "  batch 120 loss: 173.08343200683595\n",
      "  batch 125 loss: 165.78819580078124\n",
      "  batch 130 loss: 169.19179992675782\n",
      "  batch 135 loss: 171.92071838378905\n",
      "  batch 140 loss: 157.54718627929688\n",
      "  batch 145 loss: 172.46261291503907\n",
      "  batch 150 loss: 168.9810546875\n",
      "  batch 155 loss: 179.14886169433595\n",
      "  batch 160 loss: 170.35942687988282\n",
      "  batch 165 loss: 180.97839965820313\n",
      "  batch 170 loss: 176.6987274169922\n",
      "  batch 175 loss: 166.28976745605468\n",
      "  batch 180 loss: 176.48690795898438\n",
      "  batch 185 loss: 183.84229431152343\n",
      "  batch 190 loss: 160.83473510742186\n",
      "  batch 195 loss: 173.44452209472655\n",
      "  batch 200 loss: 184.50997619628907\n",
      "  batch 205 loss: 177.67953186035157\n",
      "  batch 210 loss: 171.42641906738282\n",
      "  batch 215 loss: 175.1433898925781\n",
      "  batch 220 loss: 175.1704071044922\n",
      "  batch 225 loss: 177.92228088378906\n",
      "  batch 230 loss: 172.9292205810547\n",
      "  batch 235 loss: 174.06106567382812\n",
      "  batch 240 loss: 165.85226440429688\n",
      "  batch 245 loss: 171.8647033691406\n",
      "  batch 250 loss: 175.380517578125\n",
      "  batch 255 loss: 184.73928833007812\n",
      "  batch 260 loss: 161.40692443847655\n",
      "  batch 265 loss: 162.35586242675782\n",
      "  batch 270 loss: 191.45271911621094\n",
      "  batch 275 loss: 176.37808532714843\n",
      "  batch 280 loss: 184.42339782714845\n",
      "  batch 285 loss: 180.57459106445313\n",
      "  batch 290 loss: 179.2288391113281\n",
      "  batch 295 loss: 175.34878234863282\n",
      "  batch 300 loss: 172.24152526855468\n",
      "  batch 305 loss: 176.15126953125\n",
      "  batch 310 loss: 180.87052001953126\n",
      "  batch 315 loss: 173.99000549316406\n",
      "  batch 320 loss: 180.2939697265625\n",
      "  batch 325 loss: 178.1608673095703\n",
      "  batch 330 loss: 173.50255737304687\n",
      "  batch 335 loss: 185.02326354980468\n",
      "  batch 340 loss: 191.96285095214844\n",
      "  batch 345 loss: 181.10849304199218\n",
      "  batch 350 loss: 177.29984436035156\n",
      "  batch 355 loss: 186.03175659179686\n",
      "  batch 360 loss: 197.2832244873047\n",
      "  batch 365 loss: 170.81768188476562\n",
      "  batch 370 loss: 165.27518310546876\n",
      "  batch 375 loss: 191.16781311035157\n",
      "  batch 380 loss: 178.6615417480469\n",
      "  batch 385 loss: 187.6409942626953\n",
      "  batch 390 loss: 184.3381805419922\n",
      "  batch 395 loss: 181.35112609863282\n",
      "  batch 400 loss: 184.7392333984375\n",
      "  batch 405 loss: 183.26317443847657\n",
      "  batch 410 loss: 187.35309143066405\n",
      "  batch 415 loss: 186.20297241210938\n",
      "  batch 420 loss: 190.58779296875\n",
      "  batch 425 loss: 185.70821838378907\n",
      "  batch 430 loss: 189.27373352050782\n",
      "  batch 435 loss: 186.46451721191406\n",
      "  batch 440 loss: 188.385498046875\n",
      "  batch 445 loss: 185.4324737548828\n",
      "  batch 450 loss: 180.84075622558595\n",
      "  batch 455 loss: 189.60888061523437\n",
      "  batch 460 loss: 189.4572540283203\n",
      "  batch 465 loss: 196.91522827148438\n",
      "  batch 470 loss: 184.5755126953125\n",
      "  batch 475 loss: 170.85867309570312\n",
      "  batch 480 loss: 180.52959289550782\n",
      "  batch 485 loss: 193.9799072265625\n",
      "  batch 490 loss: 192.48833923339845\n",
      "  batch 495 loss: 187.52052001953126\n",
      "  batch 500 loss: 186.73065185546875\n",
      "  batch 505 loss: 191.66207275390624\n",
      "  batch 510 loss: 186.78116760253906\n",
      "  batch 515 loss: 188.77098693847657\n",
      "Epoch: 4 is done\n",
      "  batch 5 loss: 137.63861389160155\n",
      "  batch 10 loss: 139.31130676269532\n",
      "  batch 15 loss: 156.24783325195312\n",
      "  batch 20 loss: 135.17886352539062\n",
      "  batch 25 loss: 138.69832458496094\n",
      "  batch 30 loss: 132.58770294189452\n",
      "  batch 35 loss: 143.53795776367187\n",
      "  batch 40 loss: 132.48220977783203\n",
      "  batch 45 loss: 142.80199279785157\n",
      "  batch 50 loss: 136.56724243164064\n",
      "  batch 55 loss: 142.39479370117186\n",
      "  batch 60 loss: 131.8882049560547\n",
      "  batch 65 loss: 141.0648681640625\n",
      "  batch 70 loss: 139.81619262695312\n",
      "  batch 75 loss: 145.0576904296875\n",
      "  batch 80 loss: 134.50069885253907\n",
      "  batch 85 loss: 140.19354858398438\n",
      "  batch 90 loss: 143.04238891601562\n",
      "  batch 95 loss: 136.47145080566406\n",
      "  batch 100 loss: 137.3506607055664\n",
      "  batch 105 loss: 147.30398864746093\n",
      "  batch 110 loss: 139.70838928222656\n",
      "  batch 115 loss: 144.05699157714844\n",
      "  batch 120 loss: 133.2246841430664\n",
      "  batch 125 loss: 140.61439514160156\n",
      "  batch 130 loss: 151.99407501220702\n",
      "  batch 135 loss: 148.93648681640624\n",
      "  batch 140 loss: 149.04631652832032\n",
      "  batch 145 loss: 146.89389038085938\n",
      "  batch 150 loss: 147.5297424316406\n",
      "  batch 155 loss: 144.86214904785157\n",
      "  batch 160 loss: 136.9205291748047\n",
      "  batch 165 loss: 146.32076416015624\n",
      "  batch 170 loss: 151.24215698242188\n",
      "  batch 175 loss: 147.373974609375\n",
      "  batch 180 loss: 150.37297973632812\n",
      "  batch 185 loss: 139.21793212890626\n",
      "  batch 190 loss: 153.22509765625\n",
      "  batch 195 loss: 139.4038299560547\n",
      "  batch 200 loss: 127.52115936279297\n",
      "  batch 205 loss: 157.5582260131836\n",
      "  batch 210 loss: 154.7161407470703\n",
      "  batch 215 loss: 162.07085266113282\n",
      "  batch 220 loss: 150.85340881347656\n",
      "  batch 225 loss: 159.40393981933593\n",
      "  batch 230 loss: 147.56327209472656\n",
      "  batch 235 loss: 154.4150405883789\n",
      "  batch 240 loss: 147.57835998535157\n",
      "  batch 245 loss: 153.10776062011718\n",
      "  batch 250 loss: 149.30489807128907\n",
      "  batch 255 loss: 155.33486938476562\n",
      "  batch 260 loss: 159.19151611328124\n",
      "  batch 265 loss: 149.8234375\n",
      "  batch 270 loss: 149.5286071777344\n",
      "  batch 275 loss: 153.3219207763672\n",
      "  batch 280 loss: 146.26264343261718\n",
      "  batch 285 loss: 158.35399169921874\n",
      "  batch 290 loss: 148.6136047363281\n",
      "  batch 295 loss: 150.93394165039064\n",
      "  batch 300 loss: 137.3552017211914\n",
      "  batch 305 loss: 158.3740447998047\n",
      "  batch 310 loss: 148.8252716064453\n",
      "  batch 315 loss: 162.10001220703126\n",
      "  batch 320 loss: 136.87701263427735\n",
      "  batch 325 loss: 159.55141906738282\n",
      "  batch 330 loss: 157.11146240234376\n",
      "  batch 335 loss: 151.93408203125\n",
      "  batch 340 loss: 145.4796875\n",
      "  batch 345 loss: 144.15628356933593\n",
      "  batch 350 loss: 152.5544006347656\n",
      "  batch 355 loss: 150.12169799804687\n",
      "  batch 360 loss: 155.5344482421875\n",
      "  batch 365 loss: 146.21147766113282\n",
      "  batch 370 loss: 154.73792724609376\n",
      "  batch 375 loss: 153.97644653320313\n",
      "  batch 380 loss: 138.23218994140626\n",
      "  batch 385 loss: 154.47548217773436\n",
      "  batch 390 loss: 155.9855743408203\n",
      "  batch 395 loss: 165.1554962158203\n",
      "  batch 400 loss: 163.10696105957032\n",
      "  batch 405 loss: 156.1739929199219\n",
      "  batch 410 loss: 158.6175079345703\n",
      "  batch 415 loss: 160.78766479492188\n",
      "  batch 420 loss: 161.3766876220703\n",
      "  batch 425 loss: 160.91866149902344\n",
      "  batch 430 loss: 160.0303497314453\n",
      "  batch 435 loss: 157.77501831054687\n",
      "  batch 440 loss: 157.50232543945313\n",
      "  batch 445 loss: 144.46929321289062\n",
      "  batch 450 loss: 156.8271942138672\n",
      "  batch 455 loss: 152.5539978027344\n",
      "  batch 460 loss: 156.39186401367186\n",
      "  batch 465 loss: 169.7160430908203\n",
      "  batch 470 loss: 159.56483764648436\n",
      "  batch 475 loss: 159.41900024414062\n",
      "  batch 480 loss: 158.4675537109375\n",
      "  batch 485 loss: 152.2520538330078\n",
      "  batch 490 loss: 160.68228759765626\n",
      "  batch 495 loss: 171.82700805664064\n",
      "  batch 500 loss: 165.8486328125\n",
      "  batch 505 loss: 175.20040893554688\n",
      "  batch 510 loss: 160.24554443359375\n",
      "  batch 515 loss: 167.49068603515624\n",
      "Epoch: 5 is done\n",
      "  batch 5 loss: 127.48173522949219\n",
      "  batch 10 loss: 128.25272674560546\n",
      "  batch 15 loss: 121.96621398925781\n",
      "  batch 20 loss: 126.57764129638672\n",
      "  batch 25 loss: 128.95538940429688\n",
      "  batch 30 loss: 116.32870483398438\n",
      "  batch 35 loss: 114.34827423095703\n",
      "  batch 40 loss: 115.02291564941406\n",
      "  batch 45 loss: 126.31618194580078\n",
      "  batch 50 loss: 118.77755584716797\n",
      "  batch 55 loss: 131.09212951660157\n",
      "  batch 60 loss: 130.51130065917968\n",
      "  batch 65 loss: 106.41845397949218\n",
      "  batch 70 loss: 127.98926391601563\n",
      "  batch 75 loss: 121.18195190429688\n",
      "  batch 80 loss: 119.19320831298828\n",
      "  batch 85 loss: 122.1649887084961\n",
      "  batch 90 loss: 124.48163604736328\n",
      "  batch 95 loss: 113.87714080810547\n",
      "  batch 100 loss: 120.06458740234375\n",
      "  batch 105 loss: 125.9989501953125\n",
      "  batch 110 loss: 115.73659057617188\n",
      "  batch 115 loss: 138.9996139526367\n",
      "  batch 120 loss: 130.8991928100586\n",
      "  batch 125 loss: 134.81519165039063\n",
      "  batch 130 loss: 126.17673034667969\n",
      "  batch 135 loss: 122.00029296875\n",
      "  batch 140 loss: 125.35849609375\n",
      "  batch 145 loss: 121.3600341796875\n",
      "  batch 150 loss: 123.22143249511718\n",
      "  batch 155 loss: 129.4854278564453\n",
      "  batch 160 loss: 133.2086380004883\n",
      "  batch 165 loss: 134.34595947265626\n",
      "  batch 170 loss: 123.43020172119141\n",
      "  batch 175 loss: 138.34362030029297\n",
      "  batch 180 loss: 126.51344146728516\n",
      "  batch 185 loss: 125.38145751953125\n",
      "  batch 190 loss: 139.59750671386718\n",
      "  batch 195 loss: 125.067138671875\n",
      "  batch 200 loss: 131.50658111572267\n",
      "  batch 205 loss: 121.26995391845703\n",
      "  batch 210 loss: 127.99115142822265\n",
      "  batch 215 loss: 121.90550994873047\n",
      "  batch 220 loss: 123.41977233886719\n",
      "  batch 225 loss: 128.2838333129883\n",
      "  batch 230 loss: 130.16158294677734\n",
      "  batch 235 loss: 142.84581298828124\n",
      "  batch 240 loss: 134.80026550292968\n",
      "  batch 245 loss: 123.26144714355469\n",
      "  batch 250 loss: 131.14178619384765\n",
      "  batch 255 loss: 143.46961669921876\n",
      "  batch 260 loss: 138.12925720214844\n",
      "  batch 265 loss: 115.45869598388671\n",
      "  batch 270 loss: 137.98422546386718\n",
      "  batch 275 loss: 133.59319458007812\n",
      "  batch 280 loss: 128.5197311401367\n",
      "  batch 285 loss: 128.38209381103516\n",
      "  batch 290 loss: 135.13546142578124\n",
      "  batch 295 loss: 143.18497619628906\n",
      "  batch 300 loss: 133.97802124023437\n",
      "  batch 305 loss: 141.93841247558595\n",
      "  batch 310 loss: 145.14095764160157\n",
      "  batch 315 loss: 140.2262222290039\n",
      "  batch 320 loss: 139.15725402832032\n",
      "  batch 325 loss: 134.81985168457032\n",
      "  batch 330 loss: 136.99825286865234\n",
      "  batch 335 loss: 131.9499725341797\n",
      "  batch 340 loss: 147.59332885742188\n",
      "  batch 345 loss: 143.98812866210938\n",
      "  batch 350 loss: 144.5485107421875\n",
      "  batch 355 loss: 133.4176055908203\n",
      "  batch 360 loss: 127.32644958496094\n",
      "  batch 365 loss: 126.744189453125\n",
      "  batch 370 loss: 140.18687133789064\n",
      "  batch 375 loss: 140.3309066772461\n",
      "  batch 380 loss: 136.55796661376954\n",
      "  batch 385 loss: 138.90090789794922\n",
      "  batch 390 loss: 144.18606414794922\n",
      "  batch 395 loss: 141.5702362060547\n",
      "  batch 400 loss: 132.74290161132814\n",
      "  batch 405 loss: 142.69419708251954\n",
      "  batch 410 loss: 151.63463745117187\n",
      "  batch 415 loss: 131.65208129882814\n",
      "  batch 420 loss: 132.9535903930664\n",
      "  batch 425 loss: 154.57685241699218\n",
      "  batch 430 loss: 137.1035614013672\n",
      "  batch 435 loss: 149.0696228027344\n",
      "  batch 440 loss: 133.22569580078124\n",
      "  batch 445 loss: 152.86461791992187\n",
      "  batch 450 loss: 147.4276580810547\n",
      "  batch 455 loss: 145.0545394897461\n",
      "  batch 460 loss: 142.73594970703124\n",
      "  batch 465 loss: 143.53119506835938\n",
      "  batch 470 loss: 147.49860229492188\n",
      "  batch 475 loss: 145.12239837646484\n",
      "  batch 480 loss: 129.74940338134766\n",
      "  batch 485 loss: 151.43326721191406\n",
      "  batch 490 loss: 141.19671630859375\n",
      "  batch 495 loss: 142.12596740722657\n",
      "  batch 500 loss: 139.17988128662108\n",
      "  batch 505 loss: 150.1824523925781\n",
      "  batch 510 loss: 136.37064361572266\n",
      "  batch 515 loss: 149.930419921875\n",
      "Epoch: 6 is done\n",
      "  batch 5 loss: 124.33187103271484\n",
      "  batch 10 loss: 112.73363800048828\n",
      "  batch 15 loss: 109.40067901611329\n",
      "  batch 20 loss: 112.1551498413086\n",
      "  batch 25 loss: 106.43221893310547\n",
      "  batch 30 loss: 116.40382537841796\n",
      "  batch 35 loss: 107.28135375976562\n",
      "  batch 40 loss: 124.37980041503906\n",
      "  batch 45 loss: 116.84797821044921\n",
      "  batch 50 loss: 129.52652282714843\n",
      "  batch 55 loss: 117.92396697998046\n",
      "  batch 60 loss: 105.31035919189453\n",
      "  batch 65 loss: 113.93704833984376\n",
      "  batch 70 loss: 125.16867370605469\n",
      "  batch 75 loss: 116.0512481689453\n",
      "  batch 80 loss: 112.79752044677734\n",
      "  batch 85 loss: 120.43179321289062\n",
      "  batch 90 loss: 111.63369750976562\n",
      "  batch 95 loss: 101.6302993774414\n",
      "  batch 100 loss: 109.9999008178711\n",
      "  batch 105 loss: 118.46925659179688\n",
      "  batch 110 loss: 104.8364242553711\n",
      "  batch 115 loss: 120.2178726196289\n",
      "  batch 120 loss: 118.84118041992187\n",
      "  batch 125 loss: 108.67331085205078\n",
      "  batch 130 loss: 110.00314331054688\n",
      "  batch 135 loss: 106.32561492919922\n",
      "  batch 140 loss: 126.89655303955078\n",
      "  batch 145 loss: 113.40289611816407\n",
      "  batch 150 loss: 122.23016052246093\n",
      "  batch 155 loss: 120.92850646972656\n",
      "  batch 160 loss: 111.02695770263672\n",
      "  batch 165 loss: 118.70723876953124\n",
      "  batch 170 loss: 113.84892120361329\n",
      "  batch 175 loss: 132.61461029052734\n",
      "  batch 180 loss: 119.72468719482421\n",
      "  batch 185 loss: 114.1709701538086\n",
      "  batch 190 loss: 120.4561782836914\n",
      "  batch 195 loss: 131.90741119384765\n",
      "  batch 200 loss: 119.28984222412109\n",
      "  batch 205 loss: 115.83650054931641\n",
      "  batch 210 loss: 124.11193389892578\n",
      "  batch 215 loss: 125.29769287109374\n",
      "  batch 220 loss: 129.2314926147461\n",
      "  batch 225 loss: 125.98031311035156\n",
      "  batch 230 loss: 114.72852935791016\n",
      "  batch 235 loss: 126.92261047363282\n",
      "  batch 240 loss: 126.40793914794922\n",
      "  batch 245 loss: 122.96491851806641\n",
      "  batch 250 loss: 118.8303009033203\n",
      "  batch 255 loss: 129.78419036865233\n",
      "  batch 260 loss: 130.24402313232423\n",
      "  batch 265 loss: 134.42684020996094\n",
      "  batch 270 loss: 136.2949676513672\n",
      "  batch 275 loss: 145.32366638183595\n",
      "  batch 280 loss: 120.02077941894531\n",
      "  batch 285 loss: 134.36265258789064\n",
      "  batch 290 loss: 132.66124420166017\n",
      "  batch 295 loss: 130.94471740722656\n",
      "  batch 300 loss: 129.0459747314453\n",
      "  batch 305 loss: 122.25210571289062\n",
      "  batch 310 loss: 130.7545608520508\n",
      "  batch 315 loss: 120.30068969726562\n",
      "  batch 320 loss: 126.05205078125\n",
      "  batch 325 loss: 141.86451110839843\n",
      "  batch 330 loss: 124.1630126953125\n",
      "  batch 335 loss: 128.10069885253907\n",
      "  batch 340 loss: 114.354736328125\n",
      "  batch 345 loss: 123.43674163818359\n",
      "  batch 350 loss: 131.9787811279297\n",
      "  batch 355 loss: 127.86780395507813\n",
      "  batch 360 loss: 127.9471450805664\n",
      "  batch 365 loss: 149.15345458984376\n",
      "  batch 370 loss: 133.59974365234376\n",
      "  batch 375 loss: 131.5075927734375\n",
      "  batch 380 loss: 125.14713745117187\n",
      "  batch 385 loss: 124.40290679931641\n",
      "  batch 390 loss: 125.47885437011719\n",
      "  batch 395 loss: 131.3151840209961\n",
      "  batch 400 loss: 125.29080200195312\n",
      "  batch 405 loss: 126.91655578613282\n",
      "  batch 410 loss: 124.35228576660157\n",
      "  batch 415 loss: 128.29315795898438\n",
      "  batch 420 loss: 132.0599395751953\n",
      "  batch 425 loss: 141.45124206542968\n",
      "  batch 430 loss: 130.3292221069336\n",
      "  batch 435 loss: 139.29525299072264\n",
      "  batch 440 loss: 137.80903015136718\n",
      "  batch 445 loss: 129.55386047363282\n",
      "  batch 450 loss: 131.81551666259764\n",
      "  batch 455 loss: 136.7226135253906\n",
      "  batch 460 loss: 126.13828887939454\n",
      "  batch 465 loss: 135.74154052734374\n",
      "  batch 470 loss: 137.27720489501954\n",
      "  batch 475 loss: 142.08712005615234\n",
      "  batch 480 loss: 136.20006713867187\n",
      "  batch 485 loss: 127.96307067871093\n",
      "  batch 490 loss: 138.77469482421876\n",
      "  batch 495 loss: 135.6903503417969\n",
      "  batch 500 loss: 132.86586151123046\n",
      "  batch 505 loss: 137.76585998535157\n",
      "  batch 510 loss: 133.6179000854492\n",
      "  batch 515 loss: 133.64757537841797\n",
      "Epoch: 7 is done\n",
      "  batch 5 loss: 111.40682830810547\n",
      "  batch 10 loss: 96.1883041381836\n",
      "  batch 15 loss: 112.60321197509765\n",
      "  batch 20 loss: 101.56210479736328\n",
      "  batch 25 loss: 108.33393096923828\n",
      "  batch 30 loss: 108.06582946777344\n",
      "  batch 35 loss: 103.79899749755859\n",
      "  batch 40 loss: 106.76272583007812\n",
      "  batch 45 loss: 101.28790588378907\n",
      "  batch 50 loss: 104.24139404296875\n",
      "  batch 55 loss: 88.99589233398437\n",
      "  batch 60 loss: 116.06660919189453\n",
      "  batch 65 loss: 108.24425354003907\n",
      "  batch 70 loss: 110.32918395996094\n",
      "  batch 75 loss: 114.87004089355469\n",
      "  batch 80 loss: 113.63391876220703\n",
      "  batch 85 loss: 102.67776641845703\n",
      "  batch 90 loss: 111.32992248535156\n",
      "  batch 95 loss: 100.46572265625\n",
      "  batch 100 loss: 109.39408264160156\n",
      "  batch 105 loss: 109.7841781616211\n",
      "  batch 110 loss: 107.012451171875\n",
      "  batch 115 loss: 116.05008697509766\n",
      "  batch 120 loss: 111.37567443847657\n",
      "  batch 125 loss: 105.65892944335937\n",
      "  batch 130 loss: 106.745556640625\n",
      "  batch 135 loss: 118.39016876220703\n",
      "  batch 140 loss: 112.7120361328125\n",
      "  batch 145 loss: 109.43154449462891\n",
      "  batch 150 loss: 103.43665924072266\n",
      "  batch 155 loss: 114.31183776855468\n",
      "  batch 160 loss: 105.48204040527344\n",
      "  batch 165 loss: 95.21741180419922\n",
      "  batch 170 loss: 124.98232421875\n",
      "  batch 175 loss: 116.27915496826172\n",
      "  batch 180 loss: 104.8729263305664\n",
      "  batch 185 loss: 111.78033599853515\n",
      "  batch 190 loss: 113.96134490966797\n",
      "  batch 195 loss: 106.76065368652344\n",
      "  batch 200 loss: 110.20787353515625\n",
      "  batch 205 loss: 120.8244415283203\n",
      "  batch 210 loss: 114.06629333496093\n",
      "  batch 215 loss: 115.0416473388672\n",
      "  batch 220 loss: 106.91708984375\n",
      "  batch 225 loss: 118.1870834350586\n",
      "  batch 230 loss: 111.32101287841797\n",
      "  batch 235 loss: 117.43750915527343\n",
      "  batch 240 loss: 116.19460906982422\n",
      "  batch 245 loss: 110.13696746826172\n",
      "  batch 250 loss: 105.09485473632813\n",
      "  batch 255 loss: 114.40586395263672\n",
      "  batch 260 loss: 117.55806121826171\n",
      "  batch 265 loss: 113.67035522460938\n",
      "  batch 270 loss: 119.25136260986328\n",
      "  batch 275 loss: 119.61614227294922\n",
      "  batch 280 loss: 118.37994689941407\n",
      "  batch 285 loss: 128.23056335449218\n",
      "  batch 290 loss: 119.1770751953125\n",
      "  batch 295 loss: 110.50164031982422\n",
      "  batch 300 loss: 111.54862060546876\n",
      "  batch 305 loss: 126.55568084716796\n",
      "  batch 310 loss: 102.5849624633789\n",
      "  batch 315 loss: 107.03653564453126\n",
      "  batch 320 loss: 123.16980590820313\n",
      "  batch 325 loss: 118.66829986572266\n",
      "  batch 330 loss: 117.06233367919921\n",
      "  batch 335 loss: 117.54013519287109\n",
      "  batch 340 loss: 112.75799255371093\n",
      "  batch 345 loss: 111.87257385253906\n",
      "  batch 350 loss: 125.80833892822265\n",
      "  batch 355 loss: 124.25046234130859\n",
      "  batch 360 loss: 114.64157409667969\n",
      "  batch 365 loss: 108.95421905517578\n",
      "  batch 370 loss: 124.17801818847656\n",
      "  batch 375 loss: 135.76700744628906\n",
      "  batch 380 loss: 121.83797454833984\n",
      "  batch 385 loss: 121.14327392578124\n",
      "  batch 390 loss: 119.13566284179687\n",
      "  batch 395 loss: 115.83753967285156\n",
      "  batch 400 loss: 123.236181640625\n",
      "  batch 405 loss: 130.85661926269532\n",
      "  batch 410 loss: 132.06048583984375\n",
      "  batch 415 loss: 114.35038452148437\n",
      "  batch 420 loss: 129.18453369140624\n",
      "  batch 425 loss: 125.74093780517578\n",
      "  batch 430 loss: 118.34048309326172\n",
      "  batch 435 loss: 130.09872741699218\n",
      "  batch 440 loss: 127.02370758056641\n",
      "  batch 445 loss: 126.03509979248047\n",
      "  batch 450 loss: 125.33450622558594\n",
      "  batch 455 loss: 120.98841552734375\n",
      "  batch 460 loss: 121.61786804199218\n",
      "  batch 465 loss: 127.69396362304687\n",
      "  batch 470 loss: 111.82455291748047\n",
      "  batch 475 loss: 117.25780792236328\n",
      "  batch 480 loss: 121.99765319824219\n",
      "  batch 485 loss: 123.55812377929688\n",
      "  batch 490 loss: 124.47141876220704\n",
      "  batch 495 loss: 119.21712493896484\n",
      "  batch 500 loss: 127.45592651367187\n",
      "  batch 505 loss: 122.12872009277343\n",
      "  batch 510 loss: 133.38445129394532\n",
      "  batch 515 loss: 123.89024200439454\n",
      "Epoch: 8 is done\n",
      "  batch 5 loss: 97.23215942382812\n",
      "  batch 10 loss: 95.29388427734375\n",
      "  batch 15 loss: 100.84072418212891\n",
      "  batch 20 loss: 97.81950378417969\n",
      "  batch 25 loss: 99.10482177734374\n",
      "  batch 30 loss: 107.97413635253906\n",
      "  batch 35 loss: 90.44289093017578\n",
      "  batch 40 loss: 98.31600189208984\n",
      "  batch 45 loss: 89.97539672851562\n",
      "  batch 50 loss: 99.45608215332031\n",
      "  batch 55 loss: 103.2130859375\n",
      "  batch 60 loss: 95.57279815673829\n",
      "  batch 65 loss: 96.3817626953125\n",
      "  batch 70 loss: 94.36918182373047\n",
      "  batch 75 loss: 96.28670959472656\n",
      "  batch 80 loss: 111.24444122314453\n",
      "  batch 85 loss: 103.25785369873047\n",
      "  batch 90 loss: 106.97057189941407\n",
      "  batch 95 loss: 99.86253662109375\n",
      "  batch 100 loss: 111.79907684326172\n",
      "  batch 105 loss: 114.7941162109375\n",
      "  batch 110 loss: 96.1591079711914\n",
      "  batch 115 loss: 103.22714080810547\n",
      "  batch 120 loss: 97.30426940917968\n",
      "  batch 125 loss: 96.43487396240235\n",
      "  batch 130 loss: 97.84258575439453\n",
      "  batch 135 loss: 111.09438018798828\n",
      "  batch 140 loss: 103.7421859741211\n",
      "  batch 145 loss: 105.46581115722657\n",
      "  batch 150 loss: 113.84627532958984\n",
      "  batch 155 loss: 104.43362274169922\n",
      "  batch 160 loss: 100.08162994384766\n",
      "  batch 165 loss: 104.54114837646485\n",
      "  batch 170 loss: 98.04259643554687\n",
      "  batch 175 loss: 112.96525573730469\n",
      "  batch 180 loss: 104.91498260498047\n",
      "  batch 185 loss: 98.59410705566407\n",
      "  batch 190 loss: 100.34022369384766\n",
      "  batch 195 loss: 90.22272491455078\n",
      "  batch 200 loss: 113.51312103271485\n",
      "  batch 205 loss: 107.31390991210938\n",
      "  batch 210 loss: 109.81024017333985\n",
      "  batch 215 loss: 98.63507232666015\n",
      "  batch 220 loss: 93.96851501464843\n",
      "  batch 225 loss: 117.81822509765625\n",
      "  batch 230 loss: 100.90374603271485\n",
      "  batch 235 loss: 114.53629455566406\n",
      "  batch 240 loss: 110.11995697021484\n",
      "  batch 245 loss: 109.39673156738282\n",
      "  batch 250 loss: 104.92794494628906\n",
      "  batch 255 loss: 105.02050170898437\n",
      "  batch 260 loss: 108.87183227539063\n",
      "  batch 265 loss: 105.88037872314453\n",
      "  batch 270 loss: 103.12982025146485\n",
      "  batch 275 loss: 102.06840667724609\n",
      "  batch 280 loss: 100.55309295654297\n",
      "  batch 285 loss: 104.02581634521485\n",
      "  batch 290 loss: 114.52575073242187\n",
      "  batch 295 loss: 105.99561004638672\n",
      "  batch 300 loss: 111.55894317626954\n",
      "  batch 305 loss: 108.08833160400391\n",
      "  batch 310 loss: 116.94580383300782\n",
      "  batch 315 loss: 112.63648071289063\n",
      "  batch 320 loss: 115.1755859375\n",
      "  batch 325 loss: 105.77330322265625\n",
      "  batch 330 loss: 123.42532958984376\n",
      "  batch 335 loss: 106.29668731689453\n",
      "  batch 340 loss: 108.35540313720703\n",
      "  batch 345 loss: 102.05975494384765\n",
      "  batch 350 loss: 111.35993804931641\n",
      "  batch 355 loss: 116.31717529296876\n",
      "  batch 360 loss: 111.02538757324218\n",
      "  batch 365 loss: 149.744580078125\n",
      "  batch 370 loss: 162.04974060058595\n",
      "  batch 375 loss: 416.03908081054686\n",
      "  batch 380 loss: 908.830615234375\n",
      "  batch 385 loss: 1136.4385131835938\n",
      "  batch 390 loss: 1371.289599609375\n",
      "  batch 395 loss: 980.3319091796875\n",
      "  batch 400 loss: 698.3286865234375\n",
      "  batch 405 loss: 619.996875\n",
      "  batch 410 loss: 889.7243530273438\n",
      "  batch 415 loss: 735.4608154296875\n",
      "  batch 420 loss: 911.7708251953125\n",
      "  batch 425 loss: 849.8539672851563\n",
      "  batch 430 loss: 688.7153564453125\n",
      "  batch 435 loss: 704.7582763671875\n",
      "  batch 440 loss: 722.229833984375\n",
      "  batch 445 loss: 769.1269165039063\n",
      "  batch 450 loss: 619.800634765625\n",
      "  batch 455 loss: 656.0600646972656\n",
      "  batch 460 loss: 589.0795654296875\n",
      "  batch 465 loss: 562.1391723632812\n",
      "  batch 470 loss: 509.3669738769531\n",
      "  batch 475 loss: 462.3446044921875\n",
      "  batch 480 loss: 491.4206115722656\n",
      "  batch 485 loss: 517.4537841796875\n",
      "  batch 490 loss: 505.6262634277344\n",
      "  batch 495 loss: 465.5171691894531\n",
      "  batch 500 loss: 414.04376831054685\n",
      "  batch 505 loss: 400.6592956542969\n",
      "  batch 510 loss: 481.9678588867188\n",
      "  batch 515 loss: 368.5042785644531\n",
      "Epoch: 9 is done\n",
      "  batch 5 loss: 321.25481567382815\n",
      "  batch 10 loss: 370.988623046875\n",
      "  batch 15 loss: 333.96672973632815\n",
      "  batch 20 loss: 329.9366516113281\n",
      "  batch 25 loss: 370.3751983642578\n",
      "  batch 30 loss: 347.48895874023435\n",
      "  batch 35 loss: 330.6890563964844\n",
      "  batch 40 loss: 317.7453186035156\n",
      "  batch 45 loss: 259.4349609375\n",
      "  batch 50 loss: 249.91380615234374\n",
      "  batch 55 loss: 255.91363525390625\n",
      "  batch 60 loss: 257.1164154052734\n",
      "  batch 65 loss: 305.1715362548828\n",
      "  batch 70 loss: 306.548828125\n",
      "  batch 75 loss: 298.2663116455078\n",
      "  batch 80 loss: 253.788525390625\n",
      "  batch 85 loss: 275.4438842773437\n",
      "  batch 90 loss: 241.40355529785157\n",
      "  batch 95 loss: 241.14147338867187\n",
      "  batch 100 loss: 220.98326110839844\n",
      "  batch 105 loss: 217.88145751953124\n",
      "  batch 110 loss: 212.14995727539062\n",
      "  batch 115 loss: 213.5378845214844\n",
      "  batch 120 loss: 210.14361267089845\n",
      "  batch 125 loss: 185.68528747558594\n",
      "  batch 130 loss: 201.0570495605469\n",
      "  batch 135 loss: 177.54680786132812\n",
      "  batch 140 loss: 189.62399444580078\n",
      "  batch 145 loss: 182.18919677734374\n",
      "  batch 150 loss: 201.59841918945312\n",
      "  batch 155 loss: 164.68506469726563\n",
      "  batch 160 loss: 196.16705932617188\n",
      "  batch 165 loss: 181.82029418945314\n",
      "  batch 170 loss: 191.15774536132812\n",
      "  batch 175 loss: 197.35218505859376\n",
      "  batch 180 loss: 202.38598022460937\n",
      "  batch 185 loss: 175.64774169921876\n",
      "  batch 190 loss: 186.13719787597657\n",
      "  batch 195 loss: 175.97938842773436\n",
      "  batch 200 loss: 187.69275817871093\n",
      "  batch 205 loss: 179.05057678222656\n",
      "  batch 210 loss: 192.09102783203124\n",
      "  batch 215 loss: 168.17696228027344\n",
      "  batch 220 loss: 176.36505126953125\n",
      "  batch 225 loss: 176.90311279296876\n",
      "  batch 230 loss: 155.91580200195312\n",
      "  batch 235 loss: 188.4747314453125\n",
      "  batch 240 loss: 177.84664001464844\n",
      "  batch 245 loss: 144.5305206298828\n",
      "  batch 250 loss: 166.6947509765625\n",
      "  batch 255 loss: 181.66473999023438\n",
      "  batch 260 loss: 161.27847900390626\n",
      "  batch 265 loss: 180.12207641601563\n",
      "  batch 270 loss: 169.96404418945312\n",
      "  batch 275 loss: 161.11551208496093\n",
      "  batch 280 loss: 175.51668701171874\n",
      "  batch 285 loss: 177.1406005859375\n",
      "  batch 290 loss: 158.4874755859375\n",
      "  batch 295 loss: 168.29615325927733\n",
      "  batch 300 loss: 155.53916625976564\n",
      "  batch 305 loss: 161.04110107421874\n",
      "  batch 310 loss: 181.48018798828124\n",
      "  batch 315 loss: 167.13961181640624\n",
      "  batch 320 loss: 160.96642150878907\n",
      "  batch 325 loss: 149.66073608398438\n",
      "  batch 330 loss: 165.0413024902344\n",
      "  batch 335 loss: 168.5973907470703\n",
      "  batch 340 loss: 164.56627807617187\n",
      "  batch 345 loss: 163.96802978515626\n",
      "  batch 350 loss: 164.24903869628906\n",
      "  batch 355 loss: 152.16993713378906\n",
      "  batch 360 loss: 142.32760009765624\n",
      "  batch 365 loss: 149.91390228271484\n",
      "  batch 370 loss: 148.61316528320313\n",
      "  batch 375 loss: 171.00650024414062\n",
      "  batch 380 loss: 174.77324523925782\n",
      "  batch 385 loss: 146.71962280273436\n",
      "  batch 390 loss: 181.03726196289062\n",
      "  batch 395 loss: 146.48035278320313\n",
      "  batch 400 loss: 157.79299621582032\n",
      "  batch 405 loss: 159.95706481933593\n",
      "  batch 410 loss: 155.63545837402344\n",
      "  batch 415 loss: 154.6893737792969\n",
      "  batch 420 loss: 133.35334167480468\n",
      "  batch 425 loss: 160.93162231445314\n",
      "  batch 430 loss: 160.3059326171875\n",
      "  batch 435 loss: 157.18388977050782\n",
      "  batch 440 loss: 161.85672302246093\n",
      "  batch 445 loss: 155.42039489746094\n",
      "  batch 450 loss: 156.327490234375\n",
      "  batch 455 loss: 157.45742797851562\n",
      "  batch 460 loss: 158.436669921875\n",
      "  batch 465 loss: 158.81329040527345\n",
      "  batch 470 loss: 157.86165466308594\n",
      "  batch 475 loss: 138.69874725341796\n",
      "  batch 480 loss: 143.54520568847656\n",
      "  batch 485 loss: 146.5452133178711\n",
      "  batch 490 loss: 147.27796630859376\n",
      "  batch 495 loss: 143.9770263671875\n",
      "  batch 500 loss: 140.29114379882813\n",
      "  batch 505 loss: 151.96612548828125\n",
      "  batch 510 loss: 138.05279998779298\n",
      "  batch 515 loss: 136.15875091552735\n",
      "Epoch: 10 is done\n",
      "  batch 5 loss: 97.58508605957032\n",
      "  batch 10 loss: 110.01246643066406\n",
      "  batch 15 loss: 109.22385711669922\n",
      "  batch 20 loss: 115.00240325927734\n",
      "  batch 25 loss: 116.25843963623046\n",
      "  batch 30 loss: 126.31502990722656\n",
      "  batch 35 loss: 103.98741912841797\n",
      "  batch 40 loss: 112.67531127929688\n",
      "  batch 45 loss: 114.3603530883789\n",
      "  batch 50 loss: 103.31953582763671\n",
      "  batch 55 loss: 117.94209289550781\n",
      "  batch 60 loss: 106.34432983398438\n",
      "  batch 65 loss: 120.0428970336914\n",
      "  batch 70 loss: 104.61927185058593\n",
      "  batch 75 loss: 106.07016143798828\n",
      "  batch 80 loss: 104.29839935302735\n",
      "  batch 85 loss: 107.3501480102539\n",
      "  batch 90 loss: 107.2928253173828\n",
      "  batch 95 loss: 116.680517578125\n",
      "  batch 100 loss: 119.81771087646484\n",
      "  batch 105 loss: 108.43914337158203\n",
      "  batch 110 loss: 120.18374481201172\n",
      "  batch 115 loss: 98.00546722412109\n",
      "  batch 120 loss: 106.52802276611328\n",
      "  batch 125 loss: 112.13586883544922\n",
      "  batch 130 loss: 112.21165618896484\n",
      "  batch 135 loss: 122.40747680664063\n",
      "  batch 140 loss: 99.1289291381836\n",
      "  batch 145 loss: 119.78773040771485\n",
      "  batch 150 loss: 116.66541748046875\n",
      "  batch 155 loss: 111.46350555419922\n",
      "  batch 160 loss: 116.63869934082031\n",
      "  batch 165 loss: 106.3989486694336\n",
      "  batch 170 loss: 111.60890045166016\n",
      "  batch 175 loss: 114.74259490966797\n",
      "  batch 180 loss: 117.28577117919922\n",
      "  batch 185 loss: 122.0593032836914\n",
      "  batch 190 loss: 113.66248321533203\n",
      "  batch 195 loss: 106.85127410888671\n",
      "  batch 200 loss: 111.80581359863281\n",
      "  batch 205 loss: 117.02771301269532\n",
      "  batch 210 loss: 112.26624450683593\n",
      "  batch 215 loss: 111.78308410644532\n",
      "  batch 220 loss: 106.50733489990235\n",
      "  batch 225 loss: 113.26471710205078\n",
      "  batch 230 loss: 117.00835418701172\n",
      "  batch 235 loss: 112.3533447265625\n",
      "  batch 240 loss: 113.7608413696289\n",
      "  batch 245 loss: 122.4059555053711\n",
      "  batch 250 loss: 96.75271453857422\n",
      "  batch 255 loss: 102.90294647216797\n",
      "  batch 260 loss: 109.00386047363281\n",
      "  batch 265 loss: 118.73738708496094\n",
      "  batch 270 loss: 108.84035186767578\n",
      "  batch 275 loss: 117.91686096191407\n",
      "  batch 280 loss: 132.53192596435548\n",
      "  batch 285 loss: 111.76926116943359\n",
      "  batch 290 loss: 112.20849914550782\n",
      "  batch 295 loss: 115.70492706298828\n",
      "  batch 300 loss: 104.45357818603516\n",
      "  batch 305 loss: 117.69830932617188\n",
      "  batch 310 loss: 104.97791595458985\n",
      "  batch 315 loss: 104.07057037353516\n",
      "  batch 320 loss: 114.75196685791016\n",
      "  batch 325 loss: 111.19045867919922\n",
      "  batch 330 loss: 110.63396301269532\n",
      "  batch 335 loss: 127.86885986328124\n",
      "  batch 340 loss: 114.59933776855469\n",
      "  batch 345 loss: 110.73435668945312\n",
      "  batch 350 loss: 110.85954437255859\n",
      "  batch 355 loss: 112.31318359375\n",
      "  batch 360 loss: 125.46809692382813\n",
      "  batch 365 loss: 105.53753051757812\n",
      "  batch 370 loss: 117.98943786621093\n",
      "  batch 375 loss: 116.4019287109375\n",
      "  batch 380 loss: 114.73462982177735\n",
      "  batch 385 loss: 109.52037811279297\n",
      "  batch 390 loss: 111.4921859741211\n",
      "  batch 395 loss: 110.83158264160156\n",
      "  batch 400 loss: 104.06885375976563\n",
      "  batch 405 loss: 134.4772735595703\n",
      "  batch 410 loss: 118.89318542480468\n",
      "  batch 415 loss: 110.58472900390625\n",
      "  batch 420 loss: 131.07915496826172\n",
      "  batch 425 loss: 100.56056671142578\n",
      "  batch 430 loss: 116.39193725585938\n",
      "  batch 435 loss: 122.67735443115234\n",
      "  batch 440 loss: 116.64564971923828\n",
      "  batch 445 loss: 109.86638793945312\n",
      "  batch 450 loss: 121.91730194091797\n",
      "  batch 455 loss: 111.70528411865234\n",
      "  batch 460 loss: 131.35440063476562\n",
      "  batch 465 loss: 115.58191680908203\n",
      "  batch 470 loss: 124.87061767578125\n",
      "  batch 475 loss: 113.32559204101562\n",
      "  batch 480 loss: 121.93912048339844\n",
      "  batch 485 loss: 116.32223205566406\n",
      "  batch 490 loss: 115.18448791503906\n",
      "  batch 495 loss: 113.59852294921875\n",
      "  batch 500 loss: 120.2236557006836\n",
      "  batch 505 loss: 126.52100677490235\n",
      "  batch 510 loss: 115.81458740234375\n",
      "  batch 515 loss: 112.05804901123047\n",
      "Epoch: 11 is done\n",
      "  batch 5 loss: 91.92064971923828\n",
      "  batch 10 loss: 99.2704849243164\n",
      "  batch 15 loss: 95.03885498046876\n",
      "  batch 20 loss: 85.49752807617188\n",
      "  batch 25 loss: 96.17157592773438\n",
      "  batch 30 loss: 94.66933898925781\n",
      "  batch 35 loss: 99.13512878417968\n",
      "  batch 40 loss: 99.57383880615234\n",
      "  batch 45 loss: 88.7158432006836\n",
      "  batch 50 loss: 82.71174011230468\n",
      "  batch 55 loss: 98.37786560058593\n",
      "  batch 60 loss: 93.8044448852539\n",
      "  batch 65 loss: 87.79808654785157\n",
      "  batch 70 loss: 94.56541442871094\n",
      "  batch 75 loss: 97.00563201904296\n",
      "  batch 80 loss: 97.25828552246094\n",
      "  batch 85 loss: 100.178076171875\n",
      "  batch 90 loss: 86.68164520263672\n",
      "  batch 95 loss: 84.46684875488282\n",
      "  batch 100 loss: 94.33665771484375\n",
      "  batch 105 loss: 86.45688323974609\n",
      "  batch 110 loss: 98.25410766601563\n",
      "  batch 115 loss: 94.5328155517578\n",
      "  batch 120 loss: 99.85252380371094\n",
      "  batch 125 loss: 94.71883239746094\n",
      "  batch 130 loss: 93.6682632446289\n",
      "  batch 135 loss: 80.71613159179688\n",
      "  batch 140 loss: 92.48653259277344\n",
      "  batch 145 loss: 96.62960357666016\n",
      "  batch 150 loss: 94.76041564941406\n",
      "  batch 155 loss: 91.55863037109376\n",
      "  batch 160 loss: 92.01724243164062\n",
      "  batch 165 loss: 95.8723129272461\n",
      "  batch 170 loss: 94.0081298828125\n",
      "  batch 175 loss: 86.95315856933594\n",
      "  batch 180 loss: 90.42671813964844\n",
      "  batch 185 loss: 94.32987518310547\n",
      "  batch 190 loss: 90.63023834228515\n",
      "  batch 195 loss: 101.0247085571289\n",
      "  batch 200 loss: 102.23578338623047\n",
      "  batch 205 loss: 91.76778411865234\n",
      "  batch 210 loss: 83.70865631103516\n",
      "  batch 215 loss: 105.40829772949219\n",
      "  batch 220 loss: 115.9243392944336\n",
      "  batch 225 loss: 95.51547393798828\n",
      "  batch 230 loss: 88.87909393310547\n",
      "  batch 235 loss: 98.32088317871094\n",
      "  batch 240 loss: 90.71558990478516\n",
      "  batch 245 loss: 96.2119873046875\n",
      "  batch 250 loss: 91.79470672607422\n",
      "  batch 255 loss: 104.31209411621094\n",
      "  batch 260 loss: 91.66946563720703\n",
      "  batch 265 loss: 88.06336059570313\n",
      "  batch 270 loss: 93.50899963378906\n",
      "  batch 275 loss: 88.88207702636718\n",
      "  batch 280 loss: 94.97096252441406\n",
      "  batch 285 loss: 87.34667053222657\n",
      "  batch 290 loss: 99.08863372802735\n",
      "  batch 295 loss: 101.00825805664063\n",
      "  batch 300 loss: 97.697021484375\n",
      "  batch 305 loss: 89.60687255859375\n",
      "  batch 310 loss: 84.23802490234375\n",
      "  batch 315 loss: 101.28824768066406\n",
      "  batch 320 loss: 92.14395751953126\n",
      "  batch 325 loss: 91.23010864257813\n",
      "  batch 330 loss: 96.41358947753906\n",
      "  batch 335 loss: 94.80502471923828\n",
      "  batch 340 loss: 99.06513214111328\n",
      "  batch 345 loss: 101.75143280029297\n",
      "  batch 350 loss: 95.98132781982422\n",
      "  batch 355 loss: 108.03714141845703\n",
      "  batch 360 loss: 99.70210418701171\n",
      "  batch 365 loss: 91.43360748291016\n",
      "  batch 370 loss: 100.70553131103516\n",
      "  batch 375 loss: 92.12640533447265\n",
      "  batch 380 loss: 85.71602783203124\n",
      "  batch 385 loss: 90.8841567993164\n",
      "  batch 390 loss: 94.33661651611328\n",
      "  batch 395 loss: 95.91020050048829\n",
      "  batch 400 loss: 105.64197692871093\n",
      "  batch 405 loss: 90.12278900146484\n",
      "  batch 410 loss: 90.9731689453125\n",
      "  batch 415 loss: 105.25199279785156\n",
      "  batch 420 loss: 94.14370269775391\n",
      "  batch 425 loss: 99.52279205322266\n",
      "  batch 430 loss: 97.48668212890625\n",
      "  batch 435 loss: 93.45677642822265\n",
      "  batch 440 loss: 92.44958038330078\n",
      "  batch 445 loss: 99.1116439819336\n",
      "  batch 450 loss: 96.16820526123047\n",
      "  batch 455 loss: 100.64448699951171\n",
      "  batch 460 loss: 87.68072509765625\n",
      "  batch 465 loss: 102.96104125976562\n",
      "  batch 470 loss: 85.57260437011719\n",
      "  batch 475 loss: 98.35344543457032\n",
      "  batch 480 loss: 89.74478454589844\n",
      "  batch 485 loss: 98.71580810546875\n",
      "  batch 490 loss: 99.26338500976563\n",
      "  batch 495 loss: 100.27693939208984\n",
      "  batch 500 loss: 97.87290802001954\n",
      "  batch 505 loss: 97.55855560302734\n",
      "  batch 510 loss: 98.34883422851563\n",
      "  batch 515 loss: 96.7825439453125\n",
      "Epoch: 12 is done\n",
      "  batch 5 loss: 80.84380187988282\n",
      "  batch 10 loss: 81.1290054321289\n",
      "  batch 15 loss: 81.16253967285157\n",
      "  batch 20 loss: 83.74817581176758\n",
      "  batch 25 loss: 74.76480255126953\n",
      "  batch 30 loss: 91.76667556762695\n",
      "  batch 35 loss: 83.1778060913086\n",
      "  batch 40 loss: 83.65437927246094\n",
      "  batch 45 loss: 74.32418975830078\n",
      "  batch 50 loss: 73.51070785522461\n",
      "  batch 55 loss: 95.8330078125\n",
      "  batch 60 loss: 83.29709167480469\n",
      "  batch 65 loss: 80.11750640869141\n",
      "  batch 70 loss: 99.14577178955078\n",
      "  batch 75 loss: 85.06178436279296\n",
      "  batch 80 loss: 87.81325225830078\n",
      "  batch 85 loss: 86.78319244384765\n",
      "  batch 90 loss: 86.16745452880859\n",
      "  batch 95 loss: 88.96972732543945\n",
      "  batch 100 loss: 95.64093017578125\n",
      "  batch 105 loss: 81.616552734375\n",
      "  batch 110 loss: 91.3455810546875\n",
      "  batch 115 loss: 86.93872528076172\n",
      "  batch 120 loss: 77.48221893310547\n",
      "  batch 125 loss: 86.704931640625\n",
      "  batch 130 loss: 78.47412567138672\n",
      "  batch 135 loss: 79.96546478271485\n",
      "  batch 140 loss: 76.20125274658203\n",
      "  batch 145 loss: 90.8490982055664\n",
      "  batch 150 loss: 95.42574310302734\n",
      "  batch 155 loss: 78.86832275390626\n",
      "  batch 160 loss: 80.13473739624024\n",
      "  batch 165 loss: 92.35951385498046\n",
      "  batch 170 loss: 81.34709167480469\n",
      "  batch 175 loss: 85.24313354492188\n",
      "  batch 180 loss: 86.02668609619141\n",
      "  batch 185 loss: 91.81197357177734\n",
      "  batch 190 loss: 88.95667724609375\n",
      "  batch 195 loss: 93.98843078613281\n",
      "  batch 200 loss: 78.12612609863281\n",
      "  batch 205 loss: 93.28457489013672\n",
      "  batch 210 loss: 92.31395416259765\n",
      "  batch 215 loss: 89.14270935058593\n",
      "  batch 220 loss: 82.21448211669922\n",
      "  batch 225 loss: 96.71208190917969\n",
      "  batch 230 loss: 88.10972137451172\n",
      "  batch 235 loss: 83.43321380615234\n",
      "  batch 240 loss: 82.60642852783204\n",
      "  batch 245 loss: 85.39085388183594\n",
      "  batch 250 loss: 90.94114685058594\n",
      "  batch 255 loss: 77.75275802612305\n",
      "  batch 260 loss: 98.97639312744141\n",
      "  batch 265 loss: 99.69836730957032\n",
      "  batch 270 loss: 88.87750244140625\n",
      "  batch 275 loss: 84.85581970214844\n",
      "  batch 280 loss: 88.26402587890625\n",
      "  batch 285 loss: 84.84441986083985\n",
      "  batch 290 loss: 81.40120162963868\n",
      "  batch 295 loss: 90.5158920288086\n",
      "  batch 300 loss: 85.02485046386718\n",
      "  batch 305 loss: 92.65424499511718\n",
      "  batch 310 loss: 89.15676727294922\n",
      "  batch 315 loss: 81.17185974121094\n",
      "  batch 320 loss: 91.61261901855468\n",
      "  batch 325 loss: 95.74148254394531\n",
      "  batch 330 loss: 92.36636352539062\n",
      "  batch 335 loss: 89.7060546875\n",
      "  batch 340 loss: 91.10751953125\n",
      "  batch 345 loss: 81.22950134277343\n",
      "  batch 350 loss: 95.20258178710938\n",
      "  batch 355 loss: 87.05150756835937\n",
      "  batch 360 loss: 90.60326080322265\n",
      "  batch 365 loss: 94.77330627441407\n",
      "  batch 370 loss: 97.93243560791015\n",
      "  batch 375 loss: 74.49712677001953\n",
      "  batch 380 loss: 92.92765045166016\n",
      "  batch 385 loss: 81.6303726196289\n",
      "  batch 390 loss: 91.12801361083984\n",
      "  batch 395 loss: 75.95266571044922\n",
      "  batch 400 loss: 91.28311614990234\n",
      "  batch 405 loss: 92.5701690673828\n",
      "  batch 410 loss: 85.41022644042968\n",
      "  batch 415 loss: 93.89591827392579\n",
      "  batch 420 loss: 94.64555358886719\n",
      "  batch 425 loss: 94.34959869384765\n",
      "  batch 430 loss: 93.49369812011719\n",
      "  batch 435 loss: 94.99033660888672\n",
      "  batch 440 loss: 93.95108337402344\n",
      "  batch 445 loss: 93.50124053955078\n",
      "  batch 450 loss: 94.67527313232422\n",
      "  batch 455 loss: 99.88638458251953\n",
      "  batch 460 loss: 96.76235656738281\n",
      "  batch 465 loss: 89.30847930908203\n",
      "  batch 470 loss: 92.90082702636718\n",
      "  batch 475 loss: 93.46117248535157\n",
      "  batch 480 loss: 96.16814270019532\n",
      "  batch 485 loss: 88.98300170898438\n",
      "  batch 490 loss: 94.97580413818359\n",
      "  batch 495 loss: 100.29303131103515\n",
      "  batch 500 loss: 89.97656707763672\n",
      "  batch 505 loss: 89.9449447631836\n",
      "  batch 510 loss: 97.77035217285156\n",
      "  batch 515 loss: 98.53037872314454\n",
      "Epoch: 13 is done\n",
      "  batch 5 loss: 75.98336029052734\n",
      "  batch 10 loss: 78.00951919555663\n",
      "  batch 15 loss: 73.19010848999024\n",
      "  batch 20 loss: 85.55009613037109\n",
      "  batch 25 loss: 86.97309265136718\n",
      "  batch 30 loss: 72.47917785644532\n",
      "  batch 35 loss: 77.9559211730957\n",
      "  batch 40 loss: 71.78169708251953\n",
      "  batch 45 loss: 87.09670562744141\n",
      "  batch 50 loss: 79.90244140625\n",
      "  batch 55 loss: 80.36427001953125\n",
      "  batch 60 loss: 81.65176391601562\n",
      "  batch 65 loss: 82.43328552246093\n",
      "  batch 70 loss: 76.26707458496094\n",
      "  batch 75 loss: 69.78353729248047\n",
      "  batch 80 loss: 72.75570220947266\n",
      "  batch 85 loss: 77.44949340820312\n",
      "  batch 90 loss: 79.58407287597656\n",
      "  batch 95 loss: 75.56297073364257\n",
      "  batch 100 loss: 80.16277618408203\n",
      "  batch 105 loss: 77.74237289428712\n",
      "  batch 110 loss: 79.79783325195312\n",
      "  batch 115 loss: 74.27245559692383\n",
      "  batch 120 loss: 87.61767883300782\n",
      "  batch 125 loss: 76.25784149169922\n",
      "  batch 130 loss: 74.55311279296875\n",
      "  batch 135 loss: 82.69185867309571\n",
      "  batch 140 loss: 83.00613861083984\n",
      "  batch 145 loss: 78.40877685546874\n",
      "  batch 150 loss: 79.2006233215332\n",
      "  batch 155 loss: 76.45035552978516\n",
      "  batch 160 loss: 77.62190246582031\n",
      "  batch 165 loss: 74.12782592773438\n",
      "  batch 170 loss: 83.41676483154296\n",
      "  batch 175 loss: 87.90252380371093\n",
      "  batch 180 loss: 81.62041778564453\n",
      "  batch 185 loss: 77.11918029785156\n",
      "  batch 190 loss: 79.46732482910156\n",
      "  batch 195 loss: 79.73191986083984\n",
      "  batch 200 loss: 81.39126892089844\n",
      "  batch 205 loss: 88.25235290527344\n",
      "  batch 210 loss: 84.13889923095704\n",
      "  batch 215 loss: 83.81237030029297\n",
      "  batch 220 loss: 74.50295562744141\n",
      "  batch 225 loss: 70.81334686279297\n",
      "  batch 230 loss: 80.3450439453125\n",
      "  batch 235 loss: 79.0291961669922\n",
      "  batch 240 loss: 80.64630279541015\n",
      "  batch 245 loss: 76.64533920288086\n",
      "  batch 250 loss: 92.1099365234375\n",
      "  batch 255 loss: 83.16176147460938\n",
      "  batch 260 loss: 106.20956115722656\n",
      "  batch 265 loss: 80.40295791625977\n",
      "  batch 270 loss: 86.7957260131836\n",
      "  batch 275 loss: 82.63958587646485\n",
      "  batch 280 loss: 86.37735748291016\n",
      "  batch 285 loss: 88.40347747802734\n",
      "  batch 290 loss: 80.22853546142578\n",
      "  batch 295 loss: 93.75327758789062\n",
      "  batch 300 loss: 80.50820465087891\n",
      "  batch 305 loss: 83.46860046386719\n",
      "  batch 310 loss: 93.97445373535156\n",
      "  batch 315 loss: 71.74400939941407\n",
      "  batch 320 loss: 91.02310638427734\n",
      "  batch 325 loss: 97.98275146484374\n",
      "  batch 330 loss: 88.56534576416016\n",
      "  batch 335 loss: 90.55321197509765\n",
      "  batch 340 loss: 70.32538986206055\n",
      "  batch 345 loss: 95.14369201660156\n",
      "  batch 350 loss: 77.14366912841797\n",
      "  batch 355 loss: 88.02067108154297\n",
      "  batch 360 loss: 86.16757202148438\n",
      "  batch 365 loss: 78.40569610595703\n",
      "  batch 370 loss: 87.9374267578125\n",
      "  batch 375 loss: 88.02592468261719\n",
      "  batch 380 loss: 89.68191986083984\n",
      "  batch 385 loss: 80.95100708007813\n",
      "  batch 390 loss: 80.93992462158204\n",
      "  batch 395 loss: 87.72996063232422\n",
      "  batch 400 loss: 77.9646385192871\n",
      "  batch 405 loss: 78.45101165771484\n",
      "  batch 410 loss: 98.30234832763672\n",
      "  batch 415 loss: 93.16710052490234\n",
      "  batch 420 loss: 91.10416870117187\n",
      "  batch 425 loss: 90.28504333496093\n",
      "  batch 430 loss: 80.105078125\n",
      "  batch 435 loss: 101.15925903320313\n",
      "  batch 440 loss: 82.56648254394531\n",
      "  batch 445 loss: 92.78126068115235\n",
      "  batch 450 loss: 77.56838836669922\n",
      "  batch 455 loss: 77.67242431640625\n",
      "  batch 460 loss: 86.1171028137207\n",
      "  batch 465 loss: 94.09866943359376\n",
      "  batch 470 loss: 83.89107742309571\n",
      "  batch 475 loss: 89.45910568237305\n",
      "  batch 480 loss: 79.9451171875\n",
      "  batch 485 loss: 85.45421600341797\n",
      "  batch 490 loss: 81.09076843261718\n",
      "  batch 495 loss: 93.17029266357422\n",
      "  batch 500 loss: 85.2718017578125\n",
      "  batch 505 loss: 92.07509307861328\n",
      "  batch 510 loss: 92.93777770996094\n",
      "  batch 515 loss: 86.69783172607421\n",
      "Epoch: 14 is done\n",
      "  batch 5 loss: 67.51642608642578\n",
      "  batch 10 loss: 78.12708129882813\n",
      "  batch 15 loss: 63.620809173583986\n",
      "  batch 20 loss: 69.69780197143555\n",
      "  batch 25 loss: 74.19584274291992\n",
      "  batch 30 loss: 62.07254409790039\n",
      "  batch 35 loss: 85.14618682861328\n",
      "  batch 40 loss: 68.72899169921875\n",
      "  batch 45 loss: 70.71963653564453\n",
      "  batch 50 loss: 80.47379302978516\n",
      "  batch 55 loss: 76.25050354003906\n",
      "  batch 60 loss: 66.74818496704101\n",
      "  batch 65 loss: 80.11058197021484\n",
      "  batch 70 loss: 83.35995941162109\n",
      "  batch 75 loss: 74.73474426269532\n",
      "  batch 80 loss: 72.91467895507813\n",
      "  batch 85 loss: 79.43501892089844\n",
      "  batch 90 loss: 77.3805160522461\n",
      "  batch 95 loss: 64.76892776489258\n",
      "  batch 100 loss: 75.53397521972656\n",
      "  batch 105 loss: 78.93390655517578\n",
      "  batch 110 loss: 73.56710510253906\n",
      "  batch 115 loss: 84.8313980102539\n",
      "  batch 120 loss: 72.50599060058593\n",
      "  batch 125 loss: 75.48614349365235\n",
      "  batch 130 loss: 79.08158874511719\n",
      "  batch 135 loss: 73.46114120483398\n",
      "  batch 140 loss: 71.31181488037109\n",
      "  batch 145 loss: 83.48747863769532\n",
      "  batch 150 loss: 73.70306091308593\n",
      "  batch 155 loss: 78.15486450195313\n",
      "  batch 160 loss: 84.15402374267578\n",
      "  batch 165 loss: 74.67874908447266\n",
      "  batch 170 loss: 78.14096221923828\n",
      "  batch 175 loss: 71.74719467163087\n",
      "  batch 180 loss: 74.08132247924804\n",
      "  batch 185 loss: 73.9775619506836\n",
      "  batch 190 loss: 70.40312957763672\n",
      "  batch 195 loss: 76.52749328613281\n",
      "  batch 200 loss: 74.4892562866211\n",
      "  batch 205 loss: 71.5970558166504\n",
      "  batch 210 loss: 74.97884826660156\n",
      "  batch 215 loss: 69.22852172851563\n",
      "  batch 220 loss: 74.80527114868164\n",
      "  batch 225 loss: 77.55657958984375\n",
      "  batch 230 loss: 86.35644989013672\n",
      "  batch 235 loss: 82.37149810791016\n",
      "  batch 240 loss: 81.75887298583984\n",
      "  batch 245 loss: 81.742236328125\n",
      "  batch 250 loss: 83.18159255981445\n",
      "  batch 255 loss: 71.49598693847656\n",
      "  batch 260 loss: 73.47501068115234\n",
      "  batch 265 loss: 74.54222412109375\n",
      "  batch 270 loss: 80.85908355712891\n",
      "  batch 275 loss: 81.2319351196289\n",
      "  batch 280 loss: 78.7878646850586\n",
      "  batch 285 loss: 95.5660385131836\n",
      "  batch 290 loss: 85.76373748779297\n",
      "  batch 295 loss: 74.37878036499023\n",
      "  batch 300 loss: 83.06227111816406\n",
      "  batch 305 loss: 77.27731628417969\n",
      "  batch 310 loss: 89.60202178955078\n",
      "  batch 315 loss: 68.36654052734374\n",
      "  batch 320 loss: 67.26372756958008\n",
      "  batch 325 loss: 89.93790130615234\n",
      "  batch 330 loss: 82.64696350097657\n",
      "  batch 335 loss: 72.8808364868164\n",
      "  batch 340 loss: 86.58439331054687\n",
      "  batch 345 loss: 74.18884506225587\n",
      "  batch 350 loss: 88.64388122558594\n",
      "  batch 355 loss: 77.5198959350586\n",
      "  batch 360 loss: 74.72769165039062\n",
      "  batch 365 loss: 87.16541748046875\n",
      "  batch 370 loss: 85.14437103271484\n",
      "  batch 375 loss: 79.6668472290039\n",
      "  batch 380 loss: 71.93083572387695\n",
      "  batch 385 loss: 86.93217163085937\n",
      "  batch 390 loss: 94.76717834472656\n",
      "  batch 395 loss: 89.63495788574218\n",
      "  batch 400 loss: 92.36339569091797\n",
      "  batch 405 loss: 79.26714935302735\n",
      "  batch 410 loss: 79.21445922851562\n",
      "  batch 415 loss: 83.48795166015626\n",
      "  batch 420 loss: 83.38626251220703\n",
      "  batch 425 loss: 89.17733306884766\n",
      "  batch 430 loss: 83.50602264404297\n",
      "  batch 435 loss: 87.36665344238281\n",
      "  batch 440 loss: 72.75056304931641\n",
      "  batch 445 loss: 87.57345123291016\n",
      "  batch 450 loss: 89.73823852539063\n",
      "  batch 455 loss: 84.47754669189453\n",
      "  batch 460 loss: 83.77011108398438\n",
      "  batch 465 loss: 89.24790496826172\n",
      "  batch 470 loss: 81.20987396240234\n",
      "  batch 475 loss: 89.707373046875\n",
      "  batch 480 loss: 83.37607879638672\n",
      "  batch 485 loss: 78.710546875\n",
      "  batch 490 loss: 87.85163879394531\n",
      "  batch 495 loss: 145.68052673339844\n",
      "  batch 500 loss: 159.67579650878906\n",
      "  batch 505 loss: 145.2212890625\n",
      "  batch 510 loss: 175.10041198730468\n",
      "  batch 515 loss: 145.47157287597656\n",
      "Epoch: 15 is done\n",
      "  batch 5 loss: 101.48355407714844\n",
      "  batch 10 loss: 104.83844757080078\n",
      "  batch 15 loss: 116.77991485595703\n",
      "  batch 20 loss: 95.59760437011718\n",
      "  batch 25 loss: 89.52258605957032\n",
      "  batch 30 loss: 101.70609741210937\n",
      "  batch 35 loss: 91.80807189941406\n",
      "  batch 40 loss: 81.30598907470703\n",
      "  batch 45 loss: 101.96438446044922\n",
      "  batch 50 loss: 80.92641906738281\n",
      "  batch 55 loss: 91.60449523925782\n",
      "  batch 60 loss: 99.37680053710938\n",
      "  batch 65 loss: 83.20299377441407\n",
      "  batch 70 loss: 96.59684600830079\n",
      "  batch 75 loss: 82.90704040527343\n",
      "  batch 80 loss: 91.65940399169922\n",
      "  batch 85 loss: 91.96841735839844\n",
      "  batch 90 loss: 80.43800506591796\n",
      "  batch 95 loss: 79.07087707519531\n",
      "  batch 100 loss: 74.25325927734374\n",
      "  batch 105 loss: 77.72129364013672\n",
      "  batch 110 loss: 85.53030090332031\n",
      "  batch 115 loss: 89.63496856689453\n",
      "  batch 120 loss: 78.71992492675781\n",
      "  batch 125 loss: 84.48503952026367\n",
      "  batch 130 loss: 88.44571914672852\n",
      "  batch 135 loss: 73.16574554443359\n",
      "  batch 140 loss: 85.95166931152343\n",
      "  batch 145 loss: 88.4071548461914\n",
      "  batch 150 loss: 77.46507415771484\n",
      "  batch 155 loss: 88.36521453857422\n",
      "  batch 160 loss: 82.2891845703125\n",
      "  batch 165 loss: 79.285205078125\n",
      "  batch 170 loss: 81.16080169677734\n",
      "  batch 175 loss: 87.31207275390625\n",
      "  batch 180 loss: 88.20204620361328\n",
      "  batch 185 loss: 84.62265625\n",
      "  batch 190 loss: 76.6077667236328\n",
      "  batch 195 loss: 82.12387237548828\n",
      "  batch 200 loss: 86.89235382080078\n",
      "  batch 205 loss: 72.19618072509766\n",
      "  batch 210 loss: 76.31634979248047\n",
      "  batch 215 loss: 89.81250610351563\n",
      "  batch 220 loss: 80.65091476440429\n",
      "  batch 225 loss: 75.89810028076172\n",
      "  batch 230 loss: 92.68794097900391\n",
      "  batch 235 loss: 89.06033782958984\n",
      "  batch 240 loss: 79.99585418701172\n",
      "  batch 245 loss: 80.44524612426758\n",
      "  batch 250 loss: 81.78602905273438\n",
      "  batch 255 loss: 95.57810821533204\n",
      "  batch 260 loss: 87.08255920410156\n",
      "  batch 265 loss: 86.67303009033203\n",
      "  batch 270 loss: 124.44327850341797\n",
      "  batch 275 loss: 102.82462768554687\n",
      "  batch 280 loss: 108.60444946289063\n",
      "  batch 285 loss: 85.38648376464843\n",
      "  batch 290 loss: 84.24717712402344\n",
      "  batch 295 loss: 91.73872680664063\n",
      "  batch 300 loss: 99.24727325439453\n",
      "  batch 305 loss: 113.01511077880859\n",
      "  batch 310 loss: 115.14723510742188\n",
      "  batch 315 loss: 109.23523864746093\n",
      "  batch 320 loss: 105.4142562866211\n",
      "  batch 325 loss: 118.8567108154297\n",
      "  batch 330 loss: 98.80998077392579\n",
      "  batch 335 loss: 106.27998199462891\n",
      "  batch 340 loss: 112.40111694335937\n",
      "  batch 345 loss: 99.1265625\n",
      "  batch 350 loss: 97.23550415039062\n",
      "  batch 355 loss: 102.94408111572265\n",
      "  batch 360 loss: 105.20786590576172\n",
      "  batch 365 loss: 102.5413330078125\n",
      "  batch 370 loss: 93.51809692382812\n",
      "  batch 375 loss: 115.07461090087891\n",
      "  batch 380 loss: 102.30765533447266\n",
      "  batch 385 loss: 101.25898590087891\n",
      "  batch 390 loss: 93.00923156738281\n",
      "  batch 395 loss: 78.5454818725586\n",
      "  batch 400 loss: 79.68148803710938\n",
      "  batch 405 loss: 81.34029235839844\n",
      "  batch 410 loss: 102.56640625\n",
      "  batch 415 loss: 85.09198226928712\n",
      "  batch 420 loss: 97.96909790039062\n",
      "  batch 425 loss: 82.73986511230468\n",
      "  batch 430 loss: 96.22447509765625\n",
      "  batch 435 loss: 81.52918853759766\n",
      "  batch 440 loss: 97.1385726928711\n",
      "  batch 445 loss: 90.72201232910156\n",
      "  batch 450 loss: 98.80677947998046\n",
      "  batch 455 loss: 105.64351196289063\n",
      "  batch 460 loss: 106.4630126953125\n",
      "  batch 465 loss: 100.29765625\n",
      "  batch 470 loss: 88.35967254638672\n",
      "  batch 475 loss: 101.13854064941407\n",
      "  batch 480 loss: 96.79464111328124\n",
      "  batch 485 loss: 103.76081848144531\n",
      "  batch 490 loss: 101.18705902099609\n",
      "  batch 495 loss: 96.11797485351562\n",
      "  batch 500 loss: 94.51465148925782\n",
      "  batch 505 loss: 72.69325942993164\n",
      "  batch 510 loss: 90.5056167602539\n",
      "  batch 515 loss: 90.4692169189453\n",
      "Epoch: 16 is done\n",
      "  batch 5 loss: 78.55965576171874\n",
      "  batch 10 loss: 87.03546905517578\n",
      "  batch 15 loss: 79.83838348388672\n",
      "  batch 20 loss: 76.43355712890624\n",
      "  batch 25 loss: 84.36453704833984\n",
      "  batch 30 loss: 73.32958221435547\n",
      "  batch 35 loss: 74.1276611328125\n",
      "  batch 40 loss: 71.7789436340332\n",
      "  batch 45 loss: 83.45233001708985\n",
      "  batch 50 loss: 79.61395721435547\n",
      "  batch 55 loss: 79.56063537597656\n",
      "  batch 60 loss: 66.37818603515625\n",
      "  batch 65 loss: 75.22052993774415\n",
      "  batch 70 loss: 73.84231948852539\n",
      "  batch 75 loss: 83.34881744384765\n",
      "  batch 80 loss: 73.04560699462891\n",
      "  batch 85 loss: 76.46654357910157\n",
      "  batch 90 loss: 79.65321731567383\n",
      "  batch 95 loss: 82.28121490478516\n",
      "  batch 100 loss: 86.25325012207031\n",
      "  batch 105 loss: 88.53309936523438\n",
      "  batch 110 loss: 76.75030517578125\n",
      "  batch 115 loss: 84.77743988037109\n",
      "  batch 120 loss: 72.49535293579102\n",
      "  batch 125 loss: 83.30291290283203\n",
      "  batch 130 loss: 70.83047180175781\n",
      "  batch 135 loss: 76.42190399169922\n",
      "  batch 140 loss: 73.67098846435547\n",
      "  batch 145 loss: 66.9347137451172\n",
      "  batch 150 loss: 81.15180358886718\n",
      "  batch 155 loss: 71.31031188964843\n",
      "  batch 160 loss: 68.26999969482422\n",
      "  batch 165 loss: 76.20505981445312\n",
      "  batch 170 loss: 83.03621368408203\n",
      "  batch 175 loss: 64.77933502197266\n",
      "  batch 180 loss: 77.47090148925781\n",
      "  batch 185 loss: 72.82747039794921\n",
      "  batch 190 loss: 73.2823989868164\n",
      "  batch 195 loss: 79.86880340576172\n",
      "  batch 200 loss: 83.98586959838867\n",
      "  batch 205 loss: 76.68627090454102\n",
      "  batch 210 loss: 79.6952880859375\n",
      "  batch 215 loss: 87.02146606445312\n",
      "  batch 220 loss: 74.06138153076172\n",
      "  batch 225 loss: 90.16457061767578\n",
      "  batch 230 loss: 84.2068588256836\n",
      "  batch 235 loss: 76.74451599121093\n",
      "  batch 240 loss: 72.11037216186523\n",
      "  batch 245 loss: 66.45347671508789\n",
      "  batch 250 loss: 71.33924713134766\n",
      "  batch 255 loss: 75.30740356445312\n",
      "  batch 260 loss: 84.60111541748047\n",
      "  batch 265 loss: 90.66592864990234\n",
      "  batch 270 loss: 84.8986099243164\n",
      "  batch 275 loss: 94.56815185546876\n",
      "  batch 280 loss: 84.35014343261719\n",
      "  batch 285 loss: 82.7659927368164\n",
      "  batch 290 loss: 81.00065460205079\n",
      "  batch 295 loss: 74.65704040527343\n",
      "  batch 300 loss: 83.8035659790039\n",
      "  batch 305 loss: 87.45491790771484\n",
      "  batch 310 loss: 76.67441253662109\n",
      "  batch 315 loss: 72.64316101074219\n",
      "  batch 320 loss: 89.61089630126953\n",
      "  batch 325 loss: 86.14834289550781\n",
      "  batch 330 loss: 84.98338165283204\n",
      "  batch 335 loss: 80.29086074829101\n",
      "  batch 340 loss: 72.86287460327148\n",
      "  batch 345 loss: 78.50322113037109\n",
      "  batch 350 loss: 72.62502899169922\n",
      "  batch 355 loss: 83.64196472167968\n",
      "  batch 360 loss: 80.06773681640625\n",
      "  batch 365 loss: 82.9448959350586\n",
      "  batch 370 loss: 82.79189453125\n",
      "  batch 375 loss: 84.86349792480469\n",
      "  batch 380 loss: 69.7285659790039\n",
      "  batch 385 loss: 82.74227142333984\n",
      "  batch 390 loss: 82.02655944824218\n",
      "  batch 395 loss: 86.2884765625\n",
      "  batch 400 loss: 77.23077697753907\n",
      "  batch 405 loss: 77.01156005859374\n",
      "  batch 410 loss: 83.55814819335937\n",
      "  batch 415 loss: 84.3473892211914\n",
      "  batch 420 loss: 80.68805236816407\n",
      "  batch 425 loss: 83.12596893310547\n",
      "  batch 430 loss: 77.53011169433594\n",
      "  batch 435 loss: 84.91142120361329\n",
      "  batch 440 loss: 84.8512191772461\n",
      "  batch 445 loss: 88.95442504882813\n",
      "  batch 450 loss: 84.82499237060547\n",
      "  batch 455 loss: 90.27745666503907\n",
      "  batch 460 loss: 87.76538696289063\n",
      "  batch 465 loss: 81.8431182861328\n",
      "  batch 470 loss: 73.02767028808594\n",
      "  batch 475 loss: 83.33069229125977\n",
      "  batch 480 loss: 80.30084686279297\n",
      "  batch 485 loss: 89.12494812011718\n",
      "  batch 490 loss: 75.71232757568359\n",
      "  batch 495 loss: 77.63352890014649\n",
      "  batch 500 loss: 85.68393249511719\n",
      "  batch 505 loss: 89.98374938964844\n",
      "  batch 510 loss: 85.06077575683594\n",
      "  batch 515 loss: 91.8138442993164\n",
      "Epoch: 17 is done\n",
      "  batch 5 loss: 73.08320465087891\n",
      "  batch 10 loss: 66.2521125793457\n",
      "  batch 15 loss: 75.91919860839843\n",
      "  batch 20 loss: 72.65430297851563\n",
      "  batch 25 loss: 76.29606170654297\n",
      "  batch 30 loss: 61.617768096923825\n",
      "  batch 35 loss: 67.95260467529297\n",
      "  batch 40 loss: 71.2030746459961\n",
      "  batch 45 loss: 72.02388763427734\n",
      "  batch 50 loss: 72.97923126220704\n",
      "  batch 55 loss: 67.25587692260743\n",
      "  batch 60 loss: 82.96950225830078\n",
      "  batch 65 loss: 73.2241828918457\n",
      "  batch 70 loss: 69.1745506286621\n",
      "  batch 75 loss: 68.32717971801758\n",
      "  batch 80 loss: 75.01219863891602\n",
      "  batch 85 loss: 70.61024780273438\n",
      "  batch 90 loss: 74.16294403076172\n",
      "  batch 95 loss: 73.3948371887207\n",
      "  batch 100 loss: 66.41520614624024\n",
      "  batch 105 loss: 67.4201057434082\n",
      "  batch 110 loss: 66.81504974365234\n",
      "  batch 115 loss: 71.42907104492187\n",
      "  batch 120 loss: 75.70126037597656\n",
      "  batch 125 loss: 70.00346298217774\n",
      "  batch 130 loss: 63.990177154541016\n",
      "  batch 135 loss: 81.38298034667969\n",
      "  batch 140 loss: 68.32904052734375\n",
      "  batch 145 loss: 78.91198425292968\n",
      "  batch 150 loss: 68.5018310546875\n",
      "  batch 155 loss: 70.17078170776367\n",
      "  batch 160 loss: 71.58224716186524\n",
      "  batch 165 loss: 76.04701690673828\n",
      "  batch 170 loss: 69.47290267944337\n",
      "  batch 175 loss: 69.01765365600586\n",
      "  batch 180 loss: 76.39866943359375\n",
      "  batch 185 loss: 68.6666473388672\n",
      "  batch 190 loss: 78.54925842285157\n",
      "  batch 195 loss: 71.66910705566406\n",
      "  batch 200 loss: 88.35891265869141\n",
      "  batch 205 loss: 82.6782470703125\n",
      "  batch 210 loss: 84.72543487548828\n",
      "  batch 215 loss: 79.0557144165039\n",
      "  batch 220 loss: 65.98035736083985\n",
      "  batch 225 loss: 70.21465911865235\n",
      "  batch 230 loss: 73.44595031738281\n",
      "  batch 235 loss: 64.8251564025879\n",
      "  batch 240 loss: 79.70028686523438\n",
      "  batch 245 loss: 77.0903923034668\n",
      "  batch 250 loss: 72.08716735839843\n",
      "  batch 255 loss: 72.73677139282226\n",
      "  batch 260 loss: 79.72798156738281\n",
      "  batch 265 loss: 70.45416870117188\n",
      "  batch 270 loss: 70.42734375\n",
      "  batch 275 loss: 74.07135009765625\n",
      "  batch 280 loss: 74.41056823730469\n",
      "  batch 285 loss: 71.62734832763672\n",
      "  batch 290 loss: 74.48336944580078\n",
      "  batch 295 loss: 74.43027877807617\n",
      "  batch 300 loss: 79.14377136230469\n",
      "  batch 305 loss: 71.36426544189453\n",
      "  batch 310 loss: 87.03705444335938\n",
      "  batch 315 loss: 74.06379089355468\n",
      "  batch 320 loss: 74.64467849731446\n",
      "  batch 325 loss: 72.77593460083008\n",
      "  batch 330 loss: 70.61885452270508\n",
      "  batch 335 loss: 72.09594268798828\n",
      "  batch 340 loss: 78.51764831542968\n",
      "  batch 345 loss: 71.97871551513671\n",
      "  batch 350 loss: 73.98276062011719\n",
      "  batch 355 loss: 83.58865051269531\n",
      "  batch 360 loss: 72.56586608886718\n",
      "  batch 365 loss: 65.8523338317871\n",
      "  batch 370 loss: 80.12679977416992\n",
      "  batch 375 loss: 75.14111633300782\n",
      "  batch 380 loss: 81.23880157470703\n",
      "  batch 385 loss: 79.51188507080079\n",
      "  batch 390 loss: 82.36446533203124\n",
      "  batch 395 loss: 67.83669357299804\n",
      "  batch 400 loss: 75.97549057006836\n",
      "  batch 405 loss: 80.96258544921875\n",
      "  batch 410 loss: 79.91682662963868\n",
      "  batch 415 loss: 80.68818054199218\n",
      "  batch 420 loss: 77.20223617553711\n",
      "  batch 425 loss: 80.79657745361328\n",
      "  batch 430 loss: 86.87248992919922\n",
      "  batch 435 loss: 69.73466339111329\n",
      "  batch 440 loss: 68.54806442260742\n",
      "  batch 445 loss: 76.72178268432617\n",
      "  batch 450 loss: 73.8564208984375\n",
      "  batch 455 loss: 76.8939308166504\n",
      "  batch 460 loss: 78.65751495361329\n",
      "  batch 465 loss: 86.92341232299805\n",
      "  batch 470 loss: 85.70014190673828\n",
      "  batch 475 loss: 97.22410583496094\n",
      "  batch 480 loss: 71.21797561645508\n",
      "  batch 485 loss: 81.78047943115234\n",
      "  batch 490 loss: 79.2619644165039\n",
      "  batch 495 loss: 88.29221954345704\n",
      "  batch 500 loss: 76.07123336791992\n",
      "  batch 505 loss: 84.8372703552246\n",
      "  batch 510 loss: 110.3849105834961\n",
      "  batch 515 loss: 144.4635971069336\n",
      "Epoch: 18 is done\n",
      "  batch 5 loss: 111.4104736328125\n",
      "  batch 10 loss: 89.58384552001954\n",
      "  batch 15 loss: 96.87358093261719\n",
      "  batch 20 loss: 106.18748168945312\n",
      "  batch 25 loss: 100.47740631103515\n",
      "  batch 30 loss: 81.05582733154297\n",
      "  batch 35 loss: 80.40422058105469\n",
      "  batch 40 loss: 83.95210418701171\n",
      "  batch 45 loss: 76.85521240234375\n",
      "  batch 50 loss: 95.55030364990235\n",
      "  batch 55 loss: 107.43595886230469\n",
      "  batch 60 loss: 129.37238006591798\n",
      "  batch 65 loss: 182.8744873046875\n",
      "  batch 70 loss: 192.7719970703125\n",
      "  batch 75 loss: 178.89923400878905\n",
      "  batch 80 loss: 138.4305618286133\n",
      "  batch 85 loss: 135.6040237426758\n",
      "  batch 90 loss: 139.53457946777343\n",
      "  batch 95 loss: 100.38269577026367\n",
      "  batch 100 loss: 117.35398406982422\n",
      "  batch 105 loss: 107.68938293457032\n",
      "  batch 110 loss: 107.14512329101562\n",
      "  batch 115 loss: 169.8112030029297\n",
      "  batch 120 loss: 160.46730346679686\n",
      "  batch 125 loss: 202.9856170654297\n",
      "  batch 130 loss: 123.55736236572265\n",
      "  batch 135 loss: 116.59140319824219\n",
      "  batch 140 loss: 123.8021743774414\n",
      "  batch 145 loss: 134.54751586914062\n",
      "  batch 150 loss: 107.05303649902343\n",
      "  batch 155 loss: 103.8973175048828\n",
      "  batch 160 loss: 104.43377532958985\n",
      "  batch 165 loss: 102.7306121826172\n",
      "  batch 170 loss: 100.52067108154297\n",
      "  batch 175 loss: 201.22311553955078\n",
      "  batch 180 loss: 278.76200714111326\n",
      "  batch 185 loss: 1171.6563842773437\n",
      "  batch 190 loss: 411.6807434082031\n",
      "  batch 195 loss: 290.2722137451172\n",
      "  batch 200 loss: 388.73016967773435\n",
      "  batch 205 loss: 295.09288330078124\n",
      "  batch 210 loss: 176.0218963623047\n",
      "  batch 215 loss: 201.6019714355469\n",
      "  batch 220 loss: 168.89946594238282\n",
      "  batch 225 loss: 192.36436157226564\n",
      "  batch 230 loss: 187.5924087524414\n",
      "  batch 235 loss: 176.009912109375\n",
      "  batch 240 loss: 170.55349731445312\n",
      "  batch 245 loss: 167.20814056396483\n",
      "  batch 250 loss: 144.48060760498046\n",
      "  batch 255 loss: 155.41245727539064\n",
      "  batch 260 loss: 128.9160949707031\n",
      "  batch 265 loss: 142.58722686767578\n",
      "  batch 270 loss: 124.34883117675781\n",
      "  batch 275 loss: 153.786083984375\n",
      "  batch 280 loss: 142.74051513671876\n",
      "  batch 285 loss: 128.11054382324218\n",
      "  batch 290 loss: 126.58133850097656\n",
      "  batch 295 loss: 104.137353515625\n",
      "  batch 300 loss: 150.8599594116211\n",
      "  batch 305 loss: 113.564501953125\n",
      "  batch 310 loss: 131.7030227661133\n",
      "  batch 315 loss: 112.05684051513671\n",
      "  batch 320 loss: 138.76592254638672\n",
      "  batch 325 loss: 119.81104278564453\n",
      "  batch 330 loss: 117.93760375976562\n",
      "  batch 335 loss: 122.75663146972656\n",
      "  batch 340 loss: 133.53637084960937\n",
      "  batch 345 loss: 143.61420593261718\n",
      "  batch 350 loss: 120.6978973388672\n",
      "  batch 355 loss: 125.76240844726563\n",
      "  batch 360 loss: 106.1222152709961\n",
      "  batch 365 loss: 129.506298828125\n",
      "  batch 370 loss: 136.8197479248047\n",
      "  batch 375 loss: 124.4539077758789\n",
      "  batch 380 loss: 114.43188171386718\n",
      "  batch 385 loss: 107.43217163085937\n",
      "  batch 390 loss: 103.2476821899414\n",
      "  batch 395 loss: 121.07441711425781\n",
      "  batch 400 loss: 111.86392364501953\n",
      "  batch 405 loss: 99.53564453125\n",
      "  batch 410 loss: 104.8145751953125\n",
      "  batch 415 loss: 90.10718078613282\n",
      "  batch 420 loss: 123.04695129394531\n",
      "  batch 425 loss: 121.25692443847656\n",
      "  batch 430 loss: 100.4658935546875\n",
      "  batch 435 loss: 104.96081085205078\n",
      "  batch 440 loss: 104.38828887939454\n",
      "  batch 445 loss: 97.63644256591797\n",
      "  batch 450 loss: 97.24761505126953\n",
      "  batch 455 loss: 115.47476501464844\n",
      "  batch 460 loss: 96.41071166992188\n",
      "  batch 465 loss: 97.53740692138672\n",
      "  batch 470 loss: 98.93006286621093\n",
      "  batch 475 loss: 98.45978393554688\n",
      "  batch 480 loss: 128.14216003417968\n",
      "  batch 485 loss: 111.04949645996093\n",
      "  batch 490 loss: 104.42490692138672\n",
      "  batch 495 loss: 82.57616424560547\n",
      "  batch 500 loss: 106.19803771972656\n",
      "  batch 505 loss: 86.84105682373047\n",
      "  batch 510 loss: 105.41165313720703\n",
      "  batch 515 loss: 97.33812103271484\n",
      "Epoch: 19 is done\n",
      "Finished training!\n",
      "\n",
      "[DMRL] Evaluation started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking: 100%|██████████| 19975/19975 [11:00<00:00, 30.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "     | NDCG@-1 | Train (s) | Test (s)\n",
      "---- + ------- + --------- + --------\n",
      "DMRL |  0.2388 |  328.0936 | 660.7677\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Put everything together into an experiment and run it\n",
    "cornac.Experiment(\n",
    "    eval_method=ratio_split, models=[dmrl_recommender], metrics=[NDCG()]\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method save in module cornac.models.recommender:\n",
      "\n",
      "save(save_dir=None, save_trainset=False, metadata=None) method of cornac.models.dmrl.recom_dmrl.DMRL instance\n",
      "    Save a recommender model to the filesystem.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    save_dir: str, default: None\n",
      "        Path to a directory for the model to be stored.\n",
      "    \n",
      "    save_trainset: bool, default: False\n",
      "        Save train_set together with the model. This is useful\n",
      "        if we want to deploy model later because train_set is\n",
      "        required for certain evaluation steps.\n",
      "    \n",
      "    metadata: dict, default: None\n",
      "        Metadata to be saved with the model. This is useful\n",
      "        to store model details.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    model_file : str\n",
      "        Path to the model file stored on the filesystem.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x7f364e536dc0>: attribute lookup <lambda> on cornac.data.text failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdmrl_recommender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDMRL_version_3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/sistema_recomendacao_tp2_versao3/lib/python3.9/site-packages/cornac/models/recommender.py:254\u001b[0m, in \u001b[0;36mRecommender.save\u001b[0;34m(self, save_dir, save_trainset, metadata)\u001b[0m\n\u001b[1;32m    251\u001b[0m model_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(timestamp))\n\u001b[1;32m    253\u001b[0m saved_model \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 254\u001b[0m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43msaved_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHIGHEST_PROTOCOL\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m model is saved to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, model_file))\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x7f364e536dc0>: attribute lookup <lambda> on cornac.data.text failed"
     ]
    }
   ],
   "source": [
    "dmrl_recommender.save(\"DMRL_version_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_Recommender__item_ids',\n",
       " '_Recommender__user_ids',\n",
       " '__class__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_fit_dmrl',\n",
       " '_get_init_params',\n",
       " 'batch_size',\n",
       " 'best_epoch',\n",
       " 'best_value',\n",
       " 'clone',\n",
       " 'current_epoch',\n",
       " 'decay_c',\n",
       " 'decay_r',\n",
       " 'default_score',\n",
       " 'device',\n",
       " 'dropout',\n",
       " 'early_stop',\n",
       " 'embedding_dim',\n",
       " 'epochs',\n",
       " 'eval_train_set_performance',\n",
       " 'fit',\n",
       " 'get_item_image_embedding',\n",
       " 'get_item_text_embeddings',\n",
       " 'get_modality_embeddings',\n",
       " 'global_mean',\n",
       " 'ignored_attrs',\n",
       " 'iid_map',\n",
       " 'image_dim',\n",
       " 'initialize_and_build_modalities',\n",
       " 'is_fitted',\n",
       " 'is_unknown_item',\n",
       " 'is_unknown_user',\n",
       " 'item_ids',\n",
       " 'item_text',\n",
       " 'knows_item',\n",
       " 'knows_user',\n",
       " 'learning_rate',\n",
       " 'load',\n",
       " 'log_metrics',\n",
       " 'max_rating',\n",
       " 'min_rating',\n",
       " 'model',\n",
       " 'monitor_value',\n",
       " 'name',\n",
       " 'num_factors',\n",
       " 'num_items',\n",
       " 'num_neg',\n",
       " 'num_users',\n",
       " 'rank',\n",
       " 'rate',\n",
       " 'recommend',\n",
       " 'reset_info',\n",
       " 'sampler',\n",
       " 'save',\n",
       " 'score',\n",
       " 'stopped_epoch',\n",
       " 'text_dim',\n",
       " 'total_items',\n",
       " 'total_users',\n",
       " 'train_set',\n",
       " 'trainable',\n",
       " 'transform',\n",
       " 'uid_map',\n",
       " 'user_ids',\n",
       " 'val_set',\n",
       " 'verbose',\n",
       " 'wait']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(dmrl_recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = targets.copy()\n",
    "target_prediction[\"Rating\"] = -1\n",
    "\n",
    "user_id_list = targets.UserId.unique()\n",
    "for user_id in user_id_list:\n",
    "    # Get the train dataframe index of the user to predict\n",
    "    user_index = ratio_split.train_set.uid_map.get(user_id)\n",
    "\n",
    "    if user_index is None:\n",
    "        print(f\"User {user_id} is not in the train set\")\n",
    "        continue\n",
    "\n",
    "    # Flter by items to predict \n",
    "    items_to_predict = targets.loc[targets.UserId == user_id, \"ItemId\"].to_list()\n",
    "\n",
    "    # Get the train dataframe index of the items to predict\n",
    "    items_to_predict_index = np.array([ratio_split.train_set.iid_map.get(item_id) for item_id in items_to_predict])\n",
    "\n",
    "    items_to_predict_tensor = torch.tensor([idx for idx in items_to_predict_index if idx is not None])\n",
    "\n",
    "    # Get the position of items that are not in the train set\n",
    "    none_indices = [i for i, x in enumerate(items_to_predict_index) if x is None]\n",
    "\n",
    "    # Get the prediction for the items\n",
    "    line_rating = dmrl_recommender.score(user_index=user_index, item_indices=items_to_predict_tensor)\n",
    "\n",
    "    # Insert -1 in the position of items that are not in the train set\n",
    "    for index_to_insert in none_indices:\n",
    "        line_rating = np.insert(line_rating, index_to_insert, -1)\n",
    "\n",
    "    # Insert the prediction in the target_prediction dataframe\n",
    "    target_prediction.loc[targets.UserId == user_id, \"Rating\"] = line_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = targets.copy()\n",
    "target_prediction[\"Rating\"] = -1\n",
    "\n",
    "user_id_list = targets.UserId.unique()\n",
    "for user_id in user_id_list:\n",
    "    # Get the train dataframe index of the user to predict\n",
    "    user_index = ratio_split.train_set.uid_map.get(user_id)\n",
    "\n",
    "    if user_index is None:\n",
    "        print(f\"User {user_id} is not in the train set\")\n",
    "        continue\n",
    "\n",
    "    # Flter by items to predict \n",
    "    items_to_predict = targets.loc[targets.UserId == user_id, \"ItemId\"].to_list()\n",
    "\n",
    "    # Get the train dataframe index of the items to predict\n",
    "    items_to_predict_index = np.array([ratio_split.train_set.iid_map.get(item_id) for item_id in items_to_predict])\n",
    "\n",
    "    items_to_predict_tensor = torch.tensor([idx for idx in items_to_predict_index if idx is not None])\n",
    "\n",
    "    # Get the position of items that are not in the train set\n",
    "    none_indices = [i for i, x in enumerate(items_to_predict_index) if x is None]\n",
    "\n",
    "    # Get the prediction for the items\n",
    "    line_rating = dmrl_recommender.score(user_index=user_index, item_indices=items_to_predict_tensor)\n",
    "\n",
    "    # Insert -1 in the position of items that are not in the train set\n",
    "    for index_to_insert in none_indices:\n",
    "        line_rating = np.insert(line_rating, index_to_insert, -1)\n",
    "\n",
    "    # Insert the prediction in the target_prediction dataframe\n",
    "    target_prediction.loc[targets.UserId == user_id, \"Rating\"] = line_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = target_prediction.sort_values([\"UserId\", \"Rating\"], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction.to_csv(\"submissao_4_DMRL_versao_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = target_prediction.drop(columns=\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction.to_csv(\"submissao_4_DMRL_versao_3_sem_rating.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sistema_recomendacao_tp2_versao3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
