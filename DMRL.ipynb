{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disentangled Multimodal Representation Learning for Recommendation (DMRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cornac.metrics import NDCG\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.data import TextModality\n",
    "from cornac.models.dmrl.recom_dmrl import DMRL\n",
    "\n",
    "from utils import load_data, preprocessing_content_data\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings, content, targets = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[\"TimestampDate\"] = ratings['Timestamp'].dt.date\n",
    "ratings.loc[ratings.Rating == 0, \"Rating\"] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ItemId'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_columns = content.columns.to_list()\n",
    "content_columns.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_processed = content[['ItemId']].copy()\n",
    "content_processed[\"text\"] = content[content_columns].astype(str).fillna('').agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings_cornac_dataset = Dataset.build(df[['UserId', 'ItemId', 'Rating', \"text\"]].values.tolist(), fmt='UIR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique users and items\n",
    "ratings.UserId.nunique(), ratings.ItemId.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many itens purchased by each user purchase\n",
    "ratings.groupby([\"UserId\", 'Timestamp'])[\"ItemId\"].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many itens purchased by each user day by day\n",
    "ratings.groupby([\"UserId\", 'TimestampDate'])[\"ItemId\"].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times each user purchased items\n",
    "ratings.groupby(\"UserId\")['Timestamp'].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many times each user purchased items per day\n",
    "ratings.groupby(\"UserId\")['TimestampDate'].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.Rating.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_text_modality = TextModality(\n",
    "    corpus=content_processed.text.to_list(),\n",
    "    ids=content_processed.ItemId.to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating_threshold = 0.5\n",
      "exclude_unknowns = True\n",
      "---\n",
      "Training data:\n",
      "Number of users = 46776\n",
      "Number of items = 26964\n",
      "Number of ratings = 527776\n",
      "Max rating = 10.0\n",
      "Min rating = 0.0\n",
      "Global mean = 7.3\n",
      "---\n",
      "Test data:\n",
      "Number of users = 46776\n",
      "Number of items = 26964\n",
      "Number of ratings = 123946\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Total users = 46776\n",
      "Total items = 26964\n"
     ]
    }
   ],
   "source": [
    "ratio_split = RatioSplit(\n",
    "    data=ratings[['UserId', 'ItemId', 'Rating']].values.tolist(),\n",
    "    test_size=0.2,\n",
    "    exclude_unknowns=True,\n",
    "    verbose=True,\n",
    "    seed=123,\n",
    "    rating_threshold=0.5,\n",
    "    item_text=item_text_modality,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio_split.train_set.uid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_map = ratio_split.train_set.uid_map\n",
    "iid_map = ratio_split.train_set.iid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(ratio_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_map\n",
    "iid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate DMRL recommender\n",
    "dmrl_recommender = DMRL(\n",
    "    batch_size=4096,\n",
    "    epochs=20,\n",
    "    log_metrics=False,\n",
    "    learning_rate=0.01,\n",
    "    num_factors=2,\n",
    "    decay_r=0.5,\n",
    "    decay_c=0.01,\n",
    "    num_neg=3,\n",
    "    embedding_dim=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DMRL] Training started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matheus/miniconda/envs/sistema_recomendacao_tp2/lib/python3.9/site-packages/cornac/models/dmrl/transformer_text.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_ids = torch.load(id_path)\n",
      "/home/matheus/miniconda/envs/sistema_recomendacao_tp2/lib/python3.9/site-packages/cornac/models/dmrl/transformer_text.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.features = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda for training\n",
      "  batch 5 loss: 2839.7349609375\n",
      "  batch 10 loss: 2838.8173828125\n",
      "  batch 15 loss: 2823.062060546875\n",
      "  batch 20 loss: 2752.82568359375\n",
      "  batch 25 loss: 2639.58984375\n",
      "  batch 30 loss: 2528.148681640625\n",
      "  batch 35 loss: 2455.21103515625\n",
      "  batch 40 loss: 2305.06240234375\n",
      "  batch 45 loss: 2113.9444580078125\n",
      "  batch 50 loss: 1926.7844482421874\n",
      "  batch 55 loss: 1752.9337890625\n",
      "  batch 60 loss: 1700.745751953125\n",
      "  batch 65 loss: 1653.53310546875\n",
      "  batch 70 loss: 1632.4863525390624\n",
      "  batch 75 loss: 1605.7842529296875\n",
      "  batch 80 loss: 1592.30791015625\n",
      "  batch 85 loss: 1568.596435546875\n",
      "  batch 90 loss: 1565.9516357421876\n",
      "  batch 95 loss: 1551.0882080078125\n",
      "  batch 100 loss: 1533.9483154296875\n",
      "  batch 105 loss: 1506.35341796875\n",
      "  batch 110 loss: 1498.362939453125\n",
      "  batch 115 loss: 1481.4756591796875\n",
      "  batch 120 loss: 1442.396435546875\n",
      "  batch 125 loss: 1490.872265625\n",
      "Epoch: 0 is done\n",
      "  batch 5 loss: 1323.680517578125\n",
      "  batch 10 loss: 1297.060302734375\n",
      "  batch 15 loss: 1285.6529541015625\n",
      "  batch 20 loss: 1293.46484375\n",
      "  batch 25 loss: 1343.432666015625\n",
      "  batch 30 loss: 1332.89560546875\n",
      "  batch 35 loss: 1326.9566162109375\n",
      "  batch 40 loss: 1322.3633056640624\n",
      "  batch 45 loss: 1326.263818359375\n",
      "  batch 50 loss: 1320.3703369140626\n",
      "  batch 55 loss: 1310.5394775390625\n",
      "  batch 60 loss: 1300.35703125\n",
      "  batch 65 loss: 1298.96220703125\n",
      "  batch 70 loss: 1292.9155029296876\n",
      "  batch 75 loss: 1275.1951904296875\n",
      "  batch 80 loss: 1305.202392578125\n",
      "  batch 85 loss: 1318.8814208984375\n",
      "  batch 90 loss: 1311.567333984375\n",
      "  batch 95 loss: 1314.444189453125\n",
      "  batch 100 loss: 1318.721337890625\n",
      "  batch 105 loss: 1261.8561279296875\n",
      "  batch 110 loss: 1286.8283447265626\n",
      "  batch 115 loss: 1291.6233154296874\n",
      "  batch 120 loss: 1267.8627197265625\n",
      "  batch 125 loss: 1299.7044677734375\n",
      "Epoch: 1 is done\n",
      "  batch 5 loss: 1114.0814697265625\n",
      "  batch 10 loss: 1123.7528564453125\n",
      "  batch 15 loss: 1110.536376953125\n",
      "  batch 20 loss: 1109.2947509765625\n",
      "  batch 25 loss: 1169.2456787109375\n",
      "  batch 30 loss: 1128.631005859375\n",
      "  batch 35 loss: 1149.3764892578124\n",
      "  batch 40 loss: 1133.64189453125\n",
      "  batch 45 loss: 1133.544970703125\n",
      "  batch 50 loss: 1140.655908203125\n",
      "  batch 55 loss: 1156.3629638671875\n",
      "  batch 60 loss: 1147.64658203125\n",
      "  batch 65 loss: 1144.6580322265625\n",
      "  batch 70 loss: 1151.0935791015625\n",
      "  batch 75 loss: 1173.3141357421875\n",
      "  batch 80 loss: 1163.5302001953125\n",
      "  batch 85 loss: 1168.2740478515625\n",
      "  batch 90 loss: 1161.504345703125\n",
      "  batch 95 loss: 1176.6146728515625\n",
      "  batch 100 loss: 1155.873486328125\n",
      "  batch 105 loss: 1155.7322998046875\n",
      "  batch 110 loss: 1143.0703857421875\n",
      "  batch 115 loss: 1156.5142333984375\n",
      "  batch 120 loss: 1159.9357666015626\n",
      "  batch 125 loss: 1155.060693359375\n",
      "Epoch: 2 is done\n",
      "  batch 5 loss: 1012.9435424804688\n",
      "  batch 10 loss: 1013.6356079101563\n",
      "  batch 15 loss: 1001.6662475585938\n",
      "  batch 20 loss: 1028.5331909179688\n",
      "  batch 25 loss: 1031.5529052734375\n",
      "  batch 30 loss: 1038.3020141601562\n",
      "  batch 35 loss: 1042.1330444335938\n",
      "  batch 40 loss: 1062.8339111328125\n",
      "  batch 45 loss: 1043.4531982421875\n",
      "  batch 50 loss: 1036.710595703125\n",
      "  batch 55 loss: 1068.3871826171876\n",
      "  batch 60 loss: 1056.7664916992187\n",
      "  batch 65 loss: 1074.4303466796875\n",
      "  batch 70 loss: 1057.5725341796874\n",
      "  batch 75 loss: 1082.452734375\n",
      "  batch 80 loss: 1092.0192138671875\n",
      "  batch 85 loss: 1097.75947265625\n",
      "  batch 90 loss: 1083.3529541015625\n",
      "  batch 95 loss: 1090.78037109375\n",
      "  batch 100 loss: 1095.4148681640625\n",
      "  batch 105 loss: 1081.2645263671875\n",
      "  batch 110 loss: 1086.3270751953125\n",
      "  batch 115 loss: 1100.860595703125\n",
      "  batch 120 loss: 1133.93076171875\n",
      "  batch 125 loss: 1103.47509765625\n",
      "Epoch: 3 is done\n",
      "  batch 5 loss: 966.2340209960937\n",
      "  batch 10 loss: 963.272216796875\n",
      "  batch 15 loss: 945.6897583007812\n",
      "  batch 20 loss: 970.1702514648438\n",
      "  batch 25 loss: 966.371044921875\n",
      "  batch 30 loss: 954.4632080078125\n",
      "  batch 35 loss: 988.2029174804687\n",
      "  batch 40 loss: 1033.1291259765626\n",
      "  batch 45 loss: 1020.142041015625\n",
      "  batch 50 loss: 1009.0021850585938\n",
      "  batch 55 loss: 1035.2738159179687\n",
      "  batch 60 loss: 1001.2622192382812\n",
      "  batch 65 loss: 1037.1181518554688\n",
      "  batch 70 loss: 1018.9086791992188\n",
      "  batch 75 loss: 1043.0206298828125\n",
      "  batch 80 loss: 1026.4614013671876\n",
      "  batch 85 loss: 1074.992333984375\n",
      "  batch 90 loss: 1046.13955078125\n",
      "  batch 95 loss: 1046.1011840820313\n",
      "  batch 100 loss: 1051.1247680664062\n",
      "  batch 105 loss: 1081.857958984375\n",
      "  batch 110 loss: 1081.168408203125\n",
      "  batch 115 loss: 1094.4400146484375\n",
      "  batch 120 loss: 1105.9118896484374\n",
      "  batch 125 loss: 1098.29931640625\n",
      "Epoch: 4 is done\n",
      "  batch 5 loss: 924.9281127929687\n",
      "  batch 10 loss: 922.173828125\n",
      "  batch 15 loss: 921.2163818359375\n",
      "  batch 20 loss: 934.7738159179687\n",
      "  batch 25 loss: 972.21083984375\n",
      "  batch 30 loss: 949.1369750976562\n",
      "  batch 35 loss: 972.08203125\n",
      "  batch 40 loss: 990.458203125\n",
      "  batch 45 loss: 982.6791259765625\n",
      "  batch 50 loss: 989.5800659179688\n",
      "  batch 55 loss: 986.652783203125\n",
      "  batch 60 loss: 977.707568359375\n",
      "  batch 65 loss: 1014.0402099609375\n",
      "  batch 70 loss: 999.0368530273438\n",
      "  batch 75 loss: 1014.4184448242188\n",
      "  batch 80 loss: 1012.2563110351563\n",
      "  batch 85 loss: 1016.6326416015625\n",
      "  batch 90 loss: 1038.9632690429687\n",
      "  batch 95 loss: 1056.799560546875\n",
      "  batch 100 loss: 1017.372900390625\n",
      "  batch 105 loss: 1052.1974487304688\n",
      "  batch 110 loss: 1047.800537109375\n",
      "  batch 115 loss: 1046.252880859375\n",
      "  batch 120 loss: 1068.49794921875\n",
      "  batch 125 loss: 1080.173779296875\n",
      "Epoch: 5 is done\n",
      "  batch 5 loss: 910.2795043945313\n",
      "  batch 10 loss: 911.2199096679688\n",
      "  batch 15 loss: 920.190771484375\n",
      "  batch 20 loss: 936.1296142578125\n",
      "  batch 25 loss: 926.7384399414062\n",
      "  batch 30 loss: 934.8876831054688\n",
      "  batch 35 loss: 934.5453735351563\n",
      "  batch 40 loss: 968.9505737304687\n",
      "  batch 45 loss: 969.7484375\n",
      "  batch 50 loss: 976.2921875\n",
      "  batch 55 loss: 977.6460693359375\n",
      "  batch 60 loss: 998.94453125\n",
      "  batch 65 loss: 1004.9610595703125\n",
      "  batch 70 loss: 993.8968994140625\n",
      "  batch 75 loss: 1016.3758178710938\n",
      "  batch 80 loss: 1001.6618286132813\n",
      "  batch 85 loss: 1036.53974609375\n",
      "  batch 90 loss: 1036.4171020507813\n",
      "  batch 95 loss: 1058.77021484375\n",
      "  batch 100 loss: 1026.2943359375\n",
      "  batch 105 loss: 1051.9387084960938\n",
      "  batch 110 loss: 1049.8120361328124\n",
      "  batch 115 loss: 1055.5064697265625\n",
      "  batch 120 loss: 1048.6527587890625\n",
      "  batch 125 loss: 1052.9437377929687\n",
      "Epoch: 6 is done\n",
      "  batch 5 loss: 906.9242309570312\n",
      "  batch 10 loss: 908.530224609375\n",
      "  batch 15 loss: 910.4057373046875\n",
      "  batch 20 loss: 919.8643676757813\n",
      "  batch 25 loss: 920.4716064453125\n",
      "  batch 30 loss: 936.152197265625\n",
      "  batch 35 loss: 938.3424682617188\n",
      "  batch 40 loss: 959.1874755859375\n",
      "  batch 45 loss: 961.1422973632813\n",
      "  batch 50 loss: 970.9067504882812\n",
      "  batch 55 loss: 998.4018920898437\n",
      "  batch 60 loss: 997.7118896484375\n",
      "  batch 65 loss: 1017.879248046875\n",
      "  batch 70 loss: 989.16787109375\n",
      "  batch 75 loss: 1009.3582397460938\n",
      "  batch 80 loss: 988.9691528320312\n",
      "  batch 85 loss: 1027.411279296875\n",
      "  batch 90 loss: 1019.8595947265625\n",
      "  batch 95 loss: 1023.1658447265625\n",
      "  batch 100 loss: 1027.885009765625\n",
      "  batch 105 loss: 1036.431103515625\n",
      "  batch 110 loss: 1035.34248046875\n",
      "  batch 115 loss: 1039.4915405273437\n",
      "  batch 120 loss: 1037.4608032226563\n",
      "  batch 125 loss: 1043.74619140625\n",
      "Epoch: 7 is done\n",
      "  batch 5 loss: 900.0736328125\n",
      "  batch 10 loss: 889.7998046875\n",
      "  batch 15 loss: 917.4958984375\n",
      "  batch 20 loss: 914.1162475585937\n",
      "  batch 25 loss: 915.0812866210938\n",
      "  batch 30 loss: 962.3027709960937\n",
      "  batch 35 loss: 943.2428955078125\n",
      "  batch 40 loss: 961.5300048828125\n",
      "  batch 45 loss: 971.6905395507813\n",
      "  batch 50 loss: 960.1229125976563\n",
      "  batch 55 loss: 975.1969970703125\n",
      "  batch 60 loss: 981.2749877929688\n",
      "  batch 65 loss: 1007.8525634765625\n",
      "  batch 70 loss: 994.755224609375\n",
      "  batch 75 loss: 1016.0806518554688\n",
      "  batch 80 loss: 987.7853881835938\n",
      "  batch 85 loss: 1045.583740234375\n",
      "  batch 90 loss: 1025.805322265625\n",
      "  batch 95 loss: 1032.6703247070313\n",
      "  batch 100 loss: 1037.014892578125\n",
      "  batch 105 loss: 1055.4989990234376\n",
      "  batch 110 loss: 1024.7449829101563\n",
      "  batch 115 loss: 1047.730517578125\n",
      "  batch 120 loss: 1066.6667724609374\n",
      "  batch 125 loss: 1066.8511352539062\n",
      "Epoch: 8 is done\n",
      "  batch 5 loss: 907.3467651367188\n",
      "  batch 10 loss: 911.4386108398437\n",
      "  batch 15 loss: 926.8489501953125\n",
      "  batch 20 loss: 915.4188720703125\n",
      "  batch 25 loss: 929.5558471679688\n",
      "  batch 30 loss: 922.1791015625\n",
      "  batch 35 loss: 948.4077270507812\n",
      "  batch 40 loss: 948.1763671875\n",
      "  batch 45 loss: 938.36396484375\n",
      "  batch 50 loss: 969.2915283203125\n",
      "  batch 55 loss: 995.6700561523437\n",
      "  batch 60 loss: 988.3979125976563\n",
      "  batch 65 loss: 958.0299560546875\n",
      "  batch 70 loss: 995.7178955078125\n",
      "  batch 75 loss: 1017.1843505859375\n",
      "  batch 80 loss: 1055.0234375\n",
      "  batch 85 loss: 1016.4494750976562\n",
      "  batch 90 loss: 1042.9456787109375\n",
      "  batch 95 loss: 1040.7091186523437\n",
      "  batch 100 loss: 1022.2255493164063\n",
      "  batch 105 loss: 1058.2632934570313\n",
      "  batch 110 loss: 1049.1720703125\n",
      "  batch 115 loss: 1055.0639404296876\n",
      "  batch 120 loss: 1072.419677734375\n",
      "  batch 125 loss: 1081.721484375\n",
      "Epoch: 9 is done\n",
      "  batch 5 loss: 923.500830078125\n",
      "  batch 10 loss: 912.3918090820313\n",
      "  batch 15 loss: 930.7750244140625\n",
      "  batch 20 loss: 934.830615234375\n",
      "  batch 25 loss: 936.688818359375\n",
      "  batch 30 loss: 956.4756713867188\n",
      "  batch 35 loss: 951.2078857421875\n",
      "  batch 40 loss: 945.8536376953125\n",
      "  batch 45 loss: 947.1607299804688\n",
      "  batch 50 loss: 947.9686157226563\n",
      "  batch 55 loss: 1006.6346923828125\n",
      "  batch 60 loss: 1003.995263671875\n",
      "  batch 65 loss: 1011.7134033203125\n",
      "  batch 70 loss: 1008.6100830078125\n",
      "  batch 75 loss: 1001.9471069335938\n",
      "  batch 80 loss: 1021.4772705078125\n",
      "  batch 85 loss: 1006.5214599609375\n",
      "  batch 90 loss: 1035.3451782226562\n",
      "  batch 95 loss: 1027.6493530273438\n",
      "  batch 100 loss: 1055.099072265625\n",
      "  batch 105 loss: 1065.13427734375\n",
      "  batch 110 loss: 1056.98037109375\n",
      "  batch 115 loss: 1055.5447021484374\n",
      "  batch 120 loss: 1057.6622314453125\n",
      "  batch 125 loss: 1080.2527587890625\n",
      "Epoch: 10 is done\n",
      "  batch 5 loss: 915.8590698242188\n",
      "  batch 10 loss: 915.1016235351562\n",
      "  batch 15 loss: 925.7768310546875\n",
      "  batch 20 loss: 956.2754760742188\n",
      "  batch 25 loss: 928.1919555664062\n",
      "  batch 30 loss: 943.8233642578125\n",
      "  batch 35 loss: 935.1002563476562\n",
      "  batch 40 loss: 987.8713012695313\n",
      "  batch 45 loss: 983.401025390625\n",
      "  batch 50 loss: 999.706591796875\n",
      "  batch 55 loss: 994.4070922851563\n",
      "  batch 60 loss: 1012.7110595703125\n",
      "  batch 65 loss: 1033.6567138671876\n",
      "  batch 70 loss: 1007.088720703125\n",
      "  batch 75 loss: 1031.7003662109375\n",
      "  batch 80 loss: 1026.3737548828126\n",
      "  batch 85 loss: 1022.152490234375\n",
      "  batch 90 loss: 1043.33798828125\n",
      "  batch 95 loss: 1056.2953125\n",
      "  batch 100 loss: 1044.9481689453125\n",
      "  batch 105 loss: 1056.3890869140625\n",
      "  batch 110 loss: 1056.92939453125\n",
      "  batch 115 loss: 1084.5249267578124\n",
      "  batch 120 loss: 1063.575244140625\n",
      "  batch 125 loss: 1068.247021484375\n",
      "Epoch: 11 is done\n",
      "  batch 5 loss: 902.191455078125\n",
      "  batch 10 loss: 924.2012939453125\n",
      "  batch 15 loss: 931.3011474609375\n",
      "  batch 20 loss: 935.4904174804688\n",
      "  batch 25 loss: 952.8636474609375\n",
      "  batch 30 loss: 951.8310668945312\n",
      "  batch 35 loss: 960.3173461914063\n",
      "  batch 40 loss: 938.2454345703125\n",
      "  batch 45 loss: 968.9809936523437\n",
      "  batch 50 loss: 1001.7536743164062\n",
      "  batch 55 loss: 1027.4853393554688\n",
      "  batch 60 loss: 989.0900634765625\n",
      "  batch 65 loss: 1034.0042846679687\n",
      "  batch 70 loss: 997.8933227539062\n",
      "  batch 75 loss: 1042.08974609375\n",
      "  batch 80 loss: 1024.381982421875\n",
      "  batch 85 loss: 1035.5395751953124\n",
      "  batch 90 loss: 1027.1617797851563\n",
      "  batch 95 loss: 1051.1081787109374\n",
      "  batch 100 loss: 1033.9740234375\n",
      "  batch 105 loss: 1053.0725341796874\n",
      "  batch 110 loss: 1059.7840209960937\n",
      "  batch 115 loss: 1093.0694580078125\n",
      "  batch 120 loss: 1037.7947387695312\n",
      "  batch 125 loss: 1070.2333740234376\n",
      "Epoch: 12 is done\n",
      "  batch 5 loss: 921.9246826171875\n",
      "  batch 10 loss: 934.5907592773438\n",
      "  batch 15 loss: 945.521337890625\n",
      "  batch 20 loss: 934.2357666015625\n",
      "  batch 25 loss: 947.0274780273437\n",
      "  batch 30 loss: 964.88896484375\n",
      "  batch 35 loss: 971.2114624023437\n",
      "  batch 40 loss: 950.5238159179687\n",
      "  batch 45 loss: 982.211474609375\n",
      "  batch 50 loss: 1005.9268432617188\n",
      "  batch 55 loss: 998.746337890625\n",
      "  batch 60 loss: 990.4670776367187\n",
      "  batch 65 loss: 1024.5566162109376\n",
      "  batch 70 loss: 1006.9786865234375\n",
      "  batch 75 loss: 1050.0985961914062\n",
      "  batch 80 loss: 1048.702099609375\n",
      "  batch 85 loss: 1030.6323486328124\n",
      "  batch 90 loss: 1039.2394409179688\n",
      "  batch 95 loss: 1013.8317138671875\n",
      "  batch 100 loss: 1069.8779052734376\n",
      "  batch 105 loss: 1073.3589965820313\n",
      "  batch 110 loss: 1064.3639404296875\n",
      "  batch 115 loss: 1060.6368408203125\n",
      "  batch 120 loss: 1072.8877197265624\n",
      "  batch 125 loss: 1065.902880859375\n",
      "Epoch: 13 is done\n",
      "  batch 5 loss: 932.468359375\n",
      "  batch 10 loss: 905.1050659179688\n",
      "  batch 15 loss: 940.4353881835938\n",
      "  batch 20 loss: 927.5499145507813\n",
      "  batch 25 loss: 948.9004760742188\n",
      "  batch 30 loss: 954.1427490234375\n",
      "  batch 35 loss: 964.842626953125\n",
      "  batch 40 loss: 1001.9976928710937\n",
      "  batch 45 loss: 985.60458984375\n",
      "  batch 50 loss: 1018.4461059570312\n",
      "  batch 55 loss: 1021.203125\n",
      "  batch 60 loss: 995.7890991210937\n",
      "  batch 65 loss: 1014.5147827148437\n",
      "  batch 70 loss: 1016.8950561523437\n",
      "  batch 75 loss: 1003.300146484375\n",
      "  batch 80 loss: 1024.2438110351563\n",
      "  batch 85 loss: 1042.51640625\n",
      "  batch 90 loss: 1041.362744140625\n",
      "  batch 95 loss: 1052.4779541015625\n",
      "  batch 100 loss: 1061.46162109375\n",
      "  batch 105 loss: 1065.3804077148438\n",
      "  batch 110 loss: 1069.84296875\n",
      "  batch 115 loss: 1072.26103515625\n",
      "  batch 120 loss: 1080.1593505859375\n",
      "  batch 125 loss: 1072.591796875\n",
      "Epoch: 14 is done\n",
      "  batch 5 loss: 903.8085327148438\n",
      "  batch 10 loss: 932.7076293945313\n",
      "  batch 15 loss: 934.0053588867188\n",
      "  batch 20 loss: 930.1252197265625\n",
      "  batch 25 loss: 956.9082763671875\n",
      "  batch 30 loss: 969.0884521484375\n",
      "  batch 35 loss: 968.6373291015625\n",
      "  batch 40 loss: 986.3302124023437\n",
      "  batch 45 loss: 1002.5592407226562\n",
      "  batch 50 loss: 994.5488647460937\n",
      "  batch 55 loss: 988.696923828125\n",
      "  batch 60 loss: 1014.4716674804688\n",
      "  batch 65 loss: 1006.1983764648437\n",
      "  batch 70 loss: 1031.6001708984375\n",
      "  batch 75 loss: 1050.909033203125\n",
      "  batch 80 loss: 1046.8393798828124\n",
      "  batch 85 loss: 1051.35888671875\n",
      "  batch 90 loss: 1064.54326171875\n",
      "  batch 95 loss: 1045.9549072265625\n",
      "  batch 100 loss: 1068.5134521484374\n",
      "  batch 105 loss: 1055.066650390625\n",
      "  batch 110 loss: 1063.3778076171875\n",
      "  batch 115 loss: 1069.9811645507812\n",
      "  batch 120 loss: 1045.46201171875\n",
      "  batch 125 loss: 1079.537939453125\n",
      "Epoch: 15 is done\n",
      "  batch 5 loss: 915.1463256835938\n",
      "  batch 10 loss: 954.9672119140625\n",
      "  batch 15 loss: 951.2155395507813\n",
      "  batch 20 loss: 944.19111328125\n",
      "  batch 25 loss: 934.6322265625\n",
      "  batch 30 loss: 961.81396484375\n",
      "  batch 35 loss: 984.7946533203125\n",
      "  batch 40 loss: 994.8949340820312\n",
      "  batch 45 loss: 997.3863037109375\n",
      "  batch 50 loss: 997.4617309570312\n",
      "  batch 55 loss: 981.9515502929687\n",
      "  batch 60 loss: 998.3255737304687\n",
      "  batch 65 loss: 1006.6554565429688\n",
      "  batch 70 loss: 1027.1246826171875\n",
      "  batch 75 loss: 1034.3968017578125\n",
      "  batch 80 loss: 1043.2828979492188\n",
      "  batch 85 loss: 1038.125732421875\n",
      "  batch 90 loss: 1051.837841796875\n",
      "  batch 95 loss: 1051.8660400390625\n",
      "  batch 100 loss: 1063.3915771484376\n",
      "  batch 105 loss: 1024.8977294921874\n",
      "  batch 110 loss: 1088.1234375\n",
      "  batch 115 loss: 1067.275439453125\n",
      "  batch 120 loss: 1068.92216796875\n",
      "  batch 125 loss: 1069.3693359375\n",
      "Epoch: 16 is done\n",
      "  batch 5 loss: 913.4289428710938\n",
      "  batch 10 loss: 948.978662109375\n",
      "  batch 15 loss: 948.5901489257812\n",
      "  batch 20 loss: 936.835595703125\n",
      "  batch 25 loss: 961.8449096679688\n",
      "  batch 30 loss: 980.1694213867188\n",
      "  batch 35 loss: 987.247314453125\n",
      "  batch 40 loss: 983.7959716796875\n",
      "  batch 45 loss: 1016.7451782226562\n",
      "  batch 50 loss: 1000.6630981445312\n",
      "  batch 55 loss: 1002.6660522460937\n",
      "  batch 60 loss: 1020.8280029296875\n",
      "  batch 65 loss: 1013.6905639648437\n",
      "  batch 70 loss: 1015.3150634765625\n",
      "  batch 75 loss: 1024.3204345703125\n",
      "  batch 80 loss: 1047.1614990234375\n",
      "  batch 85 loss: 1045.4860229492188\n",
      "  batch 90 loss: 1037.9755859375\n",
      "  batch 95 loss: 1056.9701171875\n",
      "  batch 100 loss: 1050.135693359375\n",
      "  batch 105 loss: 1058.3607421875\n",
      "  batch 110 loss: 1063.0412841796874\n",
      "  batch 115 loss: 1090.5447021484374\n",
      "  batch 120 loss: 1107.3291748046875\n",
      "  batch 125 loss: 1081.12802734375\n",
      "Epoch: 17 is done\n",
      "  batch 5 loss: 907.6617919921875\n",
      "  batch 10 loss: 963.7183959960937\n",
      "  batch 15 loss: 924.5911010742187\n",
      "  batch 20 loss: 962.5351318359375\n",
      "  batch 25 loss: 966.4197998046875\n",
      "  batch 30 loss: 948.9164794921875\n",
      "  batch 35 loss: 970.3928344726562\n",
      "  batch 40 loss: 984.3359008789063\n",
      "  batch 45 loss: 998.8240478515625\n",
      "  batch 50 loss: 1022.5136352539063\n",
      "  batch 55 loss: 1000.2909912109375\n",
      "  batch 60 loss: 1012.7430786132812\n",
      "  batch 65 loss: 1046.91982421875\n",
      "  batch 70 loss: 1054.459521484375\n",
      "  batch 75 loss: 1019.1997436523437\n",
      "  batch 80 loss: 1019.9341186523437\n",
      "  batch 85 loss: 1039.3539184570313\n",
      "  batch 90 loss: 1038.7412109375\n",
      "  batch 95 loss: 1085.4263427734375\n",
      "  batch 100 loss: 1068.8052734375\n",
      "  batch 105 loss: 1081.6289916992187\n",
      "  batch 110 loss: 1093.8720458984376\n",
      "  batch 115 loss: 1084.0261474609374\n",
      "  batch 120 loss: 1078.492822265625\n",
      "  batch 125 loss: 1090.598193359375\n",
      "Epoch: 18 is done\n",
      "  batch 5 loss: 936.45400390625\n",
      "  batch 10 loss: 958.7510131835937\n",
      "  batch 15 loss: 949.5536254882812\n",
      "  batch 20 loss: 954.13681640625\n",
      "  batch 25 loss: 946.6307495117187\n",
      "  batch 30 loss: 944.7440673828125\n",
      "  batch 35 loss: 976.1825073242187\n",
      "  batch 40 loss: 990.9991943359375\n",
      "  batch 45 loss: 985.0356567382812\n",
      "  batch 50 loss: 1014.1151000976563\n",
      "  batch 55 loss: 1003.1346435546875\n",
      "  batch 60 loss: 1032.5325561523437\n",
      "  batch 65 loss: 1009.7173950195313\n",
      "  batch 70 loss: 1028.2160278320312\n",
      "  batch 75 loss: 1053.583935546875\n",
      "  batch 80 loss: 1018.6838500976562\n",
      "  batch 85 loss: 1056.4250732421874\n",
      "  batch 90 loss: 1046.0850341796875\n",
      "  batch 95 loss: 1057.4116943359375\n",
      "  batch 100 loss: 1074.053857421875\n",
      "  batch 105 loss: 1065.4703125\n",
      "  batch 110 loss: 1080.770849609375\n",
      "  batch 115 loss: 1072.1471923828126\n",
      "  batch 120 loss: 1094.8936767578125\n",
      "  batch 125 loss: 1089.529443359375\n",
      "Epoch: 19 is done\n",
      "Finished training!\n",
      "\n",
      "[DMRL] Evaluation started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking: 100%|██████████| 19966/19966 [05:46<00:00, 57.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "     | NDCG@-1 | Train (s) | Test (s)\n",
      "---- + ------- + --------- + --------\n",
      "DMRL |  0.2341 |   64.3903 | 346.1104\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Put everything together into an experiment and run it\n",
    "cornac.Experiment(\n",
    "    eval_method=ratio_split, models=[dmrl_recommender], metrics=[NDCG()]\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como usar esse algoritmo para tratar itens novos?\n",
    "# # como usar esse algoritmo para tratar usuarios novos?\n",
    "\n",
    "\n",
    "# eu tenho o conteudo de todos os itens, incluive itens que estao apenas n conjunto de teste?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 019f8b946a is not in the train set\n",
      "User 046dd69e7a is not in the train set\n",
      "User 08b0ad8c2b is not in the train set\n",
      "User 0b49b88c68 is not in the train set\n",
      "User 0bf04bee73 is not in the train set\n",
      "User 0d858ba37c is not in the train set\n",
      "User 0e095e054e is not in the train set\n",
      "User 1230a1696b is not in the train set\n",
      "User 1344a31569 is not in the train set\n",
      "User 14345de2c2 is not in the train set\n",
      "User 19892cd2c9 is not in the train set\n",
      "User 1c53aae1c1 is not in the train set\n",
      "User 1e44fdf9c4 is not in the train set\n",
      "User 23ea75a4cd is not in the train set\n",
      "User 254a5ecb7a is not in the train set\n",
      "User 263138a8b9 is not in the train set\n",
      "User 289319b9e5 is not in the train set\n",
      "User 290db70122 is not in the train set\n",
      "User 2a38a4a931 is not in the train set\n",
      "User 2b12a84466 is not in the train set\n",
      "User 2b634a9d15 is not in the train set\n",
      "User 2bba9f4124 is not in the train set\n",
      "User 2e61866ea9 is not in the train set\n",
      "User 2efbe23ad5 is not in the train set\n",
      "User 3002c4f348 is not in the train set\n",
      "User 302c6ebc7d is not in the train set\n",
      "User 311c3fb9c9 is not in the train set\n",
      "User 3174db8344 is not in the train set\n",
      "User 3402dfd220 is not in the train set\n",
      "User 3a7666b086 is not in the train set\n",
      "User 3af1c3da98 is not in the train set\n",
      "User 3b2570d084 is not in the train set\n",
      "User 3e91970f77 is not in the train set\n",
      "User 3f035bdfc6 is not in the train set\n",
      "User 3f332c6d51 is not in the train set\n",
      "User 42aa61c7cc is not in the train set\n",
      "User 436dc8ee53 is not in the train set\n",
      "User 454ea70f2b is not in the train set\n",
      "User 4579da10f3 is not in the train set\n",
      "User 4d251eeee2 is not in the train set\n",
      "User 51aa0a41c5 is not in the train set\n",
      "User 53d6d0c034 is not in the train set\n",
      "User 53f51ff216 is not in the train set\n",
      "User 548b57cc0f is not in the train set\n",
      "User 586ee5cb5f is not in the train set\n",
      "User 5934c1ec0c is not in the train set\n",
      "User 5a64e01384 is not in the train set\n",
      "User 5a7b238ba0 is not in the train set\n",
      "User 5fbcf70d27 is not in the train set\n",
      "User 61c38847b1 is not in the train set\n",
      "User 6318d3c5f8 is not in the train set\n",
      "User 632fb5fbe4 is not in the train set\n",
      "User 6346dc7233 is not in the train set\n",
      "User 64bb3c7589 is not in the train set\n",
      "User 67caec8041 is not in the train set\n",
      "User 67d6e2e200 is not in the train set\n",
      "User 689b243e8f is not in the train set\n",
      "User 69dd3adb00 is not in the train set\n",
      "User 6b339b7d02 is not in the train set\n",
      "User 706608cfdb is not in the train set\n",
      "User 747181176b is not in the train set\n",
      "User 786723d8a8 is not in the train set\n",
      "User 78cc593b76 is not in the train set\n",
      "User 799d3f5de8 is not in the train set\n",
      "User 7bb060764a is not in the train set\n",
      "User 7c9ff93b39 is not in the train set\n",
      "User 7ec3b3cf67 is not in the train set\n",
      "User 7ee3170aab is not in the train set\n",
      "User 80f4431bf8 is not in the train set\n",
      "User 822bfaa400 is not in the train set\n",
      "User 82de976eb7 is not in the train set\n",
      "User 83ef3c79df is not in the train set\n",
      "User 84cd1f88d0 is not in the train set\n",
      "User 86f2df8b1f is not in the train set\n",
      "User 884aefaef7 is not in the train set\n",
      "User 88561cd999 is not in the train set\n",
      "User 8a844213f1 is not in the train set\n",
      "User 8abb69b3d5 is not in the train set\n",
      "User 8cba202599 is not in the train set\n",
      "User 8d2355364e is not in the train set\n",
      "User 8df77cb5af is not in the train set\n",
      "User 8fc809ece3 is not in the train set\n",
      "User 936eebc250 is not in the train set\n",
      "User 943422623d is not in the train set\n",
      "User 965b062979 is not in the train set\n",
      "User 96632840e6 is not in the train set\n",
      "User 96d159ddda is not in the train set\n",
      "User 996740de91 is not in the train set\n",
      "User 9c2bee7128 is not in the train set\n",
      "User 9c415bdd4d is not in the train set\n",
      "User 9c677bd9ea is not in the train set\n",
      "User 9db51b0232 is not in the train set\n",
      "User a0db07376f is not in the train set\n",
      "User a29fe7085f is not in the train set\n",
      "User a390cc25cb is not in the train set\n",
      "User a65f0f2858 is not in the train set\n",
      "User a6e842dce9 is not in the train set\n",
      "User aa8234dd67 is not in the train set\n",
      "User aae58edc3a is not in the train set\n",
      "User ac02166ad1 is not in the train set\n",
      "User ac4ff5e8dc is not in the train set\n",
      "User ac975f23e8 is not in the train set\n",
      "User ad17a667ff is not in the train set\n",
      "User ad52498830 is not in the train set\n",
      "User af3303f852 is not in the train set\n",
      "User b071ffc86b is not in the train set\n",
      "User b0e7534770 is not in the train set\n",
      "User b27bbc81a8 is not in the train set\n",
      "User b33ad0646b is not in the train set\n",
      "User b35b31a24a is not in the train set\n",
      "User b54daf1b00 is not in the train set\n",
      "User b928fec593 is not in the train set\n",
      "User ba7609ee57 is not in the train set\n",
      "User baa84d7484 is not in the train set\n",
      "User bbc77a1cfa is not in the train set\n",
      "User bcc097feaf is not in the train set\n",
      "User bd6ad709f5 is not in the train set\n",
      "User be800ff41f is not in the train set\n",
      "User be9351985c is not in the train set\n",
      "User bf308d5f98 is not in the train set\n",
      "User bfda4ecd52 is not in the train set\n",
      "User c53e912b5f is not in the train set\n",
      "User c60e0a0674 is not in the train set\n",
      "User c793b3be8f is not in the train set\n",
      "User c93f6bc009 is not in the train set\n",
      "User c9b7db2d84 is not in the train set\n",
      "User cac029e0ca is not in the train set\n",
      "User cb53c399f2 is not in the train set\n",
      "User cb955adc83 is not in the train set\n",
      "User cf247d6daf is not in the train set\n",
      "User d29ee932b7 is not in the train set\n",
      "User d38901788c is not in the train set\n",
      "User d7041c219f is not in the train set\n",
      "User d85eb27c0c is not in the train set\n",
      "User daf33743af is not in the train set\n",
      "User dc82539834 is not in the train set\n",
      "User e33d974aae is not in the train set\n",
      "User e48e132073 is not in the train set\n",
      "User e5ae7b1f18 is not in the train set\n",
      "User e6da32eef0 is not in the train set\n",
      "User e9180dc77c is not in the train set\n",
      "User e96c7de8f6 is not in the train set\n",
      "User e9c46fbd9f is not in the train set\n",
      "User ee76626ee1 is not in the train set\n",
      "User ef84829515 is not in the train set\n",
      "User f3951984ba is not in the train set\n",
      "User f4573fc71c is not in the train set\n",
      "User f471223d1a is not in the train set\n",
      "User f48379ea32 is not in the train set\n",
      "User f4acda19aa is not in the train set\n",
      "User f58daf1848 is not in the train set\n",
      "User f7bf6074f6 is not in the train set\n",
      "User f843b38155 is not in the train set\n",
      "User f846ad08fc is not in the train set\n",
      "User f8ea2e8463 is not in the train set\n",
      "User fa2093fecd is not in the train set\n",
      "User fb2e203234 is not in the train set\n",
      "User fbe486dc4c is not in the train set\n",
      "User fc1dc4549d is not in the train set\n",
      "User fc22130974 is not in the train set\n",
      "User fd9dcf1d14 is not in the train set\n",
      "User ffdd1df879 is not in the train set\n"
     ]
    }
   ],
   "source": [
    "target_prediction = targets.copy()\n",
    "target_prediction[\"Rating\"] = -1\n",
    "\n",
    "user_id_list = targets.UserId.unique()\n",
    "for user_id in user_id_list:\n",
    "    # Get the train dataframe index of the user to predict\n",
    "    user_index = ratio_split.train_set.uid_map.get(user_id)\n",
    "\n",
    "    if user_index is None:\n",
    "        print(f\"User {user_id} is not in the train set\")\n",
    "        continue\n",
    "\n",
    "    # Flter by items to predict \n",
    "    items_to_predict = targets.loc[targets.UserId == user_id, \"ItemId\"].to_list()\n",
    "\n",
    "    # Get the train dataframe index of the items to predict\n",
    "    items_to_predict_index = np.array([ratio_split.train_set.iid_map.get(item_id) for item_id in items_to_predict])\n",
    "\n",
    "    items_to_predict_tensor = torch.tensor([idx for idx in items_to_predict_index if idx is not None])\n",
    "\n",
    "    # Get the position of items that are not in the train set\n",
    "    none_indices = [i for i, x in enumerate(items_to_predict_index) if x is None]\n",
    "\n",
    "    # Get the prediction for the items\n",
    "    line_rating = dmrl_recommender.score(user_index=user_index, item_indices=items_to_predict_tensor)\n",
    "\n",
    "    # Insert -1 in the position of items that are not in the train set\n",
    "    for index_to_insert in none_indices:\n",
    "        line_rating = np.insert(line_rating, index_to_insert, -1)\n",
    "\n",
    "    # Insert the prediction in the target_prediction dataframe\n",
    "    target_prediction.loc[targets.UserId == user_id, \"Rating\"] = line_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = target_prediction.sort_values([\"UserId\", \"Rating\"], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction.to_csv(\"submissao_2_DMRL.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = target_prediction.drop(columns=\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction.to_csv(\"submissao_2_DMRL_sem_rating.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sistema_recomendacao_tp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
