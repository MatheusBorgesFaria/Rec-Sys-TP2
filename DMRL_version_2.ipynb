{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disentangled Multimodal Representation Learning for Recommendation (DMRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideias:\n",
    "- tunar os hiperparametros\n",
    "- paralelizar a previsao\n",
    "- Adicionar a coluna do assunto em conjunto com o texto\n",
    "- Limpar os dados de conteudo\n",
    "- usa a imagem alem do texto\n",
    "- renomear as colunas para ter um nome mais representativo para o bert\n",
    "- usar algum algoritmo mais simples apenas para usuarios novos, knn para content based\n",
    "- ordenar os itens novos que ficaram por ultimo por popularidade \n",
    "- Entender o parametro exclude_unknowns=True do RatioSplit e se tem alguma forma do DMRL gerar previsoes para itens e usuarios novos \n",
    "\n",
    "Perguntas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Larissa\\miniconda3a\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cornac\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cornac.metrics import NDCG\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.data import TextModality\n",
    "from cornac.models.dmrl.recom_dmrl import DMRL\n",
    "\n",
    "from utils import load_data, preprocessing_content_data\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings, content, targets = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[\"TimestampDate\"] = ratings['Timestamp'].dt.date\n",
    "ratings.loc[ratings.Rating == 0, \"Rating\"] = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpar os dados\n",
    "\n",
    "- FEITO - Year: deletar o - no texto '2013–'\n",
    "- FEITO - Rated: alterar as diversas formas de escrever NA para NA. Esse e um caso especial\n",
    "- FEITO - alterar as diversas formas de escrever NA para None em todas as coluans\n",
    "- FEITO - Language: tem varias linhas que possuem bizarices como  'None, English' e 'None, French' , 'English, None'...\n",
    "- FEITO - Ratings: criar uma coluna para cada chave do dicionario, entender quais sao todas as chaves que existem\n",
    "\n",
    "-----------------------------------------------\n",
    "Variaveis qwue nao precisariam ser tratadas com um bert:\n",
    "- Metascore\n",
    "- imdbRating\n",
    "- Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar = content.drop(columns=[\"Poster\", \"Website\", \"Response\", \"Episode\", \"seriesID\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar['Year'] = content_auxiliar['Year'].str.replace('–', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = content.totalSeasons.unique()[0]\n",
    "dict_transform_to_na = {\n",
    "    \"Rated\":['N/A', 'Not Rated', 'Unrated', 'UNRATED', 'NOT RATED'],\n",
    "    \"all\": [nan, 'N/A', 'None', np.nan],\n",
    "}\n",
    "\n",
    "for na_value in dict_transform_to_na[\"all\"]:\n",
    "    content_auxiliar = content_auxiliar.replace(na_value, None)\n",
    "\n",
    "for na_value in dict_transform_to_na[\"Rated\"]:\n",
    "    content_auxiliar['Rated'] = content_auxiliar['Rated'].replace(na_value, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar['Language'] = content_auxiliar['Language'].str.replace('None, ', '')\n",
    "content_auxiliar['Language'] = content_auxiliar['Language'].str.replace(', None', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Internet Movie Database', 'Metacritic', 'Rotten Tomatoes'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entendendo os valores possiveis para a coluna Ratings\n",
    "# A coluna content_auxiliar.Ratings quarda uma lista que posde ter entre 0 e 3 dicionarios. Cada dicionario possui a chave 'Source', 'Value'.\n",
    "num_ratings_per_item = []\n",
    "unique_keys = []\n",
    "rating_sources = []\n",
    "rating_values = []\n",
    "\n",
    "for rating_list in content_auxiliar.Ratings:\n",
    "    num_ratings_per_item.append(len(rating_list))\n",
    "    for rating_dict in rating_list:\n",
    "        for key in rating_dict:\n",
    "            unique_keys.append(key)\n",
    "        rating_sources.append(rating_dict['Source'])\n",
    "        rating_values.append(rating_dict['Value'])\n",
    "\n",
    "set(rating_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "InternetMovieDatabase_list = []\n",
    "Metacritic_list = []\n",
    "RottenTomatoes_list = []\n",
    "for rating_list in content_auxiliar.Ratings:\n",
    "    InternetMovieDatabase_list.append(None)\n",
    "    Metacritic_list.append(None)\n",
    "    RottenTomatoes_list.append(None)\n",
    "    for rating_dict in rating_list:\n",
    "        if rating_dict['Source'] == 'Internet Movie Database':\n",
    "            InternetMovieDatabase_list[-1] = rating_dict['Value']\n",
    "        elif rating_dict['Source'] == 'Metacritic':\n",
    "            Metacritic_list[-1] = rating_dict['Value']\n",
    "        elif rating_dict['Source'] == 'Rotten Tomatoes':\n",
    "            RottenTomatoes_list[-1] = rating_dict['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar['Internet Movie Database'] = InternetMovieDatabase_list\n",
    "content_auxiliar['Metacritic'] = Metacritic_list\n",
    "content_auxiliar['Rotten Tomatoes'] = RottenTomatoes_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_auxiliar.drop(columns=['Ratings'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apendar a coluna no valor do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ItemId'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_columns = content_auxiliar.columns.to_list()\n",
    "content_columns.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in content_columns:\n",
    "    content_auxiliar[column] = content_auxiliar[column].apply(lambda x: f\"{column}: {x}; \" if x is not None else f\"{column}: unknown value; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_processed = content_auxiliar[['ItemId']].copy()\n",
    "content_processed[\"text\"] = content_auxiliar[content_columns].astype(str).fillna('').agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Rating</th>\n",
       "      <th>TimestampDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c4ca4238a0</td>\n",
       "      <td>91766eac45</td>\n",
       "      <td>2013-10-05 22:00:50</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2013-10-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c81e728d9d</td>\n",
       "      <td>5c739554f7</td>\n",
       "      <td>2013-08-17 16:26:38</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2013-08-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c81e728d9d</td>\n",
       "      <td>48f6d7ce7c</td>\n",
       "      <td>2013-08-17 13:28:27</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2013-08-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c81e728d9d</td>\n",
       "      <td>e9318d627a</td>\n",
       "      <td>2013-06-15 15:38:09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-06-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a87ff679a2</td>\n",
       "      <td>17e6357973</td>\n",
       "      <td>2014-01-31 23:27:59</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014-01-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       UserId      ItemId           Timestamp  Rating TimestampDate\n",
       "0  c4ca4238a0  91766eac45 2013-10-05 22:00:50     8.0    2013-10-05\n",
       "1  c81e728d9d  5c739554f7 2013-08-17 16:26:38     9.0    2013-08-17\n",
       "2  c81e728d9d  48f6d7ce7c 2013-08-17 13:28:27     8.0    2013-08-17\n",
       "3  c81e728d9d  e9318d627a 2013-06-15 15:38:09     1.0    2013-06-15\n",
       "4  a87ff679a2  17e6357973 2014-01-31 23:27:59     8.0    2014-01-31"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51671, 29674)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique users and items\n",
    "ratings.UserId.nunique(), ratings.ItemId.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     659392\n",
       "2         54\n",
       "3         14\n",
       "6          3\n",
       "7          2\n",
       "11         2\n",
       "4          2\n",
       "22         1\n",
       "28         1\n",
       "20         1\n",
       "8          1\n",
       "38         1\n",
       "Name: ItemId, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many itens purchased by each user purchase\n",
    "ratings.groupby([\"UserId\", 'Timestamp'])[\"ItemId\"].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      420843\n",
       "2       60533\n",
       "3       14547\n",
       "4        4755\n",
       "5        2065\n",
       "        ...  \n",
       "60          1\n",
       "363         1\n",
       "145         1\n",
       "189         1\n",
       "82          1\n",
       "Name: ItemId, Length: 88, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many itens purchased by each user day by day\n",
    "ratings.groupby([\"UserId\", 'TimestampDate'])[\"ItemId\"].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      23092\n",
       "2       6193\n",
       "3       3341\n",
       "4       2229\n",
       "5       1646\n",
       "       ...  \n",
       "471        1\n",
       "427        1\n",
       "321        1\n",
       "429        1\n",
       "392        1\n",
       "Name: Timestamp, Length: 440, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many times each user purchased items\n",
    "ratings.groupby(\"UserId\")['Timestamp'].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      25113\n",
       "2       6048\n",
       "3       3158\n",
       "4       2187\n",
       "5       1579\n",
       "       ...  \n",
       "420        1\n",
       "198        1\n",
       "602        1\n",
       "224        1\n",
       "332        1\n",
       "Name: TimestampDate, Length: 341, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many times each user purchased items per day\n",
    "ratings.groupby(\"UserId\")['TimestampDate'].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ItemId              0\n",
       "Title               0\n",
       "Year                0\n",
       "Rated               0\n",
       "Released            0\n",
       "Runtime             0\n",
       "Genre               0\n",
       "Director            0\n",
       "Writer              0\n",
       "Actors              0\n",
       "Plot                0\n",
       "Language            0\n",
       "Country             0\n",
       "Awards              0\n",
       "Poster              0\n",
       "Ratings             0\n",
       "Metascore           0\n",
       "imdbRating          0\n",
       "imdbVotes           0\n",
       "Type                0\n",
       "DVD                24\n",
       "BoxOffice          24\n",
       "Production         24\n",
       "Website            24\n",
       "Response            0\n",
       "totalSeasons    37989\n",
       "Season          38011\n",
       "Episode         38011\n",
       "seriesID        38011\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.  ,  9.  ,  1.  ,  7.  ,  6.  , 10.  ,  5.  ,  4.  ,  2.  ,\n",
       "        3.  ,  0.01])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.Rating.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_text_modality = TextModality(\n",
    "    corpus=content_processed.text.to_list(),\n",
    "    ids=content_processed.ItemId.to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating_threshold = 0.5\n",
      "exclude_unknowns = True\n",
      "---\n",
      "Training data:\n",
      "Number of users = 46750\n",
      "Number of items = 27045\n",
      "Number of ratings = 527776\n",
      "Max rating = 10.0\n",
      "Min rating = 0.0\n",
      "Global mean = 7.3\n",
      "---\n",
      "Test data:\n",
      "Number of users = 46750\n",
      "Number of items = 27045\n",
      "Number of ratings = 124031\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Total users = 46750\n",
      "Total items = 27045\n"
     ]
    }
   ],
   "source": [
    "ratio_split = RatioSplit(\n",
    "    data=ratings[['UserId', 'ItemId', 'Rating']].values.tolist(),\n",
    "    test_size=0.2,\n",
    "    exclude_unknowns=True,\n",
    "    verbose=True,\n",
    "    seed=12012001,\n",
    "    rating_threshold=0.5,\n",
    "    item_text=item_text_modality,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate DMRL recommender\n",
    "dmrl_recommender = DMRL(\n",
    "    batch_size=1024,\n",
    "    epochs=20,\n",
    "    log_metrics=False,\n",
    "    learning_rate=0.1,\n",
    "    num_factors=2,\n",
    "    decay_r=0.0001,\n",
    "    decay_c=0.001,\n",
    "    num_neg=4,\n",
    "    embedding_dim=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DMRL] Training started!\n",
      "Pre-encoding the entire corpus. This might take a while.\n",
      "Using device cpu for training\n",
      "  batch 5 loss: 714.3922241210937\n",
      "  batch 10 loss: 737.5060668945313\n",
      "  batch 15 loss: 790.3735595703125\n",
      "  batch 20 loss: 864.58916015625\n",
      "  batch 25 loss: 877.6736572265625\n",
      "  batch 30 loss: 810.6030151367188\n",
      "  batch 35 loss: 764.0460815429688\n",
      "  batch 40 loss: 727.0631713867188\n",
      "  batch 45 loss: 714.0484008789062\n",
      "  batch 50 loss: 705.6056518554688\n",
      "  batch 55 loss: 701.7337646484375\n",
      "  batch 60 loss: 690.3317138671875\n",
      "  batch 65 loss: 677.1714477539062\n",
      "  batch 70 loss: 680.3330810546875\n",
      "  batch 75 loss: 676.5327392578125\n",
      "  batch 80 loss: 671.8184814453125\n",
      "  batch 85 loss: 650.7092407226562\n",
      "  batch 90 loss: 649.2645263671875\n",
      "  batch 95 loss: 651.9765380859375\n",
      "  batch 100 loss: 637.3532470703125\n",
      "  batch 105 loss: 628.8649047851562\n",
      "  batch 110 loss: 623.105859375\n",
      "  batch 115 loss: 614.1168579101562\n",
      "  batch 120 loss: 605.641064453125\n",
      "  batch 125 loss: 621.0272583007812\n",
      "  batch 130 loss: 617.7872802734375\n",
      "  batch 135 loss: 614.586083984375\n",
      "  batch 140 loss: 599.4906494140625\n",
      "  batch 145 loss: 600.3583129882812\n",
      "  batch 150 loss: 593.6350463867187\n",
      "  batch 155 loss: 586.7304565429688\n",
      "  batch 160 loss: 583.0970947265625\n",
      "  batch 165 loss: 572.4094360351562\n",
      "  batch 170 loss: 571.0369506835938\n",
      "  batch 175 loss: 557.2593872070313\n",
      "  batch 180 loss: 559.3906127929688\n",
      "  batch 185 loss: 552.4647094726563\n",
      "  batch 190 loss: 553.8599853515625\n",
      "  batch 195 loss: 546.1635009765625\n",
      "  batch 200 loss: 534.601611328125\n",
      "  batch 205 loss: 528.7083740234375\n",
      "  batch 210 loss: 532.753173828125\n",
      "  batch 215 loss: 523.2175048828125\n",
      "  batch 220 loss: 516.8929992675781\n",
      "  batch 225 loss: 502.0810546875\n",
      "  batch 230 loss: 513.7532470703125\n",
      "  batch 235 loss: 513.4258056640625\n",
      "  batch 240 loss: 506.18565673828124\n",
      "  batch 245 loss: 504.5043090820312\n",
      "  batch 250 loss: 495.1581970214844\n",
      "  batch 255 loss: 497.7815246582031\n",
      "  batch 260 loss: 502.8070068359375\n",
      "  batch 265 loss: 488.8885437011719\n",
      "  batch 270 loss: 486.3590454101562\n",
      "  batch 275 loss: 487.8238220214844\n",
      "  batch 280 loss: 482.199169921875\n",
      "  batch 285 loss: 475.38665771484375\n",
      "  batch 290 loss: 482.9886779785156\n",
      "  batch 295 loss: 476.26178588867185\n",
      "  batch 300 loss: 462.6754211425781\n",
      "  batch 305 loss: 471.55296630859374\n",
      "  batch 310 loss: 467.5832946777344\n",
      "  batch 315 loss: 469.2941528320313\n",
      "  batch 320 loss: 479.3813171386719\n",
      "  batch 325 loss: 461.77891235351564\n",
      "  batch 330 loss: 452.08441162109375\n",
      "  batch 335 loss: 463.5490783691406\n",
      "  batch 340 loss: 461.56439819335935\n",
      "  batch 345 loss: 445.51636962890626\n",
      "  batch 350 loss: 461.71910400390624\n",
      "  batch 355 loss: 451.68651123046874\n",
      "  batch 360 loss: 454.9356201171875\n",
      "  batch 365 loss: 464.9069580078125\n",
      "  batch 370 loss: 451.6968200683594\n",
      "  batch 375 loss: 463.5297058105469\n",
      "  batch 380 loss: 446.89434204101565\n",
      "  batch 385 loss: 448.9134887695312\n",
      "  batch 390 loss: 457.9353515625\n",
      "  batch 395 loss: 464.2600341796875\n",
      "  batch 400 loss: 500.45616455078124\n",
      "  batch 405 loss: 458.74583129882814\n",
      "  batch 410 loss: 449.780517578125\n",
      "  batch 415 loss: 479.08704833984376\n",
      "  batch 420 loss: 456.97068481445314\n",
      "  batch 425 loss: 462.70635375976565\n",
      "  batch 430 loss: 442.760498046875\n",
      "  batch 435 loss: 456.9664245605469\n",
      "  batch 440 loss: 460.2974792480469\n",
      "  batch 445 loss: 446.7347839355469\n",
      "  batch 450 loss: 444.53391723632814\n",
      "  batch 455 loss: 443.01288452148435\n",
      "  batch 460 loss: 451.267724609375\n",
      "  batch 465 loss: 446.4581604003906\n",
      "  batch 470 loss: 435.2396728515625\n",
      "  batch 475 loss: 454.1361511230469\n",
      "  batch 480 loss: 446.8299865722656\n",
      "  batch 485 loss: 436.7847106933594\n",
      "  batch 490 loss: 444.42669677734375\n",
      "  batch 495 loss: 448.4092224121094\n",
      "  batch 500 loss: 442.28553466796876\n",
      "  batch 505 loss: 424.8682067871094\n",
      "  batch 510 loss: 427.0400146484375\n",
      "  batch 515 loss: 429.97021484375\n",
      "Epoch: 0 is done\n",
      "  batch 5 loss: 337.8666015625\n",
      "  batch 10 loss: 348.1565002441406\n",
      "  batch 15 loss: 349.5618469238281\n",
      "  batch 20 loss: 331.6091369628906\n",
      "  batch 25 loss: 329.04681396484375\n",
      "  batch 30 loss: 344.5881103515625\n",
      "  batch 35 loss: 348.86517333984375\n",
      "  batch 40 loss: 337.06552734375\n",
      "  batch 45 loss: 341.3828491210937\n",
      "  batch 50 loss: 344.4253234863281\n",
      "  batch 55 loss: 329.60369873046875\n",
      "  batch 60 loss: 356.3722412109375\n",
      "  batch 65 loss: 327.077099609375\n",
      "  batch 70 loss: 335.8440368652344\n",
      "  batch 75 loss: 338.4820556640625\n",
      "  batch 80 loss: 332.340283203125\n",
      "  batch 85 loss: 356.96343383789065\n",
      "  batch 90 loss: 340.3334045410156\n",
      "  batch 95 loss: 333.1524169921875\n",
      "  batch 100 loss: 340.69190673828126\n",
      "  batch 105 loss: 323.2589050292969\n",
      "  batch 110 loss: 336.8420654296875\n",
      "  batch 115 loss: 345.100341796875\n",
      "  batch 120 loss: 352.79407958984376\n",
      "  batch 125 loss: 332.82537841796875\n",
      "  batch 130 loss: 331.46721801757815\n",
      "  batch 135 loss: 342.48546142578124\n",
      "  batch 140 loss: 350.66664428710936\n",
      "  batch 145 loss: 340.53330078125\n",
      "  batch 150 loss: 338.1664672851563\n",
      "  batch 155 loss: 340.1017761230469\n",
      "  batch 160 loss: 339.28541870117186\n",
      "  batch 165 loss: 338.86301879882814\n",
      "  batch 170 loss: 338.0030578613281\n",
      "  batch 175 loss: 336.39332275390626\n",
      "  batch 180 loss: 355.18593139648436\n",
      "  batch 185 loss: 335.28986206054685\n",
      "  batch 190 loss: 346.44130859375\n",
      "  batch 195 loss: 345.6710693359375\n",
      "  batch 200 loss: 352.29680786132815\n",
      "  batch 205 loss: 339.66167602539065\n",
      "  batch 210 loss: 344.1541381835938\n",
      "  batch 215 loss: 335.38551635742186\n",
      "  batch 220 loss: 340.7434875488281\n",
      "  batch 225 loss: 324.46196899414065\n",
      "  batch 230 loss: 332.2921508789062\n",
      "  batch 235 loss: 350.3041748046875\n",
      "  batch 240 loss: 338.041455078125\n",
      "  batch 245 loss: 342.20281982421875\n",
      "  batch 250 loss: 339.9316101074219\n",
      "  batch 255 loss: 341.5828857421875\n",
      "  batch 260 loss: 345.7625427246094\n",
      "  batch 265 loss: 331.7336120605469\n",
      "  batch 270 loss: 340.4050048828125\n",
      "  batch 275 loss: 347.51634521484374\n",
      "  batch 280 loss: 341.71763305664064\n",
      "  batch 285 loss: 345.5783935546875\n",
      "  batch 290 loss: 320.14113159179686\n",
      "  batch 295 loss: 361.1105895996094\n",
      "  batch 300 loss: 350.4039733886719\n",
      "  batch 305 loss: 339.61432495117185\n",
      "  batch 310 loss: 339.0584350585938\n",
      "  batch 315 loss: 339.93068237304686\n",
      "  batch 320 loss: 338.02410888671875\n",
      "  batch 325 loss: 341.47686767578125\n",
      "  batch 330 loss: 343.07056884765626\n",
      "  batch 335 loss: 338.44892578125\n",
      "  batch 340 loss: 336.6442077636719\n",
      "  batch 345 loss: 340.61028442382815\n",
      "  batch 350 loss: 333.13335571289065\n",
      "  batch 355 loss: 357.87196655273436\n",
      "  batch 360 loss: 335.91681518554685\n",
      "  batch 365 loss: 337.8187561035156\n",
      "  batch 370 loss: 339.4615112304688\n",
      "  batch 375 loss: 328.8535583496094\n",
      "  batch 380 loss: 330.4904846191406\n",
      "  batch 385 loss: 344.6250732421875\n",
      "  batch 390 loss: 319.90590209960936\n",
      "  batch 395 loss: 346.4538146972656\n",
      "  batch 400 loss: 347.00685424804686\n",
      "  batch 405 loss: 334.788525390625\n",
      "  batch 410 loss: 351.50440673828126\n",
      "  batch 415 loss: 337.991943359375\n",
      "  batch 420 loss: 331.2236755371094\n",
      "  batch 425 loss: 346.96942138671875\n",
      "  batch 430 loss: 340.43212890625\n",
      "  batch 435 loss: 330.01397705078125\n",
      "  batch 440 loss: 355.10841064453126\n",
      "  batch 445 loss: 342.4231262207031\n",
      "  batch 450 loss: 329.27525024414064\n",
      "  batch 455 loss: 345.3693054199219\n",
      "  batch 460 loss: 340.95183715820315\n",
      "  batch 465 loss: 341.65069580078125\n",
      "  batch 470 loss: 328.2235412597656\n",
      "  batch 475 loss: 333.74546508789064\n",
      "  batch 480 loss: 354.0751525878906\n",
      "  batch 485 loss: 346.803466796875\n",
      "  batch 490 loss: 331.9428955078125\n",
      "  batch 495 loss: 352.06788940429686\n",
      "  batch 500 loss: 339.97401123046876\n",
      "  batch 505 loss: 348.9486083984375\n",
      "  batch 510 loss: 333.9554748535156\n",
      "  batch 515 loss: 361.4194580078125\n",
      "Epoch: 1 is done\n",
      "  batch 5 loss: 257.0522430419922\n",
      "  batch 10 loss: 243.50531921386718\n",
      "  batch 15 loss: 253.40503540039063\n",
      "  batch 20 loss: 245.68702087402343\n",
      "  batch 25 loss: 248.723095703125\n",
      "  batch 30 loss: 249.9406524658203\n",
      "  batch 35 loss: 229.08331909179688\n",
      "  batch 40 loss: 256.55246887207034\n",
      "  batch 45 loss: 259.86651306152345\n",
      "  batch 50 loss: 234.13840942382814\n",
      "  batch 55 loss: 241.1534881591797\n",
      "  batch 60 loss: 243.9261932373047\n",
      "  batch 65 loss: 234.51435546875\n",
      "  batch 70 loss: 246.8442413330078\n",
      "  batch 75 loss: 234.00191345214844\n",
      "  batch 80 loss: 239.6246765136719\n",
      "  batch 85 loss: 236.54002685546874\n",
      "  batch 90 loss: 243.63839721679688\n",
      "  batch 95 loss: 239.69660339355468\n",
      "  batch 100 loss: 254.96570434570313\n",
      "  batch 105 loss: 237.58098449707032\n",
      "  batch 110 loss: 248.3015350341797\n",
      "  batch 115 loss: 252.8087890625\n",
      "  batch 120 loss: 248.10669555664063\n",
      "  batch 125 loss: 250.14049072265624\n",
      "  batch 130 loss: 240.2946350097656\n",
      "  batch 135 loss: 245.18767395019532\n",
      "  batch 140 loss: 241.2547821044922\n",
      "  batch 145 loss: 242.7663604736328\n",
      "  batch 150 loss: 238.41048583984374\n",
      "  batch 155 loss: 251.35637817382812\n",
      "  batch 160 loss: 248.24588012695312\n",
      "  batch 165 loss: 246.57672119140625\n",
      "  batch 170 loss: 253.29704895019532\n",
      "  batch 175 loss: 260.1077880859375\n",
      "  batch 180 loss: 269.84531860351564\n",
      "  batch 185 loss: 286.559375\n",
      "  batch 190 loss: 258.9472320556641\n",
      "  batch 195 loss: 249.4498779296875\n",
      "  batch 200 loss: 245.09400024414063\n",
      "  batch 205 loss: 253.708154296875\n",
      "  batch 210 loss: 255.23271789550782\n",
      "  batch 215 loss: 265.0177459716797\n",
      "  batch 220 loss: 249.38966064453126\n",
      "  batch 225 loss: 260.7403533935547\n",
      "  batch 230 loss: 265.71958923339844\n",
      "  batch 235 loss: 247.29428405761718\n",
      "  batch 240 loss: 262.65887145996095\n",
      "  batch 245 loss: 256.98377380371096\n",
      "  batch 250 loss: 258.0273132324219\n",
      "  batch 255 loss: 242.9717010498047\n",
      "  batch 260 loss: 269.69797668457034\n",
      "  batch 265 loss: 268.0897735595703\n",
      "  batch 270 loss: 264.0182220458984\n",
      "  batch 275 loss: 255.66439208984374\n",
      "  batch 280 loss: 273.16172485351564\n",
      "  batch 285 loss: 276.2903198242187\n",
      "  batch 290 loss: 273.85849914550784\n",
      "  batch 295 loss: 255.15531311035156\n",
      "  batch 300 loss: 268.13519287109375\n",
      "  batch 305 loss: 257.21572265625\n",
      "  batch 310 loss: 266.30682373046875\n",
      "  batch 315 loss: 266.4888458251953\n",
      "  batch 320 loss: 257.9433868408203\n",
      "  batch 325 loss: 285.0102783203125\n",
      "  batch 330 loss: 276.05463256835935\n",
      "  batch 335 loss: 266.00810546875\n",
      "  batch 340 loss: 254.18740234375\n",
      "  batch 345 loss: 260.7437408447266\n",
      "  batch 350 loss: 253.65699157714843\n",
      "  batch 355 loss: 270.54925231933595\n",
      "  batch 360 loss: 262.8891235351563\n",
      "  batch 365 loss: 288.7617919921875\n",
      "  batch 370 loss: 261.6145263671875\n",
      "  batch 375 loss: 258.3748840332031\n",
      "  batch 380 loss: 260.9938262939453\n",
      "  batch 385 loss: 264.9563903808594\n",
      "  batch 390 loss: 270.8083221435547\n",
      "  batch 395 loss: 273.031298828125\n",
      "  batch 400 loss: 262.69797973632814\n",
      "  batch 405 loss: 273.8588928222656\n",
      "  batch 410 loss: 266.56089782714844\n",
      "  batch 415 loss: 266.84874267578124\n",
      "  batch 420 loss: 273.7782348632812\n",
      "  batch 425 loss: 268.5601745605469\n",
      "  batch 430 loss: 275.0955474853516\n",
      "  batch 435 loss: 276.1466400146484\n",
      "  batch 440 loss: 290.6734985351562\n",
      "  batch 445 loss: 271.04998168945315\n",
      "  batch 450 loss: 264.8771087646484\n",
      "  batch 455 loss: 253.06421508789063\n",
      "  batch 460 loss: 282.51373291015625\n",
      "  batch 465 loss: 268.59071960449216\n",
      "  batch 470 loss: 263.7136657714844\n",
      "  batch 475 loss: 273.16949768066405\n",
      "  batch 480 loss: 276.68377685546875\n",
      "  batch 485 loss: 262.58722229003905\n",
      "  batch 490 loss: 271.93861694335936\n",
      "  batch 495 loss: 259.8918762207031\n",
      "  batch 500 loss: 253.0655090332031\n",
      "  batch 505 loss: 279.1230834960937\n",
      "  batch 510 loss: 278.27565612792966\n",
      "  batch 515 loss: 257.6682983398438\n",
      "Epoch: 2 is done\n",
      "  batch 5 loss: 185.37342529296876\n",
      "  batch 10 loss: 188.92822875976563\n",
      "  batch 15 loss: 195.1595001220703\n",
      "  batch 20 loss: 212.98821716308595\n",
      "  batch 25 loss: 201.51326904296874\n",
      "  batch 30 loss: 186.7138885498047\n",
      "  batch 35 loss: 192.28504943847656\n",
      "  batch 40 loss: 183.63202514648438\n",
      "  batch 45 loss: 188.3711944580078\n",
      "  batch 50 loss: 206.09380493164062\n",
      "  batch 55 loss: 182.86188354492188\n",
      "  batch 60 loss: 190.78925476074218\n",
      "  batch 65 loss: 185.59108276367186\n",
      "  batch 70 loss: 184.65731201171874\n",
      "  batch 75 loss: 189.2315643310547\n",
      "  batch 80 loss: 175.3651885986328\n",
      "  batch 85 loss: 211.23974609375\n",
      "  batch 90 loss: 187.03412780761718\n",
      "  batch 95 loss: 198.3355285644531\n",
      "  batch 100 loss: 198.98403930664062\n",
      "  batch 105 loss: 199.31037292480468\n",
      "  batch 110 loss: 193.46542053222657\n",
      "  batch 115 loss: 202.1057586669922\n",
      "  batch 120 loss: 193.44541015625\n",
      "  batch 125 loss: 197.54078979492186\n",
      "  batch 130 loss: 193.22604675292968\n",
      "  batch 135 loss: 218.5945587158203\n",
      "  batch 140 loss: 194.37127685546875\n",
      "  batch 145 loss: 194.6263458251953\n",
      "  batch 150 loss: 197.2383819580078\n",
      "  batch 155 loss: 195.41445922851562\n",
      "  batch 160 loss: 198.53213500976562\n",
      "  batch 165 loss: 194.69346923828124\n",
      "  batch 170 loss: 200.3784606933594\n",
      "  batch 175 loss: 212.90057373046875\n",
      "  batch 180 loss: 194.49317626953126\n",
      "  batch 185 loss: 207.27881469726563\n",
      "  batch 190 loss: 197.35831298828126\n",
      "  batch 195 loss: 202.6518310546875\n",
      "  batch 200 loss: 203.8773406982422\n",
      "  batch 205 loss: 205.18484191894532\n",
      "  batch 210 loss: 208.66546936035155\n",
      "  batch 215 loss: 204.12704772949218\n",
      "  batch 220 loss: 214.0156280517578\n",
      "  batch 225 loss: 206.89718322753907\n",
      "  batch 230 loss: 201.83407897949218\n",
      "  batch 235 loss: 208.9803253173828\n",
      "  batch 240 loss: 198.43880615234374\n",
      "  batch 245 loss: 202.67987670898438\n",
      "  batch 250 loss: 201.61717834472657\n",
      "  batch 255 loss: 215.74322204589845\n",
      "  batch 260 loss: 198.4107879638672\n",
      "  batch 265 loss: 199.2364471435547\n",
      "  batch 270 loss: 202.7204620361328\n",
      "  batch 275 loss: 198.95493774414064\n",
      "  batch 280 loss: 210.5377990722656\n",
      "  batch 285 loss: 204.23710021972656\n",
      "  batch 290 loss: 215.12589111328126\n",
      "  batch 295 loss: 202.09232177734376\n",
      "  batch 300 loss: 204.01030578613282\n",
      "  batch 305 loss: 212.31767883300782\n",
      "  batch 310 loss: 196.41970520019532\n",
      "  batch 315 loss: 214.26936950683594\n",
      "  batch 320 loss: 226.16085510253907\n",
      "  batch 325 loss: 199.3793182373047\n",
      "  batch 330 loss: 218.7218017578125\n",
      "  batch 335 loss: 204.55518798828126\n",
      "  batch 340 loss: 195.175244140625\n",
      "  batch 345 loss: 219.09200134277344\n",
      "  batch 350 loss: 213.971826171875\n",
      "  batch 355 loss: 221.48178100585938\n",
      "  batch 360 loss: 200.89337768554688\n",
      "  batch 365 loss: 221.6470977783203\n",
      "  batch 370 loss: 209.74069519042968\n",
      "  batch 375 loss: 224.163671875\n",
      "  batch 380 loss: 201.3501007080078\n",
      "  batch 385 loss: 220.128125\n",
      "  batch 390 loss: 214.67001037597657\n",
      "  batch 395 loss: 218.7735107421875\n",
      "  batch 400 loss: 217.38143920898438\n",
      "  batch 405 loss: 226.1912841796875\n",
      "  batch 410 loss: 220.22344055175782\n",
      "  batch 415 loss: 214.25845336914062\n",
      "  batch 420 loss: 202.11893005371093\n",
      "  batch 425 loss: 208.1781433105469\n",
      "  batch 430 loss: 212.82476806640625\n",
      "  batch 435 loss: 224.9572784423828\n",
      "  batch 440 loss: 205.11990356445312\n",
      "  batch 445 loss: 217.0050262451172\n",
      "  batch 450 loss: 224.74786682128905\n",
      "  batch 455 loss: 212.6753143310547\n",
      "  batch 460 loss: 229.90048828125\n",
      "  batch 465 loss: 228.7328674316406\n",
      "  batch 470 loss: 224.2755920410156\n",
      "  batch 475 loss: 201.58159790039062\n",
      "  batch 480 loss: 202.92632446289062\n",
      "  batch 485 loss: 216.72379455566406\n",
      "  batch 490 loss: 232.50088806152343\n",
      "  batch 495 loss: 220.0139953613281\n",
      "  batch 500 loss: 212.87074279785156\n",
      "  batch 505 loss: 217.2001953125\n",
      "  batch 510 loss: 222.6575439453125\n",
      "  batch 515 loss: 209.48381958007812\n",
      "Epoch: 3 is done\n",
      "  batch 5 loss: 151.69717407226562\n",
      "  batch 10 loss: 134.36490936279296\n",
      "  batch 15 loss: 145.77804565429688\n",
      "  batch 20 loss: 151.38543395996095\n",
      "  batch 25 loss: 145.5860626220703\n",
      "  batch 30 loss: 162.12013549804686\n",
      "  batch 35 loss: 152.44435119628906\n",
      "  batch 40 loss: 140.60376586914063\n",
      "  batch 45 loss: 149.1915252685547\n",
      "  batch 50 loss: 159.80347900390626\n",
      "  batch 55 loss: 147.68260803222657\n",
      "  batch 60 loss: 153.6559310913086\n",
      "  batch 65 loss: 153.33965454101562\n",
      "  batch 70 loss: 161.80590209960937\n",
      "  batch 75 loss: 156.9543701171875\n",
      "  batch 80 loss: 147.25504302978516\n",
      "  batch 85 loss: 156.13939819335937\n",
      "  batch 90 loss: 155.81569061279296\n",
      "  batch 95 loss: 156.55685119628907\n",
      "  batch 100 loss: 154.95446166992187\n",
      "  batch 105 loss: 140.834228515625\n",
      "  batch 110 loss: 150.62959899902344\n",
      "  batch 115 loss: 163.89272766113282\n",
      "  batch 120 loss: 159.3503204345703\n",
      "  batch 125 loss: 170.5059814453125\n",
      "  batch 130 loss: 175.13202819824218\n",
      "  batch 135 loss: 162.02557373046875\n",
      "  batch 140 loss: 167.69796142578124\n",
      "  batch 145 loss: 155.1469757080078\n",
      "  batch 150 loss: 167.0999755859375\n",
      "  batch 155 loss: 162.72708129882812\n",
      "  batch 160 loss: 163.77979736328126\n",
      "  batch 165 loss: 171.89791259765624\n",
      "  batch 170 loss: 165.29690551757812\n",
      "  batch 175 loss: 148.1789520263672\n",
      "  batch 180 loss: 161.22687377929688\n",
      "  batch 185 loss: 169.46898498535157\n",
      "  batch 190 loss: 154.20831909179688\n",
      "  batch 195 loss: 170.82469177246094\n",
      "  batch 200 loss: 161.63705139160157\n",
      "  batch 205 loss: 160.35389709472656\n",
      "  batch 210 loss: 171.08920593261718\n",
      "  batch 215 loss: 163.7824249267578\n",
      "  batch 220 loss: 178.07793273925782\n",
      "  batch 225 loss: 163.19944458007814\n",
      "  batch 230 loss: 179.52247314453126\n",
      "  batch 235 loss: 166.94424743652343\n",
      "  batch 240 loss: 158.56499633789062\n",
      "  batch 245 loss: 162.4389434814453\n",
      "  batch 250 loss: 176.57667236328126\n",
      "  batch 255 loss: 156.5907958984375\n",
      "  batch 260 loss: 157.43082580566406\n",
      "  batch 265 loss: 167.05136108398438\n",
      "  batch 270 loss: 171.28089294433593\n",
      "  batch 275 loss: 174.3699737548828\n",
      "  batch 280 loss: 168.9193908691406\n",
      "  batch 285 loss: 184.58340759277343\n",
      "  batch 290 loss: 176.99413146972657\n",
      "  batch 295 loss: 179.81507568359376\n",
      "  batch 300 loss: 163.63738708496095\n",
      "  batch 305 loss: 172.89716491699218\n",
      "  batch 310 loss: 174.42497863769532\n",
      "  batch 315 loss: 166.52001953125\n",
      "  batch 320 loss: 186.40875854492188\n",
      "  batch 325 loss: 172.94165954589843\n",
      "  batch 330 loss: 157.0481170654297\n",
      "  batch 335 loss: 176.81094055175782\n",
      "  batch 340 loss: 164.30060729980468\n",
      "  batch 345 loss: 173.2327087402344\n",
      "  batch 350 loss: 174.74423522949218\n",
      "  batch 355 loss: 175.72707824707032\n",
      "  batch 360 loss: 174.15396118164062\n",
      "  batch 365 loss: 173.75726928710938\n",
      "  batch 370 loss: 172.69388732910156\n",
      "  batch 375 loss: 169.56997680664062\n",
      "  batch 380 loss: 175.82893981933594\n",
      "  batch 385 loss: 176.09637451171875\n",
      "  batch 390 loss: 177.46316833496093\n",
      "  batch 395 loss: 189.22698364257812\n",
      "  batch 400 loss: 170.63921203613282\n",
      "  batch 405 loss: 178.42079162597656\n",
      "  batch 410 loss: 180.17752990722656\n",
      "  batch 415 loss: 176.8\n",
      "  batch 420 loss: 182.30238037109376\n",
      "  batch 425 loss: 177.59598083496093\n",
      "  batch 430 loss: 188.7743896484375\n",
      "  batch 435 loss: 177.03797912597656\n",
      "  batch 440 loss: 172.75259094238282\n",
      "  batch 445 loss: 194.0361328125\n",
      "  batch 450 loss: 185.00870361328126\n",
      "  batch 455 loss: 182.9897247314453\n",
      "  batch 460 loss: 173.01442260742186\n",
      "  batch 465 loss: 192.20388488769532\n",
      "  batch 470 loss: 184.03441772460937\n",
      "  batch 475 loss: 176.5568603515625\n",
      "  batch 480 loss: 183.94912414550782\n",
      "  batch 485 loss: 171.59430541992188\n",
      "  batch 490 loss: 178.9770263671875\n",
      "  batch 495 loss: 185.75729064941407\n",
      "  batch 500 loss: 190.44183349609375\n",
      "  batch 505 loss: 180.69634704589845\n",
      "  batch 510 loss: 173.2849090576172\n",
      "  batch 515 loss: 185.6725311279297\n",
      "Epoch: 4 is done\n",
      "  batch 5 loss: 126.81429748535156\n",
      "  batch 10 loss: 123.93165130615235\n",
      "  batch 15 loss: 125.64193572998047\n",
      "  batch 20 loss: 137.78289947509765\n",
      "  batch 25 loss: 133.39364624023438\n",
      "  batch 30 loss: 130.80857696533204\n",
      "  batch 35 loss: 143.73161926269532\n",
      "  batch 40 loss: 138.96434326171874\n",
      "  batch 45 loss: 129.50729370117188\n",
      "  batch 50 loss: 118.76441192626953\n",
      "  batch 55 loss: 126.45757141113282\n",
      "  batch 60 loss: 146.43240509033203\n",
      "  batch 65 loss: 126.62748565673829\n",
      "  batch 70 loss: 139.5212860107422\n",
      "  batch 75 loss: 130.71624908447265\n",
      "  batch 80 loss: 138.8153305053711\n",
      "  batch 85 loss: 144.64549407958984\n",
      "  batch 90 loss: 141.47683715820312\n",
      "  batch 95 loss: 146.9515869140625\n",
      "  batch 100 loss: 134.84007720947267\n",
      "  batch 105 loss: 141.81968994140624\n",
      "  batch 110 loss: 123.2432357788086\n",
      "  batch 115 loss: 129.42605590820312\n",
      "  batch 120 loss: 128.92754974365235\n",
      "  batch 125 loss: 138.20771179199218\n",
      "  batch 130 loss: 134.42675323486327\n",
      "  batch 135 loss: 131.62017669677735\n",
      "  batch 140 loss: 144.3865173339844\n",
      "  batch 145 loss: 144.08294372558595\n",
      "  batch 150 loss: 149.53013610839844\n",
      "  batch 155 loss: 148.23682556152343\n",
      "  batch 160 loss: 147.96102905273438\n",
      "  batch 165 loss: 135.1792449951172\n",
      "  batch 170 loss: 157.7923156738281\n",
      "  batch 175 loss: 141.94937591552736\n",
      "  batch 180 loss: 142.52069702148438\n",
      "  batch 185 loss: 148.60616760253907\n",
      "  batch 190 loss: 131.5853469848633\n",
      "  batch 195 loss: 139.18866119384765\n",
      "  batch 200 loss: 137.66099548339844\n",
      "  batch 205 loss: 141.01431274414062\n",
      "  batch 210 loss: 137.43397674560546\n",
      "  batch 215 loss: 151.89916381835937\n",
      "  batch 220 loss: 159.59102783203124\n",
      "  batch 225 loss: 2995.8794006347657\n",
      "  batch 230 loss: 751.6562805175781\n",
      "  batch 235 loss: 1319.1806396484376\n",
      "  batch 240 loss: 1270.4807373046874\n",
      "  batch 245 loss: 1274.6330322265626\n",
      "  batch 250 loss: 1486.1702880859375\n",
      "  batch 255 loss: 1165.6057006835938\n",
      "  batch 260 loss: 1001.7372314453125\n",
      "  batch 265 loss: 888.5092529296875\n",
      "  batch 270 loss: 702.1950317382813\n",
      "  batch 275 loss: 806.9754760742187\n",
      "  batch 280 loss: 696.4483032226562\n",
      "  batch 285 loss: 679.645166015625\n",
      "  batch 290 loss: 670.9939453125\n",
      "  batch 295 loss: 913.6722412109375\n",
      "  batch 300 loss: 871.6260498046875\n",
      "  batch 305 loss: 766.496142578125\n",
      "  batch 310 loss: 541.0292114257812\n",
      "  batch 315 loss: 533.3836791992187\n",
      "  batch 320 loss: 540.5141723632812\n",
      "  batch 325 loss: 510.5726745605469\n",
      "  batch 330 loss: 475.1417724609375\n",
      "  batch 335 loss: 475.24529418945315\n",
      "  batch 340 loss: 441.9404602050781\n",
      "  batch 345 loss: 450.3734375\n",
      "  batch 350 loss: 424.094140625\n",
      "  batch 355 loss: 405.00470581054685\n",
      "  batch 360 loss: 394.4030334472656\n",
      "  batch 365 loss: 350.395703125\n",
      "  batch 370 loss: 342.07456665039064\n",
      "  batch 375 loss: 345.08558959960936\n",
      "  batch 380 loss: 361.2077209472656\n",
      "  batch 385 loss: 343.51038818359376\n",
      "  batch 390 loss: 356.3742431640625\n",
      "  batch 395 loss: 299.98809204101565\n",
      "  batch 400 loss: 312.901904296875\n",
      "  batch 405 loss: 299.2474304199219\n",
      "  batch 410 loss: 323.59703369140624\n",
      "  batch 415 loss: 310.34483642578124\n",
      "  batch 420 loss: 296.26597900390624\n",
      "  batch 425 loss: 279.0283538818359\n",
      "  batch 430 loss: 294.5035827636719\n",
      "  batch 435 loss: 290.68193359375\n",
      "  batch 440 loss: 278.83201904296874\n",
      "  batch 445 loss: 276.8538757324219\n",
      "  batch 450 loss: 256.7621826171875\n",
      "  batch 455 loss: 337.9090637207031\n",
      "  batch 460 loss: 297.2763702392578\n",
      "  batch 465 loss: 260.9592651367187\n",
      "  batch 470 loss: 273.276953125\n",
      "  batch 475 loss: 340.32566528320314\n",
      "  batch 480 loss: 313.54660949707034\n",
      "  batch 485 loss: 302.40936889648435\n",
      "  batch 490 loss: 285.72731018066406\n",
      "  batch 495 loss: 258.29137268066404\n",
      "  batch 500 loss: 270.3914764404297\n",
      "  batch 505 loss: 264.60438537597656\n",
      "  batch 510 loss: 257.558837890625\n",
      "  batch 515 loss: 243.26223449707032\n",
      "Epoch: 5 is done\n",
      "  batch 5 loss: 189.20511474609376\n",
      "  batch 10 loss: 200.52631225585938\n",
      "  batch 15 loss: 197.2380798339844\n",
      "  batch 20 loss: 181.8206024169922\n",
      "  batch 25 loss: 196.53472595214845\n",
      "  batch 30 loss: 188.43887329101562\n",
      "  batch 35 loss: 200.59510192871093\n",
      "  batch 40 loss: 181.4247283935547\n",
      "  batch 45 loss: 178.47899780273437\n",
      "  batch 50 loss: 173.92697143554688\n",
      "  batch 55 loss: 155.6310836791992\n",
      "  batch 60 loss: 181.5167205810547\n",
      "  batch 65 loss: 198.23765258789064\n",
      "  batch 70 loss: 188.47694091796876\n",
      "  batch 75 loss: 174.51566467285156\n",
      "  batch 80 loss: 172.50852966308594\n",
      "  batch 85 loss: 160.78077392578126\n",
      "  batch 90 loss: 181.1379150390625\n",
      "  batch 95 loss: 194.23910522460938\n",
      "  batch 100 loss: 192.74774780273438\n",
      "  batch 105 loss: 175.92618713378906\n",
      "  batch 110 loss: 170.72130432128907\n",
      "  batch 115 loss: 172.66678466796876\n",
      "  batch 120 loss: 184.7156524658203\n",
      "  batch 125 loss: 191.99180297851564\n",
      "  batch 130 loss: 197.23912353515624\n",
      "  batch 135 loss: 162.22610473632812\n",
      "  batch 140 loss: 179.3105926513672\n",
      "  batch 145 loss: 180.9414489746094\n",
      "  batch 150 loss: 183.07393798828124\n",
      "  batch 155 loss: 170.55416259765624\n",
      "  batch 160 loss: 184.73302612304687\n",
      "  batch 165 loss: 188.32659912109375\n",
      "  batch 170 loss: 175.28198852539063\n",
      "  batch 175 loss: 178.8049560546875\n",
      "  batch 180 loss: 175.24832153320312\n",
      "  batch 185 loss: 166.91970825195312\n",
      "  batch 190 loss: 188.6302917480469\n",
      "  batch 195 loss: 176.46036682128906\n",
      "  batch 200 loss: 175.2723876953125\n",
      "  batch 205 loss: 174.8666534423828\n",
      "  batch 210 loss: 174.31451416015625\n",
      "  batch 215 loss: 175.99254150390624\n",
      "  batch 220 loss: 167.39758911132813\n",
      "  batch 225 loss: 171.92197875976564\n",
      "  batch 230 loss: 177.59677124023438\n",
      "  batch 235 loss: 175.28436889648438\n",
      "  batch 240 loss: 174.06680297851562\n",
      "  batch 245 loss: 191.80787048339843\n",
      "  batch 250 loss: 163.87775573730468\n",
      "  batch 255 loss: 193.81975708007812\n",
      "  batch 260 loss: 181.62158813476563\n",
      "  batch 265 loss: 172.24539184570312\n",
      "  batch 270 loss: 176.70366516113282\n",
      "  batch 275 loss: 187.80079345703126\n",
      "  batch 280 loss: 170.13502197265626\n",
      "  batch 285 loss: 178.9163787841797\n",
      "  batch 290 loss: 169.13516235351562\n",
      "  batch 295 loss: 158.5797882080078\n",
      "  batch 300 loss: 160.19607238769532\n",
      "  batch 305 loss: 172.81836853027343\n",
      "  batch 310 loss: 175.32022399902343\n",
      "  batch 315 loss: 171.29603576660156\n",
      "  batch 320 loss: 170.67164306640626\n",
      "  batch 325 loss: 165.11553955078125\n",
      "  batch 330 loss: 185.1419891357422\n",
      "  batch 335 loss: 165.41351318359375\n",
      "  batch 340 loss: 172.53989868164064\n",
      "  batch 345 loss: 174.00692443847657\n",
      "  batch 350 loss: 156.98501892089843\n",
      "  batch 355 loss: 169.49935302734374\n",
      "  batch 360 loss: 159.32114868164064\n",
      "  batch 365 loss: 157.89824371337892\n",
      "  batch 370 loss: 158.48001708984376\n",
      "  batch 375 loss: 169.12619323730468\n",
      "  batch 380 loss: 188.53174438476563\n",
      "  batch 385 loss: 157.10399780273437\n",
      "  batch 390 loss: 163.07217407226562\n",
      "  batch 395 loss: 175.1904327392578\n",
      "  batch 400 loss: 166.27153625488282\n",
      "  batch 405 loss: 170.01231994628907\n",
      "  batch 410 loss: 165.60189208984374\n",
      "  batch 415 loss: 165.21832580566405\n",
      "  batch 420 loss: 158.36009521484374\n",
      "  batch 425 loss: 178.5742980957031\n",
      "  batch 430 loss: 163.76510009765624\n",
      "  batch 435 loss: 166.62698059082032\n",
      "  batch 440 loss: 164.21978149414062\n",
      "  batch 445 loss: 177.7217529296875\n",
      "  batch 450 loss: 157.62742919921874\n",
      "  batch 455 loss: 158.58936157226563\n",
      "  batch 460 loss: 169.93941345214844\n",
      "  batch 465 loss: 169.03896484375\n",
      "  batch 470 loss: 168.56792602539062\n",
      "  batch 475 loss: 169.23623352050782\n",
      "  batch 480 loss: 174.02078247070312\n",
      "  batch 485 loss: 166.54953308105468\n",
      "  batch 490 loss: 168.16940307617188\n",
      "  batch 495 loss: 168.33353271484376\n",
      "  batch 500 loss: 176.468115234375\n",
      "  batch 505 loss: 158.2095489501953\n",
      "  batch 510 loss: 169.61275939941407\n",
      "  batch 515 loss: 168.45990905761718\n",
      "Epoch: 6 is done\n",
      "  batch 5 loss: 122.55135192871094\n",
      "  batch 10 loss: 130.01539154052733\n",
      "  batch 15 loss: 127.94547576904297\n",
      "  batch 20 loss: 116.65843963623047\n",
      "  batch 25 loss: 119.83312377929687\n",
      "  batch 30 loss: 114.97532958984375\n",
      "  batch 35 loss: 120.31081848144531\n",
      "  batch 40 loss: 113.1377944946289\n",
      "  batch 45 loss: 119.61755676269532\n",
      "  batch 50 loss: 126.49460296630859\n",
      "  batch 55 loss: 121.7190185546875\n",
      "  batch 60 loss: 133.72277374267577\n",
      "  batch 65 loss: 129.2934356689453\n",
      "  batch 70 loss: 114.88975219726562\n",
      "  batch 75 loss: 126.12249450683593\n",
      "  batch 80 loss: 120.27585906982422\n",
      "  batch 85 loss: 126.99757690429688\n",
      "  batch 90 loss: 122.11336364746094\n",
      "  batch 95 loss: 135.71218719482422\n",
      "  batch 100 loss: 118.25196685791016\n",
      "  batch 105 loss: 113.3057861328125\n",
      "  batch 110 loss: 126.41715698242187\n",
      "  batch 115 loss: 120.56481170654297\n",
      "  batch 120 loss: 122.9400421142578\n",
      "  batch 125 loss: 114.95599822998047\n",
      "  batch 130 loss: 129.50648651123046\n",
      "  batch 135 loss: 124.53852081298828\n",
      "  batch 140 loss: 139.77578735351562\n",
      "  batch 145 loss: 128.4363006591797\n",
      "  batch 150 loss: 107.89131469726563\n",
      "  batch 155 loss: 120.07897491455078\n",
      "  batch 160 loss: 121.2292694091797\n",
      "  batch 165 loss: 120.87088928222656\n",
      "  batch 170 loss: 139.61075134277343\n",
      "  batch 175 loss: 131.0150115966797\n",
      "  batch 180 loss: 127.60690460205078\n",
      "  batch 185 loss: 111.94417419433594\n",
      "  batch 190 loss: 110.30927886962891\n",
      "  batch 195 loss: 148.40711669921876\n",
      "  batch 200 loss: 133.98453063964843\n",
      "  batch 205 loss: 122.31524505615235\n",
      "  batch 210 loss: 130.22283477783202\n",
      "  batch 215 loss: 132.94986877441406\n",
      "  batch 220 loss: 139.3417495727539\n",
      "  batch 225 loss: 128.409326171875\n",
      "  batch 230 loss: 130.2777328491211\n",
      "  batch 235 loss: 126.0585205078125\n",
      "  batch 240 loss: 119.52296447753906\n",
      "  batch 245 loss: 122.44300842285156\n",
      "  batch 250 loss: 118.26737518310547\n",
      "  batch 255 loss: 118.86808776855469\n",
      "  batch 260 loss: 116.8091033935547\n",
      "  batch 265 loss: 129.36726837158204\n",
      "  batch 270 loss: 134.82454833984374\n",
      "  batch 275 loss: 125.59639587402344\n",
      "  batch 280 loss: 132.7280746459961\n",
      "  batch 285 loss: 129.9644577026367\n",
      "  batch 290 loss: 124.94824981689453\n",
      "  batch 295 loss: 131.71311798095704\n",
      "  batch 300 loss: 128.65646514892578\n",
      "  batch 305 loss: 145.5440887451172\n",
      "  batch 310 loss: 122.81636352539063\n",
      "  batch 315 loss: 132.7082305908203\n",
      "  batch 320 loss: 131.69723510742188\n",
      "  batch 325 loss: 134.5020980834961\n",
      "  batch 330 loss: 117.43807067871094\n",
      "  batch 335 loss: 132.6226791381836\n",
      "  batch 340 loss: 126.02671508789062\n",
      "  batch 345 loss: 117.18289794921876\n",
      "  batch 350 loss: 141.6389389038086\n",
      "  batch 355 loss: 123.15264282226562\n",
      "  batch 360 loss: 123.98136444091797\n",
      "  batch 365 loss: 121.87052001953126\n",
      "  batch 370 loss: 128.4291519165039\n",
      "  batch 375 loss: 120.889013671875\n",
      "  batch 380 loss: 132.91957397460936\n",
      "  batch 385 loss: 136.4414840698242\n",
      "  batch 390 loss: 137.9325698852539\n",
      "  batch 395 loss: 132.50960388183594\n",
      "  batch 400 loss: 135.26865844726564\n",
      "  batch 405 loss: 131.13156280517578\n",
      "  batch 410 loss: 129.2993179321289\n",
      "  batch 415 loss: 121.80912170410156\n",
      "  batch 420 loss: 128.75379180908203\n",
      "  batch 425 loss: 133.6153762817383\n",
      "  batch 430 loss: 126.33612823486328\n",
      "  batch 435 loss: 150.1100280761719\n",
      "  batch 440 loss: 131.35721282958986\n",
      "  batch 445 loss: 133.0622772216797\n",
      "  batch 450 loss: 123.21574554443359\n",
      "  batch 455 loss: 144.421484375\n",
      "  batch 460 loss: 125.27919158935546\n",
      "  batch 465 loss: 136.26752471923828\n",
      "  batch 470 loss: 134.26356506347656\n",
      "  batch 475 loss: 126.82251892089843\n",
      "  batch 480 loss: 141.70077514648438\n",
      "  batch 485 loss: 134.6427429199219\n",
      "  batch 490 loss: 136.61551513671876\n",
      "  batch 495 loss: 134.05915374755858\n",
      "  batch 500 loss: 133.33533325195313\n",
      "  batch 505 loss: 128.73137817382812\n",
      "  batch 510 loss: 139.37096405029297\n",
      "  batch 515 loss: 144.30376434326172\n",
      "Epoch: 7 is done\n",
      "  batch 5 loss: 100.81223602294922\n",
      "  batch 10 loss: 100.28996276855469\n",
      "  batch 15 loss: 120.60076599121093\n",
      "  batch 20 loss: 101.07259826660156\n",
      "  batch 25 loss: 111.26059112548828\n",
      "  batch 30 loss: 107.40275726318359\n",
      "  batch 35 loss: 101.64970550537109\n",
      "  batch 40 loss: 100.9795654296875\n",
      "  batch 45 loss: 113.05439758300781\n",
      "  batch 50 loss: 105.8875228881836\n",
      "  batch 55 loss: 94.94988250732422\n",
      "  batch 60 loss: 103.50260314941406\n",
      "  batch 65 loss: 93.90461730957031\n",
      "  batch 70 loss: 98.7024917602539\n",
      "  batch 75 loss: 128.7054672241211\n",
      "  batch 80 loss: 115.60558624267578\n",
      "  batch 85 loss: 108.56138610839844\n",
      "  batch 90 loss: 105.6053680419922\n",
      "  batch 95 loss: 110.00730743408204\n",
      "  batch 100 loss: 104.60111236572266\n",
      "  batch 105 loss: 111.91485137939453\n",
      "  batch 110 loss: 115.03894958496093\n",
      "  batch 115 loss: 109.3343734741211\n",
      "  batch 120 loss: 108.40630493164062\n",
      "  batch 125 loss: 106.18780822753907\n",
      "  batch 130 loss: 118.43202362060546\n",
      "  batch 135 loss: 104.0122802734375\n",
      "  batch 140 loss: 105.17092590332031\n",
      "  batch 145 loss: 116.35429077148437\n",
      "  batch 150 loss: 105.10282135009766\n",
      "  batch 155 loss: 108.00170135498047\n",
      "  batch 160 loss: 98.24654235839844\n",
      "  batch 165 loss: 115.0467529296875\n",
      "  batch 170 loss: 113.2938949584961\n",
      "  batch 175 loss: 102.56625671386719\n",
      "  batch 180 loss: 97.7948013305664\n",
      "  batch 185 loss: 109.57342987060547\n",
      "  batch 190 loss: 106.41234283447265\n",
      "  batch 195 loss: 111.1453353881836\n",
      "  batch 200 loss: 113.77597198486328\n",
      "  batch 205 loss: 118.24568634033203\n",
      "  batch 210 loss: 112.47695465087891\n",
      "  batch 215 loss: 110.43048858642578\n",
      "  batch 220 loss: 114.12981262207032\n",
      "  batch 225 loss: 111.04188537597656\n",
      "  batch 230 loss: 103.07669677734376\n",
      "  batch 235 loss: 103.14490661621093\n",
      "  batch 240 loss: 106.416943359375\n",
      "  batch 245 loss: 107.76337280273438\n",
      "  batch 250 loss: 113.06982727050782\n",
      "  batch 255 loss: 116.10352935791016\n",
      "  batch 260 loss: 116.06411895751953\n",
      "  batch 265 loss: 116.51169891357422\n",
      "  batch 270 loss: 115.0768829345703\n",
      "  batch 275 loss: 108.84894256591797\n",
      "  batch 280 loss: 104.13685760498046\n",
      "  batch 285 loss: 103.48876953125\n",
      "  batch 290 loss: 111.33745880126953\n",
      "  batch 295 loss: 109.02119140625\n",
      "  batch 300 loss: 103.81757507324218\n",
      "  batch 305 loss: 115.41333618164063\n",
      "  batch 310 loss: 112.30392150878906\n",
      "  batch 315 loss: 105.42877960205078\n",
      "  batch 320 loss: 129.16998443603515\n",
      "  batch 325 loss: 108.24189453125\n",
      "  batch 330 loss: 119.71683349609376\n",
      "  batch 335 loss: 109.39466400146485\n",
      "  batch 340 loss: 110.06967620849609\n",
      "  batch 345 loss: 117.96806945800782\n",
      "  batch 350 loss: 118.49742279052734\n",
      "  batch 355 loss: 115.60274810791016\n",
      "  batch 360 loss: 113.70335845947265\n",
      "  batch 365 loss: 99.16699523925782\n",
      "  batch 370 loss: 110.05830993652344\n",
      "  batch 375 loss: 114.2373291015625\n",
      "  batch 380 loss: 111.87297058105469\n",
      "  batch 385 loss: 111.39887390136718\n",
      "  batch 390 loss: 122.12745513916016\n",
      "  batch 395 loss: 106.76669006347656\n",
      "  batch 400 loss: 115.05490570068359\n",
      "  batch 405 loss: 107.5444122314453\n",
      "  batch 410 loss: 127.5120132446289\n",
      "  batch 415 loss: 122.79713592529296\n",
      "  batch 420 loss: 108.86491241455079\n",
      "  batch 425 loss: 125.18159637451171\n",
      "  batch 430 loss: 113.05657806396485\n",
      "  batch 435 loss: 118.79669494628907\n",
      "  batch 440 loss: 121.34794006347656\n",
      "  batch 445 loss: 109.79202728271484\n",
      "  batch 450 loss: 120.76577758789062\n",
      "  batch 455 loss: 115.20682830810547\n",
      "  batch 460 loss: 110.1673828125\n",
      "  batch 465 loss: 114.94290924072266\n",
      "  batch 470 loss: 124.92148895263672\n",
      "  batch 475 loss: 106.74617004394531\n",
      "  batch 480 loss: 128.61923370361328\n",
      "  batch 485 loss: 117.96110382080079\n",
      "  batch 490 loss: 116.04256286621094\n",
      "  batch 495 loss: 124.56693725585937\n",
      "  batch 500 loss: 102.39500122070312\n",
      "  batch 505 loss: 113.53262176513672\n",
      "  batch 510 loss: 108.04144287109375\n",
      "  batch 515 loss: 117.60978698730469\n",
      "Epoch: 8 is done\n",
      "  batch 5 loss: 90.96068572998047\n",
      "  batch 10 loss: 90.82565002441406\n",
      "  batch 15 loss: 96.31826782226562\n",
      "  batch 20 loss: 91.53763885498047\n",
      "  batch 25 loss: 85.51341705322265\n",
      "  batch 30 loss: 99.3911849975586\n",
      "  batch 35 loss: 93.8549301147461\n",
      "  batch 40 loss: 90.69403381347657\n",
      "  batch 45 loss: 93.85663757324218\n",
      "  batch 50 loss: 93.48478698730469\n",
      "  batch 55 loss: 104.1605712890625\n",
      "  batch 60 loss: 99.73524627685546\n",
      "  batch 65 loss: 96.32499847412109\n",
      "  batch 70 loss: 88.42994842529296\n",
      "  batch 75 loss: 92.82943572998047\n",
      "  batch 80 loss: 112.01875152587891\n",
      "  batch 85 loss: 91.89363555908203\n",
      "  batch 90 loss: 95.29877166748047\n",
      "  batch 95 loss: 92.1279525756836\n",
      "  batch 100 loss: 101.24535369873047\n",
      "  batch 105 loss: 98.87287750244141\n",
      "  batch 110 loss: 100.17649993896484\n",
      "  batch 115 loss: 102.89147491455078\n",
      "  batch 120 loss: 104.96248626708984\n",
      "  batch 125 loss: 101.70302886962891\n",
      "  batch 130 loss: 107.93615264892578\n",
      "  batch 135 loss: 97.06721649169921\n",
      "  batch 140 loss: 100.32957916259765\n",
      "  batch 145 loss: 96.00510101318359\n",
      "  batch 150 loss: 94.71788177490234\n",
      "  batch 155 loss: 101.00604400634765\n",
      "  batch 160 loss: 104.6364959716797\n",
      "  batch 165 loss: 100.89860229492187\n",
      "  batch 170 loss: 104.95053558349609\n",
      "  batch 175 loss: 86.96002807617188\n",
      "  batch 180 loss: 97.75910186767578\n",
      "  batch 185 loss: 99.92429809570312\n",
      "  batch 190 loss: 103.58481903076172\n",
      "  batch 195 loss: 95.96141510009765\n",
      "  batch 200 loss: 98.1601058959961\n",
      "  batch 205 loss: 111.73387908935547\n",
      "  batch 210 loss: 111.15613555908203\n",
      "  batch 215 loss: 104.91714630126953\n",
      "  batch 220 loss: 99.35791625976563\n",
      "  batch 225 loss: 104.784619140625\n",
      "  batch 230 loss: 91.66446533203126\n",
      "  batch 235 loss: 104.73999938964843\n",
      "  batch 240 loss: 96.39834594726562\n",
      "  batch 245 loss: 101.93897094726563\n",
      "  batch 250 loss: 88.25259399414062\n",
      "  batch 255 loss: 111.03113555908203\n",
      "  batch 260 loss: 105.87055969238281\n",
      "  batch 265 loss: 99.16498718261718\n",
      "  batch 270 loss: 105.0100082397461\n",
      "  batch 275 loss: 98.32093505859375\n",
      "  batch 280 loss: 104.5035171508789\n",
      "  batch 285 loss: 108.41326599121093\n",
      "  batch 290 loss: 100.23173217773437\n",
      "  batch 295 loss: 117.78773803710938\n",
      "  batch 300 loss: 106.89910736083985\n",
      "  batch 305 loss: 109.49594116210938\n",
      "  batch 310 loss: 117.34593200683594\n",
      "  batch 315 loss: 98.13876342773438\n",
      "  batch 320 loss: 108.75665740966797\n",
      "  batch 325 loss: 114.40720825195312\n",
      "  batch 330 loss: 100.69836120605468\n",
      "  batch 335 loss: 105.86355590820312\n",
      "  batch 340 loss: 102.08987579345703\n",
      "  batch 345 loss: 103.85746917724609\n",
      "  batch 350 loss: 118.11441192626953\n",
      "  batch 355 loss: 104.04744873046874\n",
      "  batch 360 loss: 120.07221221923828\n",
      "  batch 365 loss: 115.43652648925782\n",
      "  batch 370 loss: 108.1450210571289\n",
      "  batch 375 loss: 107.27682037353516\n",
      "  batch 380 loss: 103.97607879638672\n",
      "  batch 385 loss: 106.01808624267578\n",
      "  batch 390 loss: 106.27621917724609\n",
      "  batch 395 loss: 99.81565704345704\n",
      "  batch 400 loss: 101.77881927490235\n",
      "  batch 405 loss: 97.25023651123047\n",
      "  batch 410 loss: 110.37361450195313\n",
      "  batch 415 loss: 109.20426940917969\n",
      "  batch 420 loss: 109.48697509765626\n",
      "  batch 425 loss: 104.64238739013672\n",
      "  batch 430 loss: 105.31194458007812\n",
      "  batch 435 loss: 109.9967041015625\n",
      "  batch 440 loss: 114.56546936035156\n",
      "  batch 445 loss: 113.22552185058593\n",
      "  batch 450 loss: 108.47151489257813\n",
      "  batch 455 loss: 100.37006988525391\n",
      "  batch 460 loss: 117.14075775146485\n",
      "  batch 465 loss: 106.5590316772461\n",
      "  batch 470 loss: 106.42438659667968\n",
      "  batch 475 loss: 110.8828109741211\n",
      "  batch 480 loss: 112.97000427246094\n",
      "  batch 485 loss: 110.55198669433594\n",
      "  batch 490 loss: 114.95382385253906\n",
      "  batch 495 loss: 109.62300415039063\n",
      "  batch 500 loss: 118.5073974609375\n",
      "  batch 505 loss: 105.99931945800782\n",
      "  batch 510 loss: 119.35257873535156\n",
      "  batch 515 loss: 115.21057739257813\n",
      "Epoch: 9 is done\n",
      "  batch 5 loss: 94.11147918701172\n",
      "  batch 10 loss: 94.47653656005859\n",
      "  batch 15 loss: 94.83681640625\n",
      "  batch 20 loss: 91.77914886474609\n",
      "  batch 25 loss: 93.37068328857421\n",
      "  batch 30 loss: 92.29297637939453\n",
      "  batch 35 loss: 92.24457397460938\n",
      "  batch 40 loss: 94.17839813232422\n",
      "  batch 45 loss: 90.6684341430664\n",
      "  batch 50 loss: 99.13292541503907\n",
      "  batch 55 loss: 91.47618255615234\n",
      "  batch 60 loss: 86.03785552978516\n",
      "  batch 65 loss: 94.9389862060547\n",
      "  batch 70 loss: 92.22835083007813\n",
      "  batch 75 loss: 85.33150024414063\n",
      "  batch 80 loss: 99.04302062988282\n",
      "  batch 85 loss: 90.98394622802735\n",
      "  batch 90 loss: 98.85364379882813\n",
      "  batch 95 loss: 84.91514587402344\n",
      "  batch 100 loss: 87.20658416748047\n",
      "  batch 105 loss: 92.64793701171875\n",
      "  batch 110 loss: 84.24159698486328\n",
      "  batch 115 loss: 90.46496887207032\n",
      "  batch 120 loss: 87.47545318603515\n",
      "  batch 125 loss: 95.62406158447266\n",
      "  batch 130 loss: 85.99496459960938\n",
      "  batch 135 loss: 91.71343994140625\n",
      "  batch 140 loss: 104.31860046386718\n",
      "  batch 145 loss: 96.1762451171875\n",
      "  batch 150 loss: 86.5641242980957\n",
      "  batch 155 loss: 93.16061553955078\n",
      "  batch 160 loss: 98.71792144775391\n",
      "  batch 165 loss: 99.77271423339843\n",
      "  batch 170 loss: 91.09902496337891\n",
      "  batch 175 loss: 85.45476760864258\n",
      "  batch 180 loss: 96.90926513671874\n",
      "  batch 185 loss: 101.29549865722656\n",
      "  batch 190 loss: 94.84209899902343\n",
      "  batch 195 loss: 94.38991088867188\n",
      "  batch 200 loss: 90.84460754394532\n",
      "  batch 205 loss: 100.00669403076172\n",
      "  batch 210 loss: 87.97373962402344\n",
      "  batch 215 loss: 90.59899597167968\n",
      "  batch 220 loss: 104.20116119384765\n",
      "  batch 225 loss: 88.50212249755859\n",
      "  batch 230 loss: 93.35923767089844\n",
      "  batch 235 loss: 107.00137481689453\n",
      "  batch 240 loss: 100.43009185791016\n",
      "  batch 245 loss: 87.74560089111328\n",
      "  batch 250 loss: 99.78642272949219\n",
      "  batch 255 loss: 102.11226654052734\n",
      "  batch 260 loss: 86.24217834472657\n",
      "  batch 265 loss: 99.23977661132812\n",
      "  batch 270 loss: 100.1283676147461\n",
      "  batch 275 loss: 94.16677398681641\n",
      "  batch 280 loss: 106.77723541259766\n",
      "  batch 285 loss: 106.27427368164062\n",
      "  batch 290 loss: 98.03150787353516\n",
      "  batch 295 loss: 97.67542419433593\n",
      "  batch 300 loss: 100.07273559570312\n",
      "  batch 305 loss: 102.5695816040039\n",
      "  batch 310 loss: 107.7411102294922\n",
      "  batch 315 loss: 89.8653335571289\n",
      "  batch 320 loss: 98.76091156005859\n",
      "  batch 325 loss: 95.68389282226562\n",
      "  batch 330 loss: 95.10047836303711\n",
      "  batch 335 loss: 108.11500854492188\n",
      "  batch 340 loss: 98.64192047119141\n",
      "  batch 345 loss: 102.5000717163086\n",
      "  batch 350 loss: 98.64467315673828\n",
      "  batch 355 loss: 94.32476806640625\n",
      "  batch 360 loss: 101.00506744384765\n",
      "  batch 365 loss: 96.50419464111329\n",
      "  batch 370 loss: 93.95298461914062\n",
      "  batch 375 loss: 99.50097351074218\n",
      "  batch 380 loss: 111.8401382446289\n",
      "  batch 385 loss: 109.27858276367188\n",
      "  batch 390 loss: 89.94498596191406\n",
      "  batch 395 loss: 116.7548110961914\n",
      "  batch 400 loss: 99.17265167236329\n",
      "  batch 405 loss: 103.96163940429688\n",
      "  batch 410 loss: 101.89122924804687\n",
      "  batch 415 loss: 106.75846862792969\n",
      "  batch 420 loss: 105.20890502929687\n",
      "  batch 425 loss: 105.39758758544922\n",
      "  batch 430 loss: 115.75684204101563\n",
      "  batch 435 loss: 144.39507141113282\n",
      "  batch 440 loss: 102.94738464355468\n",
      "  batch 445 loss: 104.85366668701172\n",
      "  batch 450 loss: 105.66880798339844\n",
      "  batch 455 loss: 117.21574401855469\n",
      "  batch 460 loss: 100.38216857910156\n",
      "  batch 465 loss: 127.99613952636719\n",
      "  batch 470 loss: 113.88815307617188\n",
      "  batch 475 loss: 115.32032165527343\n",
      "  batch 480 loss: 120.52440185546875\n",
      "  batch 485 loss: 110.58163604736328\n",
      "  batch 490 loss: 107.41308135986328\n",
      "  batch 495 loss: 110.89477844238282\n",
      "  batch 500 loss: 103.51023712158204\n",
      "  batch 505 loss: 108.84681701660156\n",
      "  batch 510 loss: 109.87847137451172\n",
      "  batch 515 loss: 106.46267395019531\n",
      "Epoch: 10 is done\n",
      "  batch 5 loss: 93.74610290527343\n",
      "  batch 10 loss: 89.86165771484374\n",
      "  batch 15 loss: 85.13717803955078\n",
      "  batch 20 loss: 91.5220718383789\n",
      "  batch 25 loss: 86.89429168701172\n",
      "  batch 30 loss: 85.52643127441407\n",
      "  batch 35 loss: 79.7078872680664\n",
      "  batch 40 loss: 84.9621078491211\n",
      "  batch 45 loss: 89.10750732421874\n",
      "  batch 50 loss: 84.21749572753906\n",
      "  batch 55 loss: 96.38158569335937\n",
      "  batch 60 loss: 91.78125\n",
      "  batch 65 loss: 94.0618896484375\n",
      "  batch 70 loss: 82.95702209472657\n",
      "  batch 75 loss: 89.40592346191406\n",
      "  batch 80 loss: 87.56577606201172\n",
      "  batch 85 loss: 88.51740112304688\n",
      "  batch 90 loss: 78.81559753417969\n",
      "  batch 95 loss: 88.16342010498047\n",
      "  batch 100 loss: 88.68630981445312\n",
      "  batch 105 loss: 91.79266510009765\n",
      "  batch 110 loss: 93.90380249023437\n",
      "  batch 115 loss: 91.38340759277344\n",
      "  batch 120 loss: 94.2900604248047\n",
      "  batch 125 loss: 94.51880340576172\n",
      "  batch 130 loss: 97.36850280761719\n",
      "  batch 135 loss: 99.6692886352539\n",
      "  batch 140 loss: 97.41624450683594\n",
      "  batch 145 loss: 97.10840911865235\n",
      "  batch 150 loss: 100.10437469482422\n",
      "  batch 155 loss: 89.84583435058593\n",
      "  batch 160 loss: 97.13707275390625\n",
      "  batch 165 loss: 95.37168426513672\n",
      "  batch 170 loss: 99.66571960449218\n",
      "  batch 175 loss: 93.81907348632812\n",
      "  batch 180 loss: 99.51018981933593\n",
      "  batch 185 loss: 96.18233947753906\n",
      "  batch 190 loss: 102.9846206665039\n",
      "  batch 195 loss: 87.34236907958984\n",
      "  batch 200 loss: 84.14638824462891\n",
      "  batch 205 loss: 98.89280395507812\n",
      "  batch 210 loss: 95.62678680419921\n",
      "  batch 215 loss: 93.47047271728516\n",
      "  batch 220 loss: 95.04010772705078\n",
      "  batch 225 loss: 93.63544006347657\n",
      "  batch 230 loss: 97.84152526855469\n",
      "  batch 235 loss: 98.9896743774414\n",
      "  batch 240 loss: 96.68950500488282\n",
      "  batch 245 loss: 91.54075469970704\n",
      "  batch 250 loss: 105.51886901855468\n",
      "  batch 255 loss: 104.3410400390625\n",
      "  batch 260 loss: 94.32851867675781\n",
      "  batch 265 loss: 99.27855377197265\n",
      "  batch 270 loss: 89.32440032958985\n",
      "  batch 275 loss: 94.79824371337891\n",
      "  batch 280 loss: 90.0962905883789\n",
      "  batch 285 loss: 91.63396911621093\n",
      "  batch 290 loss: 77.68068389892578\n",
      "  batch 295 loss: 97.21904602050782\n",
      "  batch 300 loss: 88.09983825683594\n",
      "  batch 305 loss: 98.9437469482422\n",
      "  batch 310 loss: 90.66405487060547\n",
      "  batch 315 loss: 105.0484634399414\n",
      "  batch 320 loss: 97.64986114501953\n",
      "  batch 325 loss: 98.98081512451172\n",
      "  batch 330 loss: 100.89242248535156\n",
      "  batch 335 loss: 98.70977935791015\n",
      "  batch 340 loss: 88.83070068359375\n",
      "  batch 345 loss: 95.73037872314453\n",
      "  batch 350 loss: 99.7840591430664\n",
      "  batch 355 loss: 96.88314361572266\n",
      "  batch 360 loss: 107.72364959716796\n",
      "  batch 365 loss: 91.35574798583984\n",
      "  batch 370 loss: 101.07512054443359\n",
      "  batch 375 loss: 92.89599609375\n",
      "  batch 380 loss: 99.34505767822266\n",
      "  batch 385 loss: 99.90178527832032\n",
      "  batch 390 loss: 99.36708984375\n",
      "  batch 395 loss: 99.41322021484375\n",
      "  batch 400 loss: 97.20236358642578\n",
      "  batch 405 loss: 99.44774322509765\n",
      "  batch 410 loss: 105.52532653808593\n",
      "  batch 415 loss: 105.70802307128906\n",
      "  batch 420 loss: 85.62281494140625\n",
      "  batch 425 loss: 106.2796844482422\n",
      "  batch 430 loss: 103.43323211669922\n",
      "  batch 435 loss: 97.52752838134765\n",
      "  batch 440 loss: 102.11561126708985\n",
      "  batch 445 loss: 94.2153076171875\n",
      "  batch 450 loss: 85.41821594238282\n",
      "  batch 455 loss: 95.44635772705078\n",
      "  batch 460 loss: 104.97994384765624\n",
      "  batch 465 loss: 104.61397247314453\n",
      "  batch 470 loss: 104.28065795898438\n",
      "  batch 475 loss: 106.91901550292968\n",
      "  batch 480 loss: 106.8883773803711\n",
      "  batch 485 loss: 101.57656860351562\n",
      "  batch 490 loss: 95.27801818847657\n",
      "  batch 495 loss: 100.16710510253907\n",
      "  batch 500 loss: 107.28377685546874\n",
      "  batch 505 loss: 107.17898712158203\n",
      "  batch 510 loss: 107.33514404296875\n",
      "  batch 515 loss: 97.60152435302734\n",
      "Epoch: 11 is done\n",
      "  batch 5 loss: 80.38850402832031\n",
      "  batch 10 loss: 88.67518005371093\n",
      "  batch 15 loss: 76.53285369873046\n",
      "  batch 20 loss: 82.05701446533203\n",
      "  batch 25 loss: 86.92343292236328\n",
      "  batch 30 loss: 87.40368194580078\n",
      "  batch 35 loss: 88.49970092773438\n",
      "  batch 40 loss: 86.15873413085937\n",
      "  batch 45 loss: 83.84364471435546\n",
      "  batch 50 loss: 79.7882469177246\n",
      "  batch 55 loss: 82.31541595458984\n",
      "  batch 60 loss: 96.79680480957032\n",
      "  batch 65 loss: 86.35368957519532\n",
      "  batch 70 loss: 82.34309539794921\n",
      "  batch 75 loss: 84.68327026367187\n",
      "  batch 80 loss: 78.41415786743164\n",
      "  batch 85 loss: 82.92168502807617\n",
      "  batch 90 loss: 77.43565826416015\n",
      "  batch 95 loss: 77.49504089355469\n",
      "  batch 100 loss: 81.60723419189453\n",
      "  batch 105 loss: 83.79988098144531\n",
      "  batch 110 loss: 77.19808197021484\n",
      "  batch 115 loss: 79.14116516113282\n",
      "  batch 120 loss: 84.42462310791015\n",
      "  batch 125 loss: 83.79442138671875\n",
      "  batch 130 loss: 84.72115478515624\n",
      "  batch 135 loss: 91.05975036621093\n",
      "  batch 140 loss: 89.07740936279296\n",
      "  batch 145 loss: 92.51492919921876\n",
      "  batch 150 loss: 86.16152954101562\n",
      "  batch 155 loss: 88.45616760253907\n",
      "  batch 160 loss: 95.20524597167969\n",
      "  batch 165 loss: 83.60884170532226\n",
      "  batch 170 loss: 85.69122161865235\n",
      "  batch 175 loss: 89.345849609375\n",
      "  batch 180 loss: 86.35591735839844\n",
      "  batch 185 loss: 93.3453857421875\n",
      "  batch 190 loss: 77.77042083740234\n",
      "  batch 195 loss: 88.88454742431641\n",
      "  batch 200 loss: 90.75597839355468\n",
      "  batch 205 loss: 88.60356140136719\n",
      "  batch 210 loss: 98.65836791992187\n",
      "  batch 215 loss: 87.52051162719727\n",
      "  batch 220 loss: 94.91308898925782\n",
      "  batch 225 loss: 86.96424102783203\n",
      "  batch 230 loss: 89.84102478027344\n",
      "  batch 235 loss: 84.74576263427734\n",
      "  batch 240 loss: 81.7631820678711\n",
      "  batch 245 loss: 99.5636978149414\n",
      "  batch 250 loss: 86.93510131835937\n",
      "  batch 255 loss: 92.54832763671875\n",
      "  batch 260 loss: 92.42535247802735\n",
      "  batch 265 loss: 86.90060729980469\n",
      "  batch 270 loss: 84.36978912353516\n",
      "  batch 275 loss: 89.05824432373046\n",
      "  batch 280 loss: 97.97924346923828\n",
      "  batch 285 loss: 93.57436981201172\n",
      "  batch 290 loss: 84.40067138671876\n",
      "  batch 295 loss: 90.56600189208984\n",
      "  batch 300 loss: 97.48165130615234\n",
      "  batch 305 loss: 92.53586273193359\n",
      "  batch 310 loss: 90.46037445068359\n",
      "  batch 315 loss: 88.14624328613282\n",
      "  batch 320 loss: 100.11146240234375\n",
      "  batch 325 loss: 100.0338348388672\n",
      "  batch 330 loss: 91.3942138671875\n",
      "  batch 335 loss: 88.49609832763672\n",
      "  batch 340 loss: 86.21072387695312\n",
      "  batch 345 loss: 88.82004852294922\n",
      "  batch 350 loss: 104.3544204711914\n",
      "  batch 355 loss: 94.1426513671875\n",
      "  batch 360 loss: 89.56529388427734\n",
      "  batch 365 loss: 95.49601745605469\n",
      "  batch 370 loss: 100.22582397460937\n",
      "  batch 375 loss: 94.16505737304688\n",
      "  batch 380 loss: 94.42457122802735\n",
      "  batch 385 loss: 101.16187744140625\n",
      "  batch 390 loss: 93.52610778808594\n",
      "  batch 395 loss: 97.64576568603516\n",
      "  batch 400 loss: 95.99409942626953\n",
      "  batch 405 loss: 87.13824081420898\n",
      "  batch 410 loss: 90.26478118896485\n",
      "  batch 415 loss: 95.22429809570312\n",
      "  batch 420 loss: 85.78110198974609\n",
      "  batch 425 loss: 98.35564575195312\n",
      "  batch 430 loss: 95.31670837402343\n",
      "  batch 435 loss: 86.73652954101563\n",
      "  batch 440 loss: 95.06443786621094\n",
      "  batch 445 loss: 95.92068023681641\n",
      "  batch 450 loss: 101.43006591796875\n",
      "  batch 455 loss: 93.40496826171875\n",
      "  batch 460 loss: 100.8609619140625\n",
      "  batch 465 loss: 98.84557342529297\n",
      "  batch 470 loss: 104.88107757568359\n",
      "  batch 475 loss: 96.74641876220703\n",
      "  batch 480 loss: 90.8839599609375\n",
      "  batch 485 loss: 105.02580108642579\n",
      "  batch 490 loss: 101.76578369140626\n",
      "  batch 495 loss: 105.030859375\n",
      "  batch 500 loss: 102.05752258300781\n",
      "  batch 505 loss: 93.86702575683594\n",
      "  batch 510 loss: 94.3033233642578\n",
      "  batch 515 loss: 106.30096435546875\n",
      "Epoch: 12 is done\n",
      "  batch 5 loss: 73.12703857421874\n",
      "  batch 10 loss: 72.63532638549805\n",
      "  batch 15 loss: 83.7477813720703\n",
      "  batch 20 loss: 82.7418426513672\n",
      "  batch 25 loss: 82.14631805419921\n",
      "  batch 30 loss: 88.79124298095704\n",
      "  batch 35 loss: 74.55701522827148\n",
      "  batch 40 loss: 83.884326171875\n",
      "  batch 45 loss: 80.69010009765626\n",
      "  batch 50 loss: 78.8106918334961\n",
      "  batch 55 loss: 82.62507019042968\n",
      "  batch 60 loss: 77.58110198974609\n",
      "  batch 65 loss: 82.54059600830078\n",
      "  batch 70 loss: 90.62657775878907\n",
      "  batch 75 loss: 78.50016937255859\n",
      "  batch 80 loss: 95.68369903564454\n",
      "  batch 85 loss: 76.47897338867188\n",
      "  batch 90 loss: 78.46171722412109\n",
      "  batch 95 loss: 85.14537048339844\n",
      "  batch 100 loss: 85.13018951416015\n",
      "  batch 105 loss: 85.90655364990235\n",
      "  batch 110 loss: 80.88629150390625\n",
      "  batch 115 loss: 75.45271301269531\n",
      "  batch 120 loss: 86.29829864501953\n",
      "  batch 125 loss: 89.98334045410157\n",
      "  batch 130 loss: 72.34387741088867\n",
      "  batch 135 loss: 77.03355255126954\n",
      "  batch 140 loss: 89.20892944335938\n",
      "  batch 145 loss: 75.02538223266602\n",
      "  batch 150 loss: 82.70236358642578\n",
      "  batch 155 loss: 90.10642547607422\n",
      "  batch 160 loss: 89.53463897705078\n",
      "  batch 165 loss: 87.23878021240235\n",
      "  batch 170 loss: 88.17440795898438\n",
      "  batch 175 loss: 75.70317077636719\n",
      "  batch 180 loss: 91.00211639404297\n",
      "  batch 185 loss: 84.03195190429688\n",
      "  batch 190 loss: 92.00936584472656\n",
      "  batch 195 loss: 93.19438018798829\n",
      "  batch 200 loss: 80.67215270996094\n",
      "  batch 205 loss: 80.20390014648437\n",
      "  batch 210 loss: 85.41990509033204\n",
      "  batch 215 loss: 79.79110107421874\n",
      "  batch 220 loss: 83.33562622070312\n",
      "  batch 225 loss: 86.79884643554688\n",
      "  batch 230 loss: 90.86514739990234\n",
      "  batch 235 loss: 76.72055130004883\n",
      "  batch 240 loss: 76.75095825195312\n",
      "  batch 245 loss: 88.62604675292968\n",
      "  batch 250 loss: 85.66785583496093\n",
      "  batch 255 loss: 85.81562957763671\n",
      "  batch 260 loss: 90.95702667236328\n",
      "  batch 265 loss: 87.2782196044922\n",
      "  batch 270 loss: 111.92622222900391\n",
      "  batch 275 loss: 91.69833984375\n",
      "  batch 280 loss: 75.92686920166015\n",
      "  batch 285 loss: 81.36157836914063\n",
      "  batch 290 loss: 90.79209747314454\n",
      "  batch 295 loss: 92.65860748291016\n",
      "  batch 300 loss: 95.80289459228516\n",
      "  batch 305 loss: 79.03408203125\n",
      "  batch 310 loss: 92.23974914550782\n",
      "  batch 315 loss: 94.84405212402343\n",
      "  batch 320 loss: 77.0890121459961\n",
      "  batch 325 loss: 105.79492950439453\n",
      "  batch 330 loss: 78.78211212158203\n",
      "  batch 335 loss: 82.00667800903321\n",
      "  batch 340 loss: 98.34285278320313\n",
      "  batch 345 loss: 88.57954254150391\n",
      "  batch 350 loss: 92.89905700683593\n",
      "  batch 355 loss: 89.26946868896485\n",
      "  batch 360 loss: 96.8261703491211\n",
      "  batch 365 loss: 88.83657684326172\n",
      "  batch 370 loss: 82.23077239990235\n",
      "  batch 375 loss: 83.67077178955078\n",
      "  batch 380 loss: 86.25821838378906\n",
      "  batch 385 loss: 87.5747055053711\n",
      "  batch 390 loss: 78.80065536499023\n",
      "  batch 395 loss: 85.95169982910156\n",
      "  batch 400 loss: 93.03873596191406\n",
      "  batch 405 loss: 91.95901336669922\n",
      "  batch 410 loss: 97.66334533691406\n",
      "  batch 415 loss: 91.99866333007813\n",
      "  batch 420 loss: 89.77176208496094\n",
      "  batch 425 loss: 104.75040588378906\n",
      "  batch 430 loss: 101.93447570800781\n",
      "  batch 435 loss: 87.1418472290039\n",
      "  batch 440 loss: 102.79020538330079\n",
      "  batch 445 loss: 89.12649230957031\n",
      "  batch 450 loss: 97.44449157714844\n",
      "  batch 455 loss: 81.08634338378906\n",
      "  batch 460 loss: 87.6128936767578\n",
      "  batch 465 loss: 89.33330688476562\n",
      "  batch 470 loss: 85.32187347412109\n",
      "  batch 475 loss: 91.99239349365234\n",
      "  batch 480 loss: 91.70820465087891\n",
      "  batch 485 loss: 89.77203979492188\n",
      "  batch 490 loss: 88.3702163696289\n",
      "  batch 495 loss: 103.28061218261719\n",
      "  batch 500 loss: 99.54863891601562\n",
      "  batch 505 loss: 105.94095611572266\n",
      "  batch 510 loss: 100.17568359375\n",
      "  batch 515 loss: 95.5337905883789\n",
      "Epoch: 13 is done\n",
      "  batch 5 loss: 80.50652770996093\n",
      "  batch 10 loss: 71.63562927246093\n",
      "  batch 15 loss: 85.31895599365234\n",
      "  batch 20 loss: 81.49524383544922\n",
      "  batch 25 loss: 69.90443344116211\n",
      "  batch 30 loss: 79.14947204589843\n",
      "  batch 35 loss: 73.90552978515625\n",
      "  batch 40 loss: 77.80265197753906\n",
      "  batch 45 loss: 71.25059814453125\n",
      "  batch 50 loss: 80.31232376098633\n",
      "  batch 55 loss: 84.6706527709961\n",
      "  batch 60 loss: 75.44613494873047\n",
      "  batch 65 loss: 88.4570083618164\n",
      "  batch 70 loss: 73.69713592529297\n",
      "  batch 75 loss: 80.08316497802734\n",
      "  batch 80 loss: 75.09414520263672\n",
      "  batch 85 loss: 79.411376953125\n",
      "  batch 90 loss: 88.06427612304688\n",
      "  batch 95 loss: 90.66043853759766\n",
      "  batch 100 loss: 80.61605072021484\n",
      "  batch 105 loss: 84.8247314453125\n",
      "  batch 110 loss: 82.98968963623047\n",
      "  batch 115 loss: 76.03721771240234\n",
      "  batch 120 loss: 81.22504196166992\n",
      "  batch 125 loss: 80.9364013671875\n",
      "  batch 130 loss: 77.4064826965332\n",
      "  batch 135 loss: 78.55947341918946\n",
      "  batch 140 loss: 84.59026336669922\n",
      "  batch 145 loss: 78.28012237548828\n",
      "  batch 150 loss: 82.95953521728515\n",
      "  batch 155 loss: 76.94449768066406\n",
      "  batch 160 loss: 72.22040252685547\n",
      "  batch 165 loss: 85.02167205810547\n",
      "  batch 170 loss: 87.97962799072266\n",
      "  batch 175 loss: 81.50890045166015\n",
      "  batch 180 loss: 76.82303466796876\n",
      "  batch 185 loss: 79.26760330200196\n",
      "  batch 190 loss: 76.27337646484375\n",
      "  batch 195 loss: 88.43170166015625\n",
      "  batch 200 loss: 80.73817443847656\n",
      "  batch 205 loss: 80.74182434082032\n",
      "  batch 210 loss: 79.55848999023438\n",
      "  batch 215 loss: 87.37038116455078\n",
      "  batch 220 loss: 84.8959114074707\n",
      "  batch 225 loss: 85.32189025878907\n",
      "  batch 230 loss: 75.81905822753906\n",
      "  batch 235 loss: 82.60050201416016\n",
      "  batch 240 loss: 82.11640777587891\n",
      "  batch 245 loss: 80.69153747558593\n",
      "  batch 250 loss: 102.6698226928711\n",
      "  batch 255 loss: 88.28094940185547\n",
      "  batch 260 loss: 81.88801803588868\n",
      "  batch 265 loss: 77.27197799682617\n",
      "  batch 270 loss: 85.38391571044922\n",
      "  batch 275 loss: 84.84209747314453\n",
      "  batch 280 loss: 85.86227111816406\n",
      "  batch 285 loss: 83.27082061767578\n",
      "  batch 290 loss: 98.06650085449219\n",
      "  batch 295 loss: 75.66894302368163\n",
      "  batch 300 loss: 87.2889503479004\n",
      "  batch 305 loss: 97.21097869873047\n",
      "  batch 310 loss: 84.6098030090332\n",
      "  batch 315 loss: 82.37668838500977\n",
      "  batch 320 loss: 81.9158950805664\n",
      "  batch 325 loss: 91.87982482910157\n",
      "  batch 330 loss: 82.85240325927734\n",
      "  batch 335 loss: 77.1969207763672\n",
      "  batch 340 loss: 83.30547332763672\n",
      "  batch 345 loss: 96.99239044189453\n",
      "  batch 350 loss: 93.28959808349609\n",
      "  batch 355 loss: 76.28948211669922\n",
      "  batch 360 loss: 86.55263061523438\n",
      "  batch 365 loss: 87.6688461303711\n",
      "  batch 370 loss: 87.31424560546876\n",
      "  batch 375 loss: 85.98221893310547\n",
      "  batch 380 loss: 90.84597244262696\n",
      "  batch 385 loss: 92.37638244628906\n",
      "  batch 390 loss: 81.45682907104492\n",
      "  batch 395 loss: 85.42267303466797\n",
      "  batch 400 loss: 87.26119537353516\n",
      "  batch 405 loss: 89.90762481689453\n",
      "  batch 410 loss: 93.63749847412109\n",
      "  batch 415 loss: 89.46109771728516\n",
      "  batch 420 loss: 91.49688720703125\n",
      "  batch 425 loss: 79.98489227294922\n",
      "  batch 430 loss: 89.47840118408203\n",
      "  batch 435 loss: 83.18408660888672\n",
      "  batch 440 loss: 83.8164276123047\n",
      "  batch 445 loss: 104.65367584228515\n",
      "  batch 450 loss: 85.96971435546875\n",
      "  batch 455 loss: 88.28315505981445\n",
      "  batch 460 loss: 81.19562377929688\n",
      "  batch 465 loss: 83.53341979980469\n",
      "  batch 470 loss: 80.08078308105469\n",
      "  batch 475 loss: 79.21210632324218\n",
      "  batch 480 loss: 96.01518859863282\n",
      "  batch 485 loss: 91.68734130859374\n",
      "  batch 490 loss: 91.71185302734375\n",
      "  batch 495 loss: 98.01316528320312\n",
      "  batch 500 loss: 104.26426849365234\n",
      "  batch 505 loss: 102.19438171386719\n",
      "  batch 510 loss: 98.42776336669922\n",
      "  batch 515 loss: 89.73483276367188\n",
      "Epoch: 14 is done\n",
      "  batch 5 loss: 85.8132438659668\n",
      "  batch 10 loss: 84.33268127441406\n",
      "  batch 15 loss: 79.44940032958985\n",
      "  batch 20 loss: 82.14441223144532\n",
      "  batch 25 loss: 86.42450561523438\n",
      "  batch 30 loss: 82.55055694580078\n",
      "  batch 35 loss: 91.68219146728515\n",
      "  batch 40 loss: 74.43569869995117\n",
      "  batch 45 loss: 88.90026092529297\n",
      "  batch 50 loss: 79.9048583984375\n",
      "  batch 55 loss: 90.60620727539063\n",
      "  batch 60 loss: 86.1469108581543\n",
      "  batch 65 loss: 74.83140335083007\n",
      "  batch 70 loss: 71.64709396362305\n",
      "  batch 75 loss: 77.44910125732422\n",
      "  batch 80 loss: 66.63352737426757\n",
      "  batch 85 loss: 82.71516723632813\n",
      "  batch 90 loss: 79.67863616943359\n",
      "  batch 95 loss: 85.7271224975586\n",
      "  batch 100 loss: 78.48756561279296\n",
      "  batch 105 loss: 81.36412506103515\n",
      "  batch 110 loss: 72.53038330078125\n",
      "  batch 115 loss: 79.6884162902832\n",
      "  batch 120 loss: 84.42207794189453\n",
      "  batch 125 loss: 74.44200210571289\n",
      "  batch 130 loss: 72.50735321044922\n",
      "  batch 135 loss: 90.743603515625\n",
      "  batch 140 loss: 74.84963455200196\n",
      "  batch 145 loss: 87.0457992553711\n",
      "  batch 150 loss: 82.47462921142578\n",
      "  batch 155 loss: 86.96467895507813\n",
      "  batch 160 loss: 82.530224609375\n",
      "  batch 165 loss: 78.95214080810547\n",
      "  batch 170 loss: 84.73345184326172\n",
      "  batch 175 loss: 83.91202316284179\n",
      "  batch 180 loss: 88.38111572265625\n",
      "  batch 185 loss: 88.130029296875\n",
      "  batch 190 loss: 74.92273788452148\n",
      "  batch 195 loss: 81.09669952392578\n",
      "  batch 200 loss: 88.62416534423828\n",
      "  batch 205 loss: 87.30904235839844\n",
      "  batch 210 loss: 84.26521453857421\n",
      "  batch 215 loss: 86.62569885253906\n",
      "  batch 220 loss: 81.67429046630859\n",
      "  batch 225 loss: 78.58509521484375\n",
      "  batch 230 loss: 77.52057037353515\n",
      "  batch 235 loss: 90.61794891357422\n",
      "  batch 240 loss: 85.31049346923828\n",
      "  batch 245 loss: 78.79631042480469\n",
      "  batch 250 loss: 88.88672332763672\n",
      "  batch 255 loss: 86.56266632080079\n",
      "  batch 260 loss: 79.47414093017578\n",
      "  batch 265 loss: 85.85284271240235\n",
      "  batch 270 loss: 88.18550415039063\n",
      "  batch 275 loss: 71.8895248413086\n",
      "  batch 280 loss: 96.28516235351563\n",
      "  batch 285 loss: 78.94540557861328\n",
      "  batch 290 loss: 90.09015808105468\n",
      "  batch 295 loss: 81.77766418457031\n",
      "  batch 300 loss: 70.73660049438476\n",
      "  batch 305 loss: 90.35728759765625\n",
      "  batch 310 loss: 83.56863403320312\n",
      "  batch 315 loss: 89.165869140625\n",
      "  batch 320 loss: 89.7259735107422\n",
      "  batch 325 loss: 85.43685607910156\n",
      "  batch 330 loss: 77.15116424560547\n",
      "  batch 335 loss: 81.27859039306641\n",
      "  batch 340 loss: 86.07573089599609\n",
      "  batch 345 loss: 85.71149978637695\n",
      "  batch 350 loss: 72.40049057006836\n",
      "  batch 355 loss: 84.43582534790039\n",
      "  batch 360 loss: 87.96541290283203\n",
      "  batch 365 loss: 84.40264129638672\n",
      "  batch 370 loss: 82.52183074951172\n",
      "  batch 375 loss: 94.98521270751954\n",
      "  batch 380 loss: 78.65565185546875\n",
      "  batch 385 loss: 87.44666137695313\n",
      "  batch 390 loss: 84.13296508789062\n",
      "  batch 395 loss: 90.26053619384766\n",
      "  batch 400 loss: 91.61831512451172\n",
      "  batch 405 loss: 88.77094879150391\n",
      "  batch 410 loss: 80.30208892822266\n",
      "  batch 415 loss: 77.37614059448242\n",
      "  batch 420 loss: 81.80102233886718\n",
      "  batch 425 loss: 101.23240966796875\n",
      "  batch 430 loss: 96.16747589111328\n",
      "  batch 435 loss: 80.71371459960938\n",
      "  batch 440 loss: 77.41032257080079\n",
      "  batch 445 loss: 83.79120483398438\n",
      "  batch 450 loss: 106.85180206298828\n",
      "  batch 455 loss: 86.39879608154297\n",
      "  batch 460 loss: 92.08985748291016\n",
      "  batch 465 loss: 84.84481353759766\n",
      "  batch 470 loss: 91.47574157714844\n",
      "  batch 475 loss: 86.52828521728516\n",
      "  batch 480 loss: 78.3712875366211\n",
      "  batch 485 loss: 95.00256958007813\n",
      "  batch 490 loss: 84.17953948974609\n",
      "  batch 495 loss: 99.77856140136718\n",
      "  batch 500 loss: 93.17352905273438\n",
      "  batch 505 loss: 88.25084381103515\n",
      "  batch 510 loss: 91.48349609375\n",
      "  batch 515 loss: 85.76957855224609\n",
      "Epoch: 15 is done\n",
      "  batch 5 loss: 85.23674545288085\n",
      "  batch 10 loss: 60.69676666259765\n",
      "  batch 15 loss: 81.40581817626953\n",
      "  batch 20 loss: 85.14464721679687\n",
      "  batch 25 loss: 81.10033721923828\n",
      "  batch 30 loss: 78.31326751708984\n",
      "  batch 35 loss: 68.35481643676758\n",
      "  batch 40 loss: 66.37687377929687\n",
      "  batch 45 loss: 80.3415512084961\n",
      "  batch 50 loss: 75.09698028564453\n",
      "  batch 55 loss: 74.96643676757813\n",
      "  batch 60 loss: 68.8808723449707\n",
      "  batch 65 loss: 71.57734527587891\n",
      "  batch 70 loss: 78.66756439208984\n",
      "  batch 75 loss: 73.09221801757812\n",
      "  batch 80 loss: 83.7167739868164\n",
      "  batch 85 loss: 66.10569763183594\n",
      "  batch 90 loss: 79.89180297851563\n",
      "  batch 95 loss: 77.35990905761719\n",
      "  batch 100 loss: 77.21761932373047\n",
      "  batch 105 loss: 70.74581298828124\n",
      "  batch 110 loss: 81.61292419433593\n",
      "  batch 115 loss: 78.45244903564453\n",
      "  batch 120 loss: 74.43037948608398\n",
      "  batch 125 loss: 80.96328125\n",
      "  batch 130 loss: 85.89942474365235\n",
      "  batch 135 loss: 85.26534729003906\n",
      "  batch 140 loss: 71.1075942993164\n",
      "  batch 145 loss: 70.03755798339844\n",
      "  batch 150 loss: 77.43981628417968\n",
      "  batch 155 loss: 81.82536926269532\n",
      "  batch 160 loss: 81.81382446289062\n",
      "  batch 165 loss: 87.76353149414062\n",
      "  batch 170 loss: 74.50264129638671\n",
      "  batch 175 loss: 80.06831970214844\n",
      "  batch 180 loss: 78.20024490356445\n",
      "  batch 185 loss: 76.25587921142578\n",
      "  batch 190 loss: 76.12142486572266\n",
      "  batch 195 loss: 69.94277877807617\n",
      "  batch 200 loss: 71.85436248779297\n",
      "  batch 205 loss: 79.44934997558593\n",
      "  batch 210 loss: 78.90330963134765\n",
      "  batch 215 loss: 73.97066497802734\n",
      "  batch 220 loss: 79.20037384033203\n",
      "  batch 225 loss: 79.98871765136718\n",
      "  batch 230 loss: 78.89129943847657\n",
      "  batch 235 loss: 81.1362922668457\n",
      "  batch 240 loss: 78.3861587524414\n",
      "  batch 245 loss: 75.52418518066406\n",
      "  batch 250 loss: 76.27355499267578\n",
      "  batch 255 loss: 72.54888916015625\n",
      "  batch 260 loss: 82.62259826660156\n",
      "  batch 265 loss: 77.9146743774414\n",
      "  batch 270 loss: 78.13913116455078\n",
      "  batch 275 loss: 81.68376159667969\n",
      "  batch 280 loss: 91.273291015625\n",
      "  batch 285 loss: 80.30308685302734\n",
      "  batch 290 loss: 95.20415802001953\n",
      "  batch 295 loss: 74.64700927734376\n",
      "  batch 300 loss: 82.32500686645508\n",
      "  batch 305 loss: 83.50775756835938\n",
      "  batch 310 loss: 84.09491271972657\n",
      "  batch 315 loss: 79.86138153076172\n",
      "  batch 320 loss: 80.97237777709961\n",
      "  batch 325 loss: 88.7246307373047\n",
      "  batch 330 loss: 82.80449676513672\n",
      "  batch 335 loss: 90.38773651123047\n",
      "  batch 340 loss: 79.88734664916993\n",
      "  batch 345 loss: 83.12401428222657\n",
      "  batch 350 loss: 80.55891418457031\n",
      "  batch 355 loss: 77.63396377563477\n",
      "  batch 360 loss: 90.53935241699219\n",
      "  batch 365 loss: 70.37728424072266\n",
      "  batch 370 loss: 117.19962310791016\n",
      "  batch 375 loss: 242.70797119140624\n",
      "  batch 380 loss: 192.2781723022461\n",
      "  batch 385 loss: 131.59808654785155\n",
      "  batch 390 loss: 137.93988952636718\n",
      "  batch 395 loss: 149.84161224365235\n",
      "  batch 400 loss: 179.827197265625\n",
      "  batch 405 loss: 152.93929748535157\n",
      "  batch 410 loss: 165.53500213623047\n",
      "  batch 415 loss: 178.8808135986328\n",
      "  batch 420 loss: 138.5831771850586\n",
      "  batch 425 loss: 172.96749267578124\n",
      "  batch 430 loss: 235.682373046875\n",
      "  batch 435 loss: 448.378173828125\n",
      "  batch 440 loss: 293.16266479492185\n",
      "  batch 445 loss: 178.88683166503907\n",
      "  batch 450 loss: 151.781494140625\n",
      "  batch 455 loss: 145.35925750732423\n",
      "  batch 460 loss: 161.1006072998047\n",
      "  batch 465 loss: 156.05724487304687\n",
      "  batch 470 loss: 205.94060974121095\n",
      "  batch 475 loss: 162.5118408203125\n",
      "  batch 480 loss: 169.5625457763672\n",
      "  batch 485 loss: 160.47001953125\n",
      "  batch 490 loss: 156.5051300048828\n",
      "  batch 495 loss: 186.52994689941406\n",
      "  batch 500 loss: 144.37125091552736\n",
      "  batch 505 loss: 134.48335723876954\n",
      "  batch 510 loss: 141.9754165649414\n",
      "  batch 515 loss: 142.98997497558594\n",
      "Epoch: 16 is done\n",
      "  batch 5 loss: 111.42099456787109\n",
      "  batch 10 loss: 99.6889144897461\n",
      "  batch 15 loss: 114.23422241210938\n",
      "  batch 20 loss: 106.47371978759766\n",
      "  batch 25 loss: 85.27667388916015\n",
      "  batch 30 loss: 112.47199096679688\n",
      "  batch 35 loss: 103.98346557617188\n",
      "  batch 40 loss: 109.6000961303711\n",
      "  batch 45 loss: 92.6827621459961\n",
      "  batch 50 loss: 105.9518035888672\n",
      "  batch 55 loss: 107.655078125\n",
      "  batch 60 loss: 92.05376586914062\n",
      "  batch 65 loss: 103.43191986083984\n",
      "  batch 70 loss: 93.50271759033203\n",
      "  batch 75 loss: 103.32887268066406\n",
      "  batch 80 loss: 94.34420928955078\n",
      "  batch 85 loss: 114.9019271850586\n",
      "  batch 90 loss: 139.48200225830078\n",
      "  batch 95 loss: 104.44550476074218\n",
      "  batch 100 loss: 96.91285552978516\n",
      "  batch 105 loss: 98.298828125\n",
      "  batch 110 loss: 84.85201110839844\n",
      "  batch 115 loss: 94.6559341430664\n",
      "  batch 120 loss: 89.53711700439453\n",
      "  batch 125 loss: 92.29073791503906\n",
      "  batch 130 loss: 234.1000762939453\n",
      "  batch 135 loss: 133.60083923339843\n",
      "  batch 140 loss: 135.8885009765625\n",
      "  batch 145 loss: 105.21169738769531\n",
      "  batch 150 loss: 92.94100799560547\n",
      "  batch 155 loss: 105.76466827392578\n",
      "  batch 160 loss: 100.09461059570313\n",
      "  batch 165 loss: 113.65536804199219\n",
      "  batch 170 loss: 98.64302597045898\n",
      "  batch 175 loss: 97.26951293945312\n",
      "  batch 180 loss: 102.48167572021484\n",
      "  batch 185 loss: 107.9935516357422\n",
      "  batch 190 loss: 93.70955810546874\n",
      "  batch 195 loss: 89.07897186279297\n",
      "  batch 200 loss: 88.22579345703124\n",
      "  batch 205 loss: 97.36688385009765\n",
      "  batch 210 loss: 86.9976089477539\n",
      "  batch 215 loss: 90.08061065673829\n",
      "  batch 220 loss: 90.91042633056641\n",
      "  batch 225 loss: 84.07353363037109\n",
      "  batch 230 loss: 91.47241516113282\n",
      "  batch 235 loss: 86.05665130615235\n",
      "  batch 240 loss: 86.22991638183593\n",
      "  batch 245 loss: 92.87113952636719\n",
      "  batch 250 loss: 77.13345336914062\n",
      "  batch 255 loss: 111.7772003173828\n",
      "  batch 260 loss: 79.71403350830079\n",
      "  batch 265 loss: 86.04450073242188\n",
      "  batch 270 loss: 87.35128021240234\n",
      "  batch 275 loss: 89.06365051269532\n",
      "  batch 280 loss: 81.74349060058594\n",
      "  batch 285 loss: 93.76398468017578\n",
      "  batch 290 loss: 84.45067901611328\n",
      "  batch 295 loss: 84.17168426513672\n",
      "  batch 300 loss: 94.08811264038086\n",
      "  batch 305 loss: 79.02088241577148\n",
      "  batch 310 loss: 83.47952117919922\n",
      "  batch 315 loss: 85.17557525634766\n",
      "  batch 320 loss: 95.14679870605468\n",
      "  batch 325 loss: 94.94470977783203\n",
      "  batch 330 loss: 97.60778350830078\n",
      "  batch 335 loss: 88.58092346191407\n",
      "  batch 340 loss: 89.03747863769532\n",
      "  batch 345 loss: 79.35556182861328\n",
      "  batch 350 loss: 83.482958984375\n",
      "  batch 355 loss: 86.1318572998047\n",
      "  batch 360 loss: 88.89474945068359\n",
      "  batch 365 loss: 86.63664245605469\n",
      "  batch 370 loss: 83.36905212402344\n",
      "  batch 375 loss: 94.7420440673828\n",
      "  batch 380 loss: 93.64814605712891\n",
      "  batch 385 loss: 88.13959350585938\n",
      "  batch 390 loss: 97.34135131835937\n",
      "  batch 395 loss: 85.71611633300782\n",
      "  batch 400 loss: 91.6806625366211\n",
      "  batch 405 loss: 100.10491027832032\n",
      "  batch 410 loss: 87.45147552490235\n",
      "  batch 415 loss: 93.93846588134765\n",
      "  batch 420 loss: 90.84673309326172\n",
      "  batch 425 loss: 94.49515533447266\n",
      "  batch 430 loss: 94.78817138671874\n",
      "  batch 435 loss: 83.32804412841797\n",
      "  batch 440 loss: 88.13942642211914\n",
      "  batch 445 loss: 88.47015533447265\n",
      "  batch 450 loss: 96.247705078125\n",
      "  batch 455 loss: 84.275146484375\n",
      "  batch 460 loss: 97.5785415649414\n",
      "  batch 465 loss: 88.33844757080078\n",
      "  batch 470 loss: 96.6524642944336\n",
      "  batch 475 loss: 95.17230377197265\n",
      "  batch 480 loss: 78.44760284423828\n",
      "  batch 485 loss: 85.56550598144531\n",
      "  batch 490 loss: 93.66616516113281\n",
      "  batch 495 loss: 87.23963317871093\n",
      "  batch 500 loss: 90.65340576171874\n",
      "  batch 505 loss: 87.91546783447265\n",
      "  batch 510 loss: 95.18732147216797\n",
      "  batch 515 loss: 84.29403839111328\n",
      "Epoch: 17 is done\n",
      "  batch 5 loss: 79.89400634765624\n",
      "  batch 10 loss: 70.90889892578124\n",
      "  batch 15 loss: 68.0158905029297\n",
      "  batch 20 loss: 68.30692520141602\n",
      "  batch 25 loss: 67.5402618408203\n",
      "  batch 30 loss: 66.16960372924805\n",
      "  batch 35 loss: 65.31645812988282\n",
      "  batch 40 loss: 76.85101318359375\n",
      "  batch 45 loss: 72.91991424560547\n",
      "  batch 50 loss: 69.08532409667968\n",
      "  batch 55 loss: 72.67535247802735\n",
      "  batch 60 loss: 69.32512054443359\n",
      "  batch 65 loss: 69.91042633056641\n",
      "  batch 70 loss: 77.83999938964844\n",
      "  batch 75 loss: 64.10185394287109\n",
      "  batch 80 loss: 70.16898498535156\n",
      "  batch 85 loss: 68.59915008544922\n",
      "  batch 90 loss: 62.06520462036133\n",
      "  batch 95 loss: 72.23792419433593\n",
      "  batch 100 loss: 76.12098083496093\n",
      "  batch 105 loss: 67.94959259033203\n",
      "  batch 110 loss: 76.5205810546875\n",
      "  batch 115 loss: 72.43699340820312\n",
      "  batch 120 loss: 68.96215209960937\n",
      "  batch 125 loss: 76.65556259155274\n",
      "  batch 130 loss: 83.39310913085937\n",
      "  batch 135 loss: 71.12778701782227\n",
      "  batch 140 loss: 74.74347686767578\n",
      "  batch 145 loss: 72.9032356262207\n",
      "  batch 150 loss: 67.49114761352538\n",
      "  batch 155 loss: 76.1090476989746\n",
      "  batch 160 loss: 72.92729949951172\n",
      "  batch 165 loss: 69.29034805297852\n",
      "  batch 170 loss: 82.63294067382813\n",
      "  batch 175 loss: 68.2444076538086\n",
      "  batch 180 loss: 80.79873199462891\n",
      "  batch 185 loss: 75.30166625976562\n",
      "  batch 190 loss: 81.84635314941406\n",
      "  batch 195 loss: 70.75089645385742\n",
      "  batch 200 loss: 77.77702026367187\n",
      "  batch 205 loss: 82.26148223876953\n",
      "  batch 210 loss: 76.9889144897461\n",
      "  batch 215 loss: 79.23879852294922\n",
      "  batch 220 loss: 72.14278259277344\n",
      "  batch 225 loss: 75.94767150878906\n",
      "  batch 230 loss: 79.0836395263672\n",
      "  batch 235 loss: 79.93093719482422\n",
      "  batch 240 loss: 75.47858810424805\n",
      "  batch 245 loss: 78.08024291992187\n",
      "  batch 250 loss: 80.51226348876953\n",
      "  batch 255 loss: 69.00121688842773\n",
      "  batch 260 loss: 77.72923355102539\n",
      "  batch 265 loss: 78.5757080078125\n",
      "  batch 270 loss: 77.30656280517579\n",
      "  batch 275 loss: 71.42003021240234\n",
      "  batch 280 loss: 86.50045471191406\n",
      "  batch 285 loss: 77.10837249755859\n",
      "  batch 290 loss: 81.81496887207031\n",
      "  batch 295 loss: 78.63281707763672\n",
      "  batch 300 loss: 80.6622085571289\n",
      "  batch 305 loss: 80.03858489990235\n",
      "  batch 310 loss: 84.44097747802735\n",
      "  batch 315 loss: 76.66871185302735\n",
      "  batch 320 loss: 68.98921279907226\n",
      "  batch 325 loss: 67.40245513916015\n",
      "  batch 330 loss: 74.95251770019532\n",
      "  batch 335 loss: 61.76331634521485\n",
      "  batch 340 loss: 77.71244659423829\n",
      "  batch 345 loss: 79.0055030822754\n",
      "  batch 350 loss: 81.39497909545898\n",
      "  batch 355 loss: 63.793294525146486\n",
      "  batch 360 loss: 68.73758468627929\n",
      "  batch 365 loss: 78.26683044433594\n",
      "  batch 370 loss: 77.54205322265625\n",
      "  batch 375 loss: 75.39822006225586\n",
      "  batch 380 loss: 69.96626129150391\n",
      "  batch 385 loss: 65.26395034790039\n",
      "  batch 390 loss: 77.38375473022461\n",
      "  batch 395 loss: 74.9513671875\n",
      "  batch 400 loss: 82.54052124023437\n",
      "  batch 405 loss: 67.0422477722168\n",
      "  batch 410 loss: 81.180712890625\n",
      "  batch 415 loss: 75.94435195922851\n",
      "  batch 420 loss: 69.04769821166992\n",
      "  batch 425 loss: 75.77181549072266\n",
      "  batch 430 loss: 82.35596618652343\n",
      "  batch 435 loss: 79.98050994873047\n",
      "  batch 440 loss: 76.50604095458985\n",
      "  batch 445 loss: 77.74868927001953\n",
      "  batch 450 loss: 75.55341262817383\n",
      "  batch 455 loss: 76.43418884277344\n",
      "  batch 460 loss: 79.89414825439454\n",
      "  batch 465 loss: 70.95649642944336\n",
      "  batch 470 loss: 65.05156097412109\n",
      "  batch 475 loss: 82.02626037597656\n",
      "  batch 480 loss: 73.9928077697754\n",
      "  batch 485 loss: 77.00160217285156\n",
      "  batch 490 loss: 70.47923126220704\n",
      "  batch 495 loss: 73.60655136108399\n",
      "  batch 500 loss: 77.80057525634766\n",
      "  batch 505 loss: 75.10858154296875\n",
      "  batch 510 loss: 82.80240478515626\n",
      "  batch 515 loss: 83.49320983886719\n",
      "Epoch: 18 is done\n",
      "  batch 5 loss: 68.81151428222657\n",
      "  batch 10 loss: 65.48999938964843\n",
      "  batch 15 loss: 77.79802169799805\n",
      "  batch 20 loss: 65.50073928833008\n",
      "  batch 25 loss: 64.90314712524415\n",
      "  batch 30 loss: 56.68911209106445\n",
      "  batch 35 loss: 63.471501159667966\n",
      "  batch 40 loss: 67.4024154663086\n",
      "  batch 45 loss: 69.87275009155273\n",
      "  batch 50 loss: 70.58676300048828\n",
      "  batch 55 loss: 67.5507698059082\n",
      "  batch 60 loss: 61.097650146484376\n",
      "  batch 65 loss: 62.13047561645508\n",
      "  batch 70 loss: 59.41836013793945\n",
      "  batch 75 loss: 56.80215682983398\n",
      "  batch 80 loss: 65.28811187744141\n",
      "  batch 85 loss: 67.17161026000977\n",
      "  batch 90 loss: 70.97390365600586\n",
      "  batch 95 loss: 58.628961181640626\n",
      "  batch 100 loss: 68.44446716308593\n",
      "  batch 105 loss: 71.28005981445312\n",
      "  batch 110 loss: 76.98279571533203\n",
      "  batch 115 loss: 72.37928161621093\n",
      "  batch 120 loss: 66.65609664916992\n",
      "  batch 125 loss: 70.86276397705078\n",
      "  batch 130 loss: 65.1638412475586\n",
      "  batch 135 loss: 70.54122695922851\n",
      "  batch 140 loss: 63.86341018676758\n",
      "  batch 145 loss: 73.81694030761719\n",
      "  batch 150 loss: 65.2349464416504\n",
      "  batch 155 loss: 73.3834945678711\n",
      "  batch 160 loss: 72.9670394897461\n",
      "  batch 165 loss: 76.03251190185547\n",
      "  batch 170 loss: 65.70329055786132\n",
      "  batch 175 loss: 72.7287208557129\n",
      "  batch 180 loss: 69.49374237060547\n",
      "  batch 185 loss: 72.92192687988282\n",
      "  batch 190 loss: 61.79947204589844\n",
      "  batch 195 loss: 62.92229385375977\n",
      "  batch 200 loss: 58.588320922851565\n",
      "  batch 205 loss: 65.99910736083984\n",
      "  batch 210 loss: 81.05221557617188\n",
      "  batch 215 loss: 74.02083892822266\n",
      "  batch 220 loss: 76.54642639160156\n",
      "  batch 225 loss: 67.45144882202149\n",
      "  batch 230 loss: 73.57737884521484\n",
      "  batch 235 loss: 69.2433090209961\n",
      "  batch 240 loss: 60.60792236328125\n",
      "  batch 245 loss: 62.51395492553711\n",
      "  batch 250 loss: 64.94412155151367\n",
      "  batch 255 loss: 62.41708221435547\n",
      "  batch 260 loss: 70.96926574707031\n",
      "  batch 265 loss: 71.41372909545899\n",
      "  batch 270 loss: 68.43677368164063\n",
      "  batch 275 loss: 71.92967071533204\n",
      "  batch 280 loss: 68.88936004638671\n",
      "  batch 285 loss: 61.82653579711914\n",
      "  batch 290 loss: 60.57552719116211\n",
      "  batch 295 loss: 64.28112335205078\n",
      "  batch 300 loss: 62.29641494750977\n",
      "  batch 305 loss: 70.75222091674804\n",
      "  batch 310 loss: 71.96774597167969\n",
      "  batch 315 loss: 78.466845703125\n",
      "  batch 320 loss: 65.18447875976562\n",
      "  batch 325 loss: 78.18390808105468\n",
      "  batch 330 loss: 71.65869064331055\n",
      "  batch 335 loss: 67.98017501831055\n",
      "  batch 340 loss: 73.49959716796874\n",
      "  batch 345 loss: 64.34084777832031\n",
      "  batch 350 loss: 74.03946075439453\n",
      "  batch 355 loss: 66.03844528198242\n",
      "  batch 360 loss: 73.4002456665039\n",
      "  batch 365 loss: 66.43794097900391\n",
      "  batch 370 loss: 78.6820083618164\n",
      "  batch 375 loss: 68.50104522705078\n",
      "  batch 380 loss: 66.90858001708985\n",
      "  batch 385 loss: 68.36346435546875\n",
      "  batch 390 loss: 67.38995361328125\n",
      "  batch 395 loss: 72.9580810546875\n",
      "  batch 400 loss: 73.2344871520996\n",
      "  batch 405 loss: 74.77355499267578\n",
      "  batch 410 loss: 69.36411743164062\n",
      "  batch 415 loss: 73.26294784545898\n",
      "  batch 420 loss: 70.22042770385742\n",
      "  batch 425 loss: 67.0359115600586\n",
      "  batch 430 loss: 58.99649276733398\n",
      "  batch 435 loss: 65.93997421264649\n",
      "  batch 440 loss: 70.37466812133789\n",
      "  batch 445 loss: 87.36724395751953\n",
      "  batch 450 loss: 66.51397094726562\n",
      "  batch 455 loss: 85.22625885009765\n",
      "  batch 460 loss: 78.09836120605469\n",
      "  batch 465 loss: 70.96050186157227\n",
      "  batch 470 loss: 63.6255241394043\n",
      "  batch 475 loss: 75.7386085510254\n",
      "  batch 480 loss: 74.55432586669922\n",
      "  batch 485 loss: 79.34177093505859\n",
      "  batch 490 loss: 74.25683135986328\n",
      "  batch 495 loss: 72.94811248779297\n",
      "  batch 500 loss: 70.9637321472168\n",
      "  batch 505 loss: 60.571634674072264\n",
      "  batch 510 loss: 74.97442932128907\n",
      "  batch 515 loss: 78.52906494140625\n",
      "Epoch: 19 is done\n",
      "Finished training!\n",
      "\n",
      "[DMRL] Evaluation started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking: 100%|██████████| 19975/19975 [23:05<00:00, 14.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "     | NDCG@-1 | Train (s) |  Test (s)\n",
      "---- + ------- + --------- + ---------\n",
      "DMRL |  0.2459 |  979.5150 | 1385.9265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put everything together into an experiment and run it\n",
    "cornac.Experiment(\n",
    "    eval_method=ratio_split, models=[dmrl_recommender], metrics=[NDCG(k=20)]\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como usar esse algoritmo para tratar itens novos?\n",
    "# # como usar esse algoritmo para tratar usuarios novos?\n",
    "\n",
    "\n",
    "# eu tenho o conteudo de todos os itens, incluive itens que estao apenas n conjunto de teste?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 02b780c583 is not in the train set\n",
      "User 03fbe3c1e8 is not in the train set\n",
      "User 0503bf6097 is not in the train set\n",
      "User 0958560bd4 is not in the train set\n",
      "User 0b17c3df40 is not in the train set\n",
      "User 10c3aef33f is not in the train set\n",
      "User 119653373a is not in the train set\n",
      "User 139ccdbf5a is not in the train set\n",
      "User 1671fa4df3 is not in the train set\n",
      "User 16757f4d90 is not in the train set\n",
      "User 187454feb4 is not in the train set\n",
      "User 1af811e7b9 is not in the train set\n",
      "User 1bbf59e4a4 is not in the train set\n",
      "User 1ef563bb8b is not in the train set\n",
      "User 1f6a565547 is not in the train set\n",
      "User 211a7a84d3 is not in the train set\n",
      "User 243be2818a is not in the train set\n",
      "User 24a13b72fc is not in the train set\n",
      "User 27649dc46e is not in the train set\n",
      "User 2a3500f75b is not in the train set\n",
      "User 2ae6906d5d is not in the train set\n",
      "User 2cfdb14e05 is not in the train set\n",
      "User 2ed4d0c23c is not in the train set\n",
      "User 2ef13886ae is not in the train set\n",
      "User 302c6ebc7d is not in the train set\n",
      "User 3174db8344 is not in the train set\n",
      "User 32806098d9 is not in the train set\n",
      "User 33f009eb84 is not in the train set\n",
      "User 3402dfd220 is not in the train set\n",
      "User 35ecd9a245 is not in the train set\n",
      "User 3bf47241fd is not in the train set\n",
      "User 3cdc0a294d is not in the train set\n",
      "User 3f332c6d51 is not in the train set\n",
      "User 40008b9a53 is not in the train set\n",
      "User 40bf82623c is not in the train set\n",
      "User 4152564eeb is not in the train set\n",
      "User 448cd8ad0a is not in the train set\n",
      "User 454ea70f2b is not in the train set\n",
      "User 4579da10f3 is not in the train set\n",
      "User 49e73b2050 is not in the train set\n",
      "User 4a125a89bf is not in the train set\n",
      "User 4ad5bd1af9 is not in the train set\n",
      "User 4aec1b3435 is not in the train set\n",
      "User 4c0f9546c5 is not in the train set\n",
      "User 4c964a55e0 is not in the train set\n",
      "User 4e701bcd4c is not in the train set\n",
      "User 50a40208ac is not in the train set\n",
      "User 51aa0a41c5 is not in the train set\n",
      "User 51e0fff3f3 is not in the train set\n",
      "User 51f4efbfb3 is not in the train set\n",
      "User 52a8ed6a81 is not in the train set\n",
      "User 53d6d0c034 is not in the train set\n",
      "User 548b57cc0f is not in the train set\n",
      "User 564ed2dbdd is not in the train set\n",
      "User 621eb0b827 is not in the train set\n",
      "User 64bb3c7589 is not in the train set\n",
      "User 65d03e7a8e is not in the train set\n",
      "User 676c6ba546 is not in the train set\n",
      "User 689b243e8f is not in the train set\n",
      "User 69f65a2e91 is not in the train set\n",
      "User 6b620aedfa is not in the train set\n",
      "User 6c2665d7c3 is not in the train set\n",
      "User 6ec3ff0c92 is not in the train set\n",
      "User 70b0e5c6b6 is not in the train set\n",
      "User 71887f62f0 is not in the train set\n",
      "User 790d67b8e3 is not in the train set\n",
      "User 7c9ff93b39 is not in the train set\n",
      "User 7cef9fb420 is not in the train set\n",
      "User 7d5606f152 is not in the train set\n",
      "User 82ccad1ecc is not in the train set\n",
      "User 84550b83da is not in the train set\n",
      "User 8487e01fba is not in the train set\n",
      "User 848ceaa463 is not in the train set\n",
      "User 84a955d5ff is not in the train set\n",
      "User 84c230a5b1 is not in the train set\n",
      "User 86f2df8b1f is not in the train set\n",
      "User 87f3c42a65 is not in the train set\n",
      "User 8b2b3227d4 is not in the train set\n",
      "User 8c5a6a7482 is not in the train set\n",
      "User 8d2355364e is not in the train set\n",
      "User 8deddf03f0 is not in the train set\n",
      "User 8f1fbc45b8 is not in the train set\n",
      "User 8fc809ece3 is not in the train set\n",
      "User 8ff4a061e9 is not in the train set\n",
      "User 93c442d40a is not in the train set\n",
      "User 9424d06c85 is not in the train set\n",
      "User 943422623d is not in the train set\n",
      "User 964e752094 is not in the train set\n",
      "User 9752ba9f1e is not in the train set\n",
      "User 997d421097 is not in the train set\n",
      "User 9b03bd15cd is not in the train set\n",
      "User 9c415bdd4d is not in the train set\n",
      "User 9c810c5750 is not in the train set\n",
      "User a13e00b085 is not in the train set\n",
      "User a559244ec4 is not in the train set\n",
      "User acd6425f2d is not in the train set\n",
      "User acf73df8e4 is not in the train set\n",
      "User ad17a667ff is not in the train set\n",
      "User ad52498830 is not in the train set\n",
      "User adeaee4e38 is not in the train set\n",
      "User ae845eb437 is not in the train set\n",
      "User af4d3ce5d7 is not in the train set\n",
      "User b0e7534770 is not in the train set\n",
      "User b6ef5d5380 is not in the train set\n",
      "User b9898a075a is not in the train set\n",
      "User baa84d7484 is not in the train set\n",
      "User bb07992b31 is not in the train set\n",
      "User bbc77a1cfa is not in the train set\n",
      "User bdbe7e66f5 is not in the train set\n",
      "User be800ff41f is not in the train set\n",
      "User c14cc363e3 is not in the train set\n",
      "User c29b3351dd is not in the train set\n",
      "User c4f7cfb1e9 is not in the train set\n",
      "User c846afeb97 is not in the train set\n",
      "User c9b7db2d84 is not in the train set\n",
      "User cb955adc83 is not in the train set\n",
      "User cd13636b75 is not in the train set\n",
      "User cdd9c13725 is not in the train set\n",
      "User cf247d6daf is not in the train set\n",
      "User d1e7b08bdb is not in the train set\n",
      "User d29ee932b7 is not in the train set\n",
      "User d38901788c is not in the train set\n",
      "User d3956e2280 is not in the train set\n",
      "User d481fbe55e is not in the train set\n",
      "User d5cb15b7ec is not in the train set\n",
      "User d5e542d0cb is not in the train set\n",
      "User d606046667 is not in the train set\n",
      "User d9e6b46563 is not in the train set\n",
      "User daf33743af is not in the train set\n",
      "User db081d0be2 is not in the train set\n",
      "User db9d463dc9 is not in the train set\n",
      "User dc09c97fd7 is not in the train set\n",
      "User dcfd532698 is not in the train set\n",
      "User dd669b686f is not in the train set\n",
      "User def80cfcb4 is not in the train set\n",
      "User e321abd135 is not in the train set\n",
      "User e33bff0ce0 is not in the train set\n",
      "User e449b9317d is not in the train set\n",
      "User e480783401 is not in the train set\n",
      "User e9405e0733 is not in the train set\n",
      "User ea4b47f29f is not in the train set\n",
      "User ee16fa83c0 is not in the train set\n",
      "User ee6e910d8a is not in the train set\n",
      "User f0897e959b is not in the train set\n",
      "User f28f881458 is not in the train set\n",
      "User f4573fc71c is not in the train set\n",
      "User f7bf6074f6 is not in the train set\n",
      "User f8ea2e8463 is not in the train set\n",
      "User f9db6a7cd6 is not in the train set\n",
      "User fa07516b1d is not in the train set\n",
      "User fa2093fecd is not in the train set\n",
      "User fb2e203234 is not in the train set\n",
      "User fbe486dc4c is not in the train set\n",
      "User fc1aaf2d2e is not in the train set\n",
      "User fc1dc4549d is not in the train set\n",
      "User fdacbbcc2e is not in the train set\n",
      "User fe9de77866 is not in the train set\n",
      "User ff5552bd9e is not in the train set\n",
      "User ffbad8b9da is not in the train set\n"
     ]
    }
   ],
   "source": [
    "target_prediction = targets.copy()\n",
    "target_prediction[\"Rating\"] = -1\n",
    "\n",
    "user_id_list = targets.UserId.unique()\n",
    "for user_id in user_id_list:\n",
    "    # Get the train dataframe index of the user to predict\n",
    "    user_index = ratio_split.train_set.uid_map.get(user_id)\n",
    "\n",
    "    if user_index is None:\n",
    "        print(f\"User {user_id} is not in the train set\")\n",
    "        continue\n",
    "\n",
    "    # Flter by items to predict \n",
    "    items_to_predict = targets.loc[targets.UserId == user_id, \"ItemId\"].to_list()\n",
    "\n",
    "    # Get the train dataframe index of the items to predict\n",
    "    items_to_predict_index = np.array([ratio_split.train_set.iid_map.get(item_id) for item_id in items_to_predict])\n",
    "\n",
    "    items_to_predict_tensor = torch.tensor([idx for idx in items_to_predict_index if idx is not None])\n",
    "\n",
    "    # Get the position of items that are not in the train set\n",
    "    none_indices = [i for i, x in enumerate(items_to_predict_index) if x is None]\n",
    "\n",
    "    # Get the prediction for the items\n",
    "    line_rating = dmrl_recommender.score(user_index=user_index, item_indices=items_to_predict_tensor)\n",
    "\n",
    "    # Insert -1 in the position of items that are not in the train set\n",
    "    for index_to_insert in none_indices:\n",
    "        line_rating = np.insert(line_rating, index_to_insert, -1)\n",
    "\n",
    "    # Insert the prediction in the target_prediction dataframe\n",
    "    target_prediction.loc[targets.UserId == user_id, \"Rating\"] = line_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = target_prediction.sort_values([\"UserId\", \"Rating\"], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction.to_csv(\"submissao_3,5_DMRL_versao_2,5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction = target_prediction.drop(columns=\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_prediction.to_csv(\"submissao_3,5_DMRL_versao_2,5_sem_rating.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
